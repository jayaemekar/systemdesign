{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is System Design? The process of defining the architecture, interfaces, and data for a system to meet specified criteria is referred to as system design. When constructing a huge system, there are a few considerations to keep in mind: What are the many architectural components that can be utilized? What is the relationship between these components? What are the best tradeoffs to make in order to get the most out of these components? The issue of system design is wide. There are numerous materials on system design principles dispersed over the internet. Many tech organizations require system design as part of the technical interview process in addition to coding interviews. Building and engineering systems demand a systematic approach to system design. A smart system design considers everything in an infrastructure, from hardware and software to data and how it's kept.","title":"Home"},{"location":"#what-is-system-design","text":"The process of defining the architecture, interfaces, and data for a system to meet specified criteria is referred to as system design. When constructing a huge system, there are a few considerations to keep in mind: What are the many architectural components that can be utilized? What is the relationship between these components? What are the best tradeoffs to make in order to get the most out of these components? The issue of system design is wide. There are numerous materials on system design principles dispersed over the internet. Many tech organizations require system design as part of the technical interview process in addition to coding interviews. Building and engineering systems demand a systematic approach to system design. A smart system design considers everything in an infrastructure, from hardware and software to data and how it's kept.","title":"What is System Design?"},{"location":"BasicTerminologies/","text":"Terminologies Fundamentals We've attempted to explain some of the terms in plain English. For a more formal definition, go to wiki. Reliability The probability of a system failing in a given period is known as reliability. A distributed system is considered reliable if it continues to provide services despite the failure of one or more of its software or hardware components. Replication Replication is the process of copying data across numerous devices on a regular basis. Multiple copies of the data exist among devices after replication. This could be useful if one or more of the machines dies due to a malfunction. Consistency Assuming you have a storage system with multiple machines, consistency means the data is consistent throughout the cluster, so you can read or write to/from any node and get the same results. Eventual consistency : Consistency in the long run Exactly as the name implies. If numerous machines in a cluster store the same data, an eventual consistent model implies that all machines will eventually have the same data. It's possible that those machines have various copies of the same data at any given time (temporarily inconsistent), but they'll eventually all have the same data. Availability It's a basic metric that shows how long a system, service, or machine has been operational under typical conditions. Availability in the context of a database cluster refers to the ability to always respond to queries (read or write) regardless of node failure. The Difference Between Reliability and Availability A system is available if it is reliable. However, just because something is available does not mean it is trustworthy. Partition Tolerance In the context of a database cluster, Even if there is a \"partition\" (communications break) between two nodes in a database cluster, the cluster continues to function (both nodes are up, but can't communicate). Scaling Scalability refers to a system's, process's, or network's ability to grow and manage rising demand. Horizontal and vertical scaling are the two types of scaling. To put it another way, scaling horizontally means adding more servers. To scale vertically, the server's resources must be increased ( RAM, CPU, storage, etc. ). Sharding Sharding is a data distribution strategy that uses many machines to distribute data. Data does not fit on a single machine in most large systems. In these circumstances, sharding refers to the division of a huge database into smaller, quicker, and more manageable data shards.","title":"Basic Terminologies"},{"location":"BasicTerminologies/#terminologies-fundamentals","text":"We've attempted to explain some of the terms in plain English. For a more formal definition, go to wiki.","title":"Terminologies Fundamentals"},{"location":"BasicTerminologies/#reliability","text":"The probability of a system failing in a given period is known as reliability. A distributed system is considered reliable if it continues to provide services despite the failure of one or more of its software or hardware components.","title":"Reliability"},{"location":"BasicTerminologies/#replication","text":"Replication is the process of copying data across numerous devices on a regular basis. Multiple copies of the data exist among devices after replication. This could be useful if one or more of the machines dies due to a malfunction.","title":"Replication"},{"location":"BasicTerminologies/#consistency","text":"Assuming you have a storage system with multiple machines, consistency means the data is consistent throughout the cluster, so you can read or write to/from any node and get the same results. Eventual consistency : Consistency in the long run Exactly as the name implies. If numerous machines in a cluster store the same data, an eventual consistent model implies that all machines will eventually have the same data. It's possible that those machines have various copies of the same data at any given time (temporarily inconsistent), but they'll eventually all have the same data.","title":"Consistency"},{"location":"BasicTerminologies/#availability","text":"It's a basic metric that shows how long a system, service, or machine has been operational under typical conditions. Availability in the context of a database cluster refers to the ability to always respond to queries (read or write) regardless of node failure. The Difference Between Reliability and Availability A system is available if it is reliable. However, just because something is available does not mean it is trustworthy.","title":"Availability"},{"location":"BasicTerminologies/#partition-tolerance","text":"In the context of a database cluster, Even if there is a \"partition\" (communications break) between two nodes in a database cluster, the cluster continues to function (both nodes are up, but can't communicate).","title":"Partition Tolerance"},{"location":"BasicTerminologies/#scaling","text":"Scalability refers to a system's, process's, or network's ability to grow and manage rising demand. Horizontal and vertical scaling are the two types of scaling. To put it another way, scaling horizontally means adding more servers. To scale vertically, the server's resources must be increased ( RAM, CPU, storage, etc. ).","title":"Scaling"},{"location":"BasicTerminologies/#sharding","text":"Sharding is a data distribution strategy that uses many machines to distribute data. Data does not fit on a single machine in most large systems. In these circumstances, sharding refers to the division of a huge database into smaller, quicker, and more manageable data shards.","title":"Sharding"},{"location":"CAPTheorem/","text":"CAP Theorem According to the CAP theorem, a distributed software system cannot simultaneously provide more than two of the three guarantees (CAP): consistency, availability, and partition tolerance. Trading off among CAP is almost the first thing we want to think about when designing a distributed system. According to the CAP theorem, we can only choose two of the following three possibilities when constructing a distributed system: Consistency: At the same time, all nodes see the same data. Before enabling more reads, some nodes are updated to ensure consistency. Availability: Every request receives a success/failure response. Data is replicated across multiple servers to ensure availability. Partition tolerance: Despite communication loss or partial failure, the system continues to function. A partition-tolerant system may withstand any level of network failure without causing the entire network to fail. Data is appropriately replicated across a variety of nodes and networks to ensure that the system remains operational during outages. We won't be able to create a general data store that is always available, sequentially consistent, and resilient to partition faults. Only two of these three traits can be combined in a system. Because all nodes should see the same set of changes in the same order in order to remain consistent. If the network is partitioned, however, updates in one partition may not reach the other partitions before a client reads from the out-of-date partition after reading from the current one. The only way to deal with this issue is to stop servicing requests from the out-of-date partition, however this reduces the service's availability to zero.","title":"CAP Theorem"},{"location":"CAPTheorem/#cap-theorem","text":"According to the CAP theorem, a distributed software system cannot simultaneously provide more than two of the three guarantees (CAP): consistency, availability, and partition tolerance. Trading off among CAP is almost the first thing we want to think about when designing a distributed system. According to the CAP theorem, we can only choose two of the following three possibilities when constructing a distributed system: Consistency: At the same time, all nodes see the same data. Before enabling more reads, some nodes are updated to ensure consistency. Availability: Every request receives a success/failure response. Data is replicated across multiple servers to ensure availability. Partition tolerance: Despite communication loss or partial failure, the system continues to function. A partition-tolerant system may withstand any level of network failure without causing the entire network to fail. Data is appropriately replicated across a variety of nodes and networks to ensure that the system remains operational during outages. We won't be able to create a general data store that is always available, sequentially consistent, and resilient to partition faults. Only two of these three traits can be combined in a system. Because all nodes should see the same set of changes in the same order in order to remain consistent. If the network is partitioned, however, updates in one partition may not reach the other partitions before a client reads from the out-of-date partition after reading from the current one. The only way to deal with this issue is to stop servicing requests from the out-of-date partition, however this reduces the service's availability to zero.","title":"CAP Theorem"},{"location":"Caching/","text":"Caching What is Cache and how does it work? A cache is a type of short-term memory with a finite quantity of storage space. It is often quicker than the original data source. Caching is made up of several steps. Perform a preliminary calculation (e.g. the number of visits from each referring domain for the previous day) Creating pricey indexes in advance (for example, suggested stories based on a user's click history) Using a quicker backend (e.g. Memcache instead of PostgreSQL) to store copies of frequently accessed data. Caches in several tiers 1 Client-side Use case: Improve the speed with which web content is retrieved from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Improve the speed with which web material is retrieved from web/app servers. Web Session Management (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Improve the performance of your applications and data access. Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: It essentially stores a cache on the Application server. If local, cached data exists, the node will promptly return it when a request is made to the service. If not, the data will be queried by the requesting node by accessing network storage, such as a database. When the application server is expanded to a large number of nodes, the following concerns may arise: The load balancer distributes requests across the nodes at random. The same request can be sent to many nodes, resulting in cache misses. Additional storage is required since the same data will be stored in two or more nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Decrease the latency of database query requests Tech: Database buffers, Key/Value data stores Solutions: The database's default setup normally contains some level of caching, which is tuned for a generic use case. These variables can be tweaked for unique usage patterns, and Redis and Memcached can also be used to improve performance. 6 Content Distribution Network (CDN) Use case: Remove the burden of providing static media from your application servers by distributing it geographically. Solutions: Amazon CloudFront, Akamai Note: If your system isn't big enough for CDN, you can build it this way: Use a lightweight HTTP server to serve static material from a separate subdomain (e.g. Nginx, Apache). Switch this subdomain's DNS to a CDN layer. 7 Other Cache CPU Cache: Small memories on or near the CPU can operate at a faster pace than the main memory, which is much larger. Since the 1980s, most CPUs have used one or more caches, sometimes in a cascaded configuration; modern high-end embedded, desktop, and server microprocessors may have as many as six cache types (between levels and functions) Cache for the GPU Disk Cache: While CPU caches are often maintained fully by hardware, other caches are managed by a variety of software. The operating system kernel manages the page cache in main memory, which is an example of disk cache. Cache Invalidation If data is changed in the database, it should be invalidated in the cache; otherwise, the program may behave inconsistently. Write-through cache, Write-around cache, and Write-back cache are the three main types of caching systems. Cache invalidation is a technique for resolving this issue; there are three main schemes: Write-through cache: With this policy, data is simultaneously written to the cache and the appropriate database. Because the identical data is written in the permanent storage, we will have total data consistency between the cache and the store. This technique also assures that nothing is lost in the event of a breakdown, power outage, or other system failure. Although write through reduces the possibility of data loss, this technique has the disadvantage of increasing write latency because every write operation must be performed twice before providing success to the client. Write-around cache: This technique is similar to write through cache, except that data is written directly to permanent storage rather than traveling through the cache. This can help prevent the cache from being flooded with write operations that will not be re-read, but it also means that a read request for recently written data will result in a \"cache miss,\" requiring the data to be read from slower back-end storage and resulting in increased latency. Write-back cache: With this policy, data is written to the cache alone, and the client is notified as soon as it is complete. The persistent storage is written to at predetermined intervals or under specific conditions. For write-intensive applications, this results in low latency and high throughput; but, because the sole copy of the written data is in the cache, this speed comes with the danger of data loss in the event of a crash or other adverse event. Cache eviction policies Some of the most typical cache eviction policies are as follows: First In First Out (FIFO): The cache evicts the first block accessed first, regardless of how often or how many times it was previously accessed. Last In First Out (LIFO): The cache evicts the block that has been accessed the most recently first, regardless of how often or how many times it has been accessed previously. Least Recently Used (LRU): Tosses out the items that haven't been utilized in a long time first. Most Recently Used (MRU): Discards the most recently used objects first, as opposed to LRU. Least Frequently Used (LFU): Determines how frequently an item is required. The ones that are utilized the least are the first to be discarded. Random Replacement (RR): Selects a candidate item at random and discards it when space is needed. The following links include useful information regarding caching: Cache Introduction to architecting systems","title":"Caching"},{"location":"Caching/#caching","text":"","title":"Caching"},{"location":"Caching/#what-is-cache-and-how-does-it-work","text":"A cache is a type of short-term memory with a finite quantity of storage space. It is often quicker than the original data source. Caching is made up of several steps. Perform a preliminary calculation (e.g. the number of visits from each referring domain for the previous day) Creating pricey indexes in advance (for example, suggested stories based on a user's click history) Using a quicker backend (e.g. Memcache instead of PostgreSQL) to store copies of frequently accessed data.","title":"What is Cache and how does it work?"},{"location":"Caching/#caches-in-several-tiers","text":"1 Client-side Use case: Improve the speed with which web content is retrieved from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Improve the speed with which web material is retrieved from web/app servers. Web Session Management (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Improve the performance of your applications and data access. Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: It essentially stores a cache on the Application server. If local, cached data exists, the node will promptly return it when a request is made to the service. If not, the data will be queried by the requesting node by accessing network storage, such as a database. When the application server is expanded to a large number of nodes, the following concerns may arise: The load balancer distributes requests across the nodes at random. The same request can be sent to many nodes, resulting in cache misses. Additional storage is required since the same data will be stored in two or more nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Decrease the latency of database query requests Tech: Database buffers, Key/Value data stores Solutions: The database's default setup normally contains some level of caching, which is tuned for a generic use case. These variables can be tweaked for unique usage patterns, and Redis and Memcached can also be used to improve performance. 6 Content Distribution Network (CDN) Use case: Remove the burden of providing static media from your application servers by distributing it geographically. Solutions: Amazon CloudFront, Akamai Note: If your system isn't big enough for CDN, you can build it this way: Use a lightweight HTTP server to serve static material from a separate subdomain (e.g. Nginx, Apache). Switch this subdomain's DNS to a CDN layer. 7 Other Cache CPU Cache: Small memories on or near the CPU can operate at a faster pace than the main memory, which is much larger. Since the 1980s, most CPUs have used one or more caches, sometimes in a cascaded configuration; modern high-end embedded, desktop, and server microprocessors may have as many as six cache types (between levels and functions) Cache for the GPU Disk Cache: While CPU caches are often maintained fully by hardware, other caches are managed by a variety of software. The operating system kernel manages the page cache in main memory, which is an example of disk cache.","title":"Caches in several tiers"},{"location":"Caching/#cache-invalidation","text":"If data is changed in the database, it should be invalidated in the cache; otherwise, the program may behave inconsistently. Write-through cache, Write-around cache, and Write-back cache are the three main types of caching systems. Cache invalidation is a technique for resolving this issue; there are three main schemes: Write-through cache: With this policy, data is simultaneously written to the cache and the appropriate database. Because the identical data is written in the permanent storage, we will have total data consistency between the cache and the store. This technique also assures that nothing is lost in the event of a breakdown, power outage, or other system failure. Although write through reduces the possibility of data loss, this technique has the disadvantage of increasing write latency because every write operation must be performed twice before providing success to the client. Write-around cache: This technique is similar to write through cache, except that data is written directly to permanent storage rather than traveling through the cache. This can help prevent the cache from being flooded with write operations that will not be re-read, but it also means that a read request for recently written data will result in a \"cache miss,\" requiring the data to be read from slower back-end storage and resulting in increased latency. Write-back cache: With this policy, data is written to the cache alone, and the client is notified as soon as it is complete. The persistent storage is written to at predetermined intervals or under specific conditions. For write-intensive applications, this results in low latency and high throughput; but, because the sole copy of the written data is in the cache, this speed comes with the danger of data loss in the event of a crash or other adverse event.","title":"Cache Invalidation"},{"location":"Caching/#cache-eviction-policies","text":"Some of the most typical cache eviction policies are as follows: First In First Out (FIFO): The cache evicts the first block accessed first, regardless of how often or how many times it was previously accessed. Last In First Out (LIFO): The cache evicts the block that has been accessed the most recently first, regardless of how often or how many times it has been accessed previously. Least Recently Used (LRU): Tosses out the items that haven't been utilized in a long time first. Most Recently Used (MRU): Discards the most recently used objects first, as opposed to LRU. Least Frequently Used (LFU): Determines how frequently an item is required. The ones that are utilized the least are the first to be discarded. Random Replacement (RR): Selects a candidate item at random and discards it when space is needed. The following links include useful information regarding caching: Cache Introduction to architecting systems","title":"Cache eviction policies"},{"location":"ConsistentHashing/","text":"Consistent Hashing One of the most important components in distributed scalable systems is the Distributed Hash Table (DHT). Hash tables require a key, a value, and a hash function, which maps the key to a storage place for the value. index = hash_function(key) Let's pretend we're working on a distributed caching system. An sensible hash function for 'n' cache servers would be 'key percent n'. It's straightforward and widely utilized. However, it has two fundamental flaws: It is not scalable horizontally. All current mappings are broken whenever a new cache host is introduced to the system. If the caching system has a lot of data, it will be difficult to maintain. In practice, scheduling a downtime to update all caching mappings becomes problematic. It may not be load balanced, especially if the data is not spread consistently. In practice, it is reasonable to expect that the data will not be dispersed evenly. For the caching system, this means that some caches will become hot and saturated while others will remain inactive and nearly empty. Consistent hashing is an excellent method to improve the caching system in these scenarios. What is Consistent Hashing, and how does it work? For distributed caching systems and DHTs, consistent hashing is a particularly beneficial method. It allows us to distribute data across a cluster in such a way that nodes are added or withdrawn with minimal rearrangement. As a result, scaling up or down the caching system will be much easier. Only 'k/n' keys need to be remapped in Consistent Hashing when the hash table is resized (e.g. a new cache host is added to the system), where 'k' is the total number of keys and 'n' is the total number of servers. Remember that in a caching system that uses the hash function'mod,' all keys must be remapped. If possible, items in Consistent Hashing are mapped to the same host. When a host is removed from the system, its objects are shared by other hosts; when a new host is introduced, it takes its share from a few hosts without affecting the shares of others. How does it function? Consistent hashing is a hash function that maps a key to an integer. Assume the hash function's output is in the range [0, 256]. Consider placing the integers in the range on a ring and wrapping the values around it. The following is an example of how consistent hashing works: Hash a list of cache servers into integers in the range given. To associate a key with a server, a. Convert it to a single integer hash. b. Work your way around the ring clockwise until you reach the first cache. c. That cache is the one that contains the key. As an example, consider the following: Key1 corresponds to cache A, while key2 corresponds to cache C. When a new server, say D, is added, keys that were previously stored on C will be divided. Some will be transferred to D, while others will remain unchanged. To remove a cache or if a cache fails, say A, all keys mapped to A will fall into B, and only those keys will need to be relocated to B; other keys will not be affected. As we noted in the outset, real data for load balancing is fundamentally randomly distributed and hence may not be uniform. It's possible that the keys on caches will become imbalanced as a result. We add \"virtual replicas\" for caches to address this issue. We map each cache to many points on the ring, i.e. replicas, rather than a single point on the ring. As a result, each cache is linked to numerous ring segments. If the hash function \"mixes well,\" the keys will become more balanced as the number of replicates grows.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#consistent-hashing","text":"One of the most important components in distributed scalable systems is the Distributed Hash Table (DHT). Hash tables require a key, a value, and a hash function, which maps the key to a storage place for the value. index = hash_function(key) Let's pretend we're working on a distributed caching system. An sensible hash function for 'n' cache servers would be 'key percent n'. It's straightforward and widely utilized. However, it has two fundamental flaws: It is not scalable horizontally. All current mappings are broken whenever a new cache host is introduced to the system. If the caching system has a lot of data, it will be difficult to maintain. In practice, scheduling a downtime to update all caching mappings becomes problematic. It may not be load balanced, especially if the data is not spread consistently. In practice, it is reasonable to expect that the data will not be dispersed evenly. For the caching system, this means that some caches will become hot and saturated while others will remain inactive and nearly empty. Consistent hashing is an excellent method to improve the caching system in these scenarios.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#what-is-consistent-hashing-and-how-does-it-work","text":"For distributed caching systems and DHTs, consistent hashing is a particularly beneficial method. It allows us to distribute data across a cluster in such a way that nodes are added or withdrawn with minimal rearrangement. As a result, scaling up or down the caching system will be much easier. Only 'k/n' keys need to be remapped in Consistent Hashing when the hash table is resized (e.g. a new cache host is added to the system), where 'k' is the total number of keys and 'n' is the total number of servers. Remember that in a caching system that uses the hash function'mod,' all keys must be remapped. If possible, items in Consistent Hashing are mapped to the same host. When a host is removed from the system, its objects are shared by other hosts; when a new host is introduced, it takes its share from a few hosts without affecting the shares of others.","title":"What is Consistent Hashing, and how does it work?"},{"location":"ConsistentHashing/#how-does-it-function","text":"Consistent hashing is a hash function that maps a key to an integer. Assume the hash function's output is in the range [0, 256]. Consider placing the integers in the range on a ring and wrapping the values around it. The following is an example of how consistent hashing works: Hash a list of cache servers into integers in the range given. To associate a key with a server, a. Convert it to a single integer hash. b. Work your way around the ring clockwise until you reach the first cache. c. That cache is the one that contains the key. As an example, consider the following: Key1 corresponds to cache A, while key2 corresponds to cache C. When a new server, say D, is added, keys that were previously stored on C will be divided. Some will be transferred to D, while others will remain unchanged. To remove a cache or if a cache fails, say A, all keys mapped to A will fall into B, and only those keys will need to be relocated to B; other keys will not be affected. As we noted in the outset, real data for load balancing is fundamentally randomly distributed and hence may not be uniform. It's possible that the keys on caches will become imbalanced as a result. We add \"virtual replicas\" for caches to address this issue. We map each cache to many points on the ring, i.e. replicas, rather than a single point on the ring. As a result, each cache is linked to numerous ring segments. If the hash function \"mixes well,\" the keys will become more balanced as the number of replicates grows.","title":"How does it function?"},{"location":"DataPartitioning/","text":"Data Partitioning Data partitioning is a method of dividing a large database (DB) into multiple smaller sections. It is the process of dividing a database/table among numerous machines in order to increase an application's manageability, performance, availability, and load balancing. The reason for data partitioning is that after a certain scalability point, scaling horizontally by adding more machines is cheaper and more viable than scaling vertically by adding beefier servers. Partitioning Methods There are a variety of approaches that can be used to divide an application database into many smaller databases. Three of the most common methods employed by various large-scale applications are listed below. Horizontal partitioning is a technique for dividing a space horizontally (often called sharding). Each partition is a separate data store in this strategy, although they all share the same schema. A shard is a partition that holds a specific subset of data, for as all orders for a certain set of clients. Vertical Partitioning is a term that refers to the division of space vertically. Each partition in this strategy stores a subset of the fields for objects in the data store. The fields are separated into groups based on how they are used. For example, frequently accessible fields could be grouped together in one vertical division while less frequently visited fields are grouped together in another. Functional partitioning Data is aggregated in this technique based on how each bounded context in the system uses it. An e-commerce system, for example, might keep invoice data in one partition and product inventory data in the other. Directory Based Partitioning Creating a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code is a loosely connected solution to work around concerns highlighted in the above schemes. To determine the location of a specific data entity, we query the directory server that stores the mapping between each tuple key and its DB server. Because of this loose coupling, we can conduct operations such as adding servers to the DB pool or modifying our partitioning scheme without affecting the application. Partitioning Criteria a. Key or Hash-based partitioning: We use a hash function on some key features of the item we're storing to generate a partition number. b. List partitioning: Each partition has a set of values allocated to it, so if we want to add a new record, we'll look to see which partition holds our key and save it there. For example, we could specify that all users from Iceland, Norway, Sweden, Finland, or Denmark should be stored in a Nordic division. c. Round-robin partitioning: This is a straightforward approach for ensuring data distribution consistency. The I tuple is assigned to partition with 'n' partitions (i mod n). d. Composite partitioning: We combine any of the aforementioned partitioning schemes to create a new scheme in this scheme. For instance, apply a list partitioning strategy first, then a hash-based partitioning method. Consistent hashing is a hybrid of hash and list partitioning, in which the hash shrinks the key space to a size that can be listed.","title":"Data Partitioning"},{"location":"DataPartitioning/#data-partitioning","text":"Data partitioning is a method of dividing a large database (DB) into multiple smaller sections. It is the process of dividing a database/table among numerous machines in order to increase an application's manageability, performance, availability, and load balancing. The reason for data partitioning is that after a certain scalability point, scaling horizontally by adding more machines is cheaper and more viable than scaling vertically by adding beefier servers.","title":"Data Partitioning"},{"location":"DataPartitioning/#partitioning-methods","text":"There are a variety of approaches that can be used to divide an application database into many smaller databases. Three of the most common methods employed by various large-scale applications are listed below. Horizontal partitioning is a technique for dividing a space horizontally (often called sharding). Each partition is a separate data store in this strategy, although they all share the same schema. A shard is a partition that holds a specific subset of data, for as all orders for a certain set of clients. Vertical Partitioning is a term that refers to the division of space vertically. Each partition in this strategy stores a subset of the fields for objects in the data store. The fields are separated into groups based on how they are used. For example, frequently accessible fields could be grouped together in one vertical division while less frequently visited fields are grouped together in another. Functional partitioning Data is aggregated in this technique based on how each bounded context in the system uses it. An e-commerce system, for example, might keep invoice data in one partition and product inventory data in the other. Directory Based Partitioning Creating a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code is a loosely connected solution to work around concerns highlighted in the above schemes. To determine the location of a specific data entity, we query the directory server that stores the mapping between each tuple key and its DB server. Because of this loose coupling, we can conduct operations such as adding servers to the DB pool or modifying our partitioning scheme without affecting the application.","title":"Partitioning Methods"},{"location":"DataPartitioning/#partitioning-criteria","text":"a. Key or Hash-based partitioning: We use a hash function on some key features of the item we're storing to generate a partition number. b. List partitioning: Each partition has a set of values allocated to it, so if we want to add a new record, we'll look to see which partition holds our key and save it there. For example, we could specify that all users from Iceland, Norway, Sweden, Finland, or Denmark should be stored in a Nordic division. c. Round-robin partitioning: This is a straightforward approach for ensuring data distribution consistency. The I tuple is assigned to partition with 'n' partitions (i mod n). d. Composite partitioning: We combine any of the aforementioned partitioning schemes to create a new scheme in this scheme. For instance, apply a list partitioning strategy first, then a hash-based partitioning method. Consistent hashing is a hybrid of hash and list partitioning, in which the hash shrinks the key space to a size that can be listed.","title":"Partitioning Criteria"},{"location":"DesignCache/","text":"Design a distributed key value caching system, like Memcached or Redis. Features: This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote. Q: What is the amount of data that we need to cache? A: Let's assume we are looking to cache on the scale of Google or Twitter. The total size of the cache would be a few TBs. Q: What should be the eviction strategy? A: It is possible that we might get entries when we would not have space to accommodate new entries. In such cases, we would need to remove one or more entries to make space for the new entry. Which entry should we evict? There are multiple strategies possible. Following are a few standard ones : FIFO ( First in, first out ) : The entry that was first added to the queue would be evicted/removed first. In other words, Elements are evicted in the same order as they come in. This is the simplest to implement and performs well when either, access pattern is completely random ( All elements are equally probably to be accessed ) OR if the use of an element makes it less likely to be used in the future. LRU ( Least Recently Used ) : Default. As the name suggests, the element evicted is the ones which has the oldest last access time. The last accessed timestamp is updated when an element is put into the cache or an element is retrieved from the cache with a get call. This is by far the most popular eviction strategy. LFU ( Least Frequently Used ) : Again, as the name suggests, every entry has a frequency associated with it. At the time of eviction, the entry with lowest frequency is evicted. This is most effective in cases when most of access is limited to a very small portion of the data ( pareto distribution ). As you can see, every eviction strategy has its own set of cases when they are most effective. The choice of which eviction strategy to choose is largely dependent on the expected access pattern. We will go with the default here which is LRU. Q. What should be the access pattern for the given cache? A: There are majorly three kinds of caching systems : Write through cache : This is a caching system where writes go through the cache and write is confirmed as success only if writes to DB and the cache BOTH succeed. This is really useful for applications which write and re-read the information quickly. However, write latency will be higher in this case as there are writes to 2 separate systems. Write around cache : This is a caching system where write directly goes to the DB. The cache system reads the information from DB incase of a miss. While this ensures lower write load to the cache and faster writes, this can lead to higher read latency incase of applications which write and re-read the information quickly. Write back cache : This is a caching system where the write is directly done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the DB. This would lead to a really quick write latency and high write throughput. But, as is the case with any non-persistent / in-memory write, we stand the risk of losing the data incase the caching layer dies. We can improve our odds by introducing having more than one replica acknowledging the write ( so that we don\u2019t lose data if just one of the replica dies ). Estimations: This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier. Q: What is the kind of QPS we expect for the system? A: This estimation is important to understand the number of machines we will need to answer the queries. For example, if our estimations state that a single machine is going to handle 1M QPS, we run into a high risk of high latency / the machine dying because of queries not being answered fast enough and hence ending up in the backlog queue. Again, let's assume the scale of Twitter / Google. We can expect around 10M QPS if not more. Q: What is the number of machines required to cache? A: A cache has to be inherently of low latency. Which means all cache data has to reside in main memory. A production level caching machine would be 72G or 144G of RAM. Assuming beefier cache machines, we have 72G of main memory for 1 machine. Min. number of machine required = 30 TB / 72G which is close to 420 machines. Do know that this is the absolute minimum. Its possible we might need more machines because the QPS per machine is higher than we want it to be. Design Goals: Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other. Q: Is Latency a very important metric for us? A: Yes. The whole point of caching is low latency. Q: Consistency vs Availability? A: Unavailability in a caching system means that the caching machine goes down, which in turn means that we have a cache miss which leads to a high latency. As said before, we are caching for a Twitter / Google like system. When fetching a timeline for a user, I would be okay if I miss on a few tweets which were very recently posted as long as I eventually see them in reasonable time. Unavailability could lead to latency spikes and increased load on DB. Choosing from consistency and availability, we should prioritize for availability. Skeleton of the design: The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction. Deep Dive: Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes). Q: How would a LRU cache work on a single machine which is single threaded? Q: What if we never had to remove entries from the LRU cache because we had enough space, what would you use to support and get and set? A: A simple map / hashmap would suffice. Q: How should we modify our approach if we also have to evict keys at some stage? A: We need a data structure which at any given instance can give me the least recently used objects in order. Let's see if we can maintain a linked list to do it. We try to keep the list ordered by the order in which they are used. So whenever, a get operation happens, we would need to move that object from a certain position in the list to the front of the list. Which means a delete followed by insert at the beginning. Insert at the beginning of the list is trivial. How do we achieve erase of the object from a random position in least time possible? How about we maintain another map which stores the value to the corresponding linked list node. Ok, now when we know the node, we would need to know its previous and next node in the list to enable the deletion of the node from the list. We can get the next in the list from next pointer ? What about the previous node ? To encounter that, we make the list doubly linked list. A: Since we only have one thread to work with, we cannot do things in parallel. So we will take a simple approach and implement a LRU cache using a linked list and a map. The Map stores the value to the corresponding linked list node and is useful to move the recently accessed node to the front of the list. Q: How would a LRU cache work on a single machine which is multi threaded ? Q: How would you break down cache write and read into multiple instructions? A: Read path : Read a value corresponding to a key. This requires : Operation 1 : A read from the HashMap and then, Operation 2 : An update in the doubly LinkedList Write path : Insert a new key-value entry to the LRU cache. This requires : If the cache is full, then Operation 3: Figure out the least recently used item from the linkedList Operation 4: Remove it from the hashMap Operation 5: Remove the entry from the linkedList. Operation 6: Insert the new item in the hashMap Operation 7: Insert the new item in the linkedList. Q: How would you prioritize above operations to keep latency to a minimum for our system? A: As is the case with most concurrent systems, writes compete with reads and other writes. That requires some form of locking when a write is in progress. We can choose to have writes as granular as possible to help with performance. Read path is going to be highly frequent. As latency is our design goal, Operation 1 needs to be really fast and should require minimum locks. Operation 2 can happen asynchronously. Similarly, all of the write path can happen asynchronously and the client\u2019s latency need not be affected by anything other than Operation 1. Let's dig deeper into Operation 1. What are the things that Hashmap is dealing with? Hashmap deals with Operation 1, 4 and 6 with Operation 4 and 6 being write operations. One simple, but not so efficient way of handling read/write would be to acquire a higher level Read lock for Operation 1 and Write lock for Operation 4 and 6. However, Operation 1 as stressed earlier is the most frequent ( by a huge margin ) operation and its performance is critical to how our caching system works. Q: How would you implement HashMap? A: The HashMap itself could be implemented in multiple ways. One common way could be hashing with linked list (colliding values linked together in a linkedList) : Let's say our hashmap size is N and we wish to add (k,v) to it Let H = size N array of pointers with every element initialized to NULL For a given key k, generate g = hash(k) % N newEntry = LinkedList Node with value = v newEntry.next = H[g] H[g] = newEntry More details at https://en.wikipedia.org/wiki/Hash_table Given this implementation, we can see that instead of having a lock on a hashmap level, we can have it for every single row. This way, a read for row i and a write for row j would not affect each other if i != j. Note that we would try to keep N as high as possible here to increase granularity.","title":"Design a distributed key value caching system, like Memcached or Redis."},{"location":"DesignCache/#design-a-distributed-key-value-caching-system-like-memcached-or-redis","text":"","title":"Design a distributed key value caching system, like Memcached or Redis."},{"location":"DesignCache/#features","text":"This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote.","title":"Features:"},{"location":"DesignCache/#estimations","text":"This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier.","title":"Estimations:"},{"location":"DesignCache/#design-goals","text":"Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other.","title":"Design Goals:"},{"location":"DesignCache/#skeleton-of-the-design","text":"The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction.","title":"Skeleton of the design:"},{"location":"DesignCache/#deep-dive","text":"Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes).","title":"Deep Dive:"},{"location":"DesignTicketmaster/","text":"Design Ticketmaster Problem Statement Let's design an online ticketing system that sells movie tickets like Ticketmaster or BookMyShow. Similar Services: bookmyshow.com, ticketmaster.com Difficulty Level: Hard What is an online movie ticket booking system? Customers can acquire theater seats online through a movie ticket reservation system. Customers can browse and book seats for presently playing movies using e-ticketing systems from anywhere at any time. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Our ticket booking service should meet the following requirements: Functional Requirements: Our ticketing service should be able to provide a list of cities in which its affiliated cinemas are located. After the user selects a city, the service should show all movies that have been released in that city. After the user selects a film, the service should list the theaters showing that film as well as show times. The user should be able to select a show and purchase tickets at a certain theatre. The service should be able to show the user the cinema's seating configuration. The user should have the option of selecting multiple seats based on their preferences. The user should be able to tell the difference between available and booked seats. Users should be allowed to place a five-minute hold on seats before making a payment to complete the booking. If there is a potential chance that the seats will become available, such as when other users' holds expire, the user should be permitted to wait. Customers who are in line shall be served in a fair, first-come, first-served manner. Non-Functional Requirements: The system must be extremely concurrent. At any given time, there will be many booking requests for the same seat. This should be handled gracefully and fairly by the service. The service's main feature is ticketing, which entails financial transactions. This means the system must be secure and the database must be ACID-compliant. Some Design Considerations Assume that our service does not require any user authentication for the sake of simplicity. Partial ticket orders will not be accepted by the system. Either the user receives all of the tickets they desire, or they do not. The system is required to be fair. To prevent exploitation of the system, we can limit users to booking no more than ten seats at a time. We should expect crowds to surge for popular/anticipated film releases, with tickets filling up quickly. To keep up with the increased traffic, the system should be scalable and highly available. Capacity Estimation Traffic estimates: Assume that our service receives 3 billion monthly page views and sells 10 million tickets. Storage estimates: Assume there are 500 cities and each city has ten cinemas on average. If each cinema has 2000 seats and there are two shows every day on average. Assume that each seat booking requires 50 bytes in the database (IDs, NumberOfSeats, ShowID, MovieID, SeatNumbers, SeatStatus, Timestamp, and so on). We'd also need to save information on movies and theaters, which we'll suppose will take up 50 bytes. To store all data for a day about all shows in all cinemas in all cities: 500 cities * 10 cinemas * 2000 seats * 2 shows * (50+50) bytes = 2GB / day To store five years of this data, we would need storage of around 3.6TB. System APIs To expose the functionality of our service, we can use SOAP or REST APIs. The APIs to search movie shows and reserve seats may be defined as follows. SearchMovies(api_dev_key, keyword, city, lat_long, radius, start_datetime, end_datetime, postal_code, includeSpellcheck, results_per_page, sorting_order) Parameters: api_dev_key (string): A registered account's API developer key. This will be used to throttle users based on their quota allocation, among other things. keyword (string): Keyword to search with. city (string): City filter to search movies. lat_long (string): Latitude and longitude to filter by. radius (number): Radius of the area in which we want to search for events. start_datetime (string): Filter movies with a starting date and time. end_datetime (string): Filter movies with an ending date and time. postal_code (string): Filter movies by postal code / zipcode. includeSpellcheck (Enum: \u201cyes\u201d or \u201cno\u201d): Yes, to include spell check suggestions in the response. results_per_page (number): Number of results to return per page. Maximum is 30. sorting_order (string): Sorting order of the search result. Some allowable values : \u2018name,asc\u2019, \u2018name,desc\u2019, \u2018date,asc\u2019, \u2018date,desc\u2019, \u2018distance,asc\u2019, \u2018name,date,asc\u2019, \u2018name,date,desc\u2019, \u2018date,name,asc\u2019, \u2018date,name,desc\u2019. Returns: (JSON) Here is a sample list of movies and their shows: [ { \"MovieID\": 1, \"ShowID\": 1, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"14:00\", \"EndTime\": \"16:00\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Almost Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Available\" } ] }, { \"MovieID\": 1, \"ShowID\": 2, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"16:30\", \"EndTime\": \"18:30\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Almost Full\" } ] }, ] ReserveSeats(api_dev_key, session_id, movie_id, show_id, seats_to_reserve[]) Parameters: - api_dev_key (string): same as above - session_id (string): This reservation will be tracked using the user's session ID. When the reservation time expires, this ID will be used to delete the user's reservation from the server. - movie_id (string): Movie to reserve. - show_id (string): Show to reserve. - seats_to_reserve (number): An array containing seat IDs to reserve. Returns: (JSON) Returns the reservation's status, which can be one of the following: 1) \"Successful Reservation\" 2) \"Show Full - Reservation Failed,\" 3) \"Reservation Failed - Please try again as other users have reserved seats.\" Database Design Here are a few key observations about the data we are going to store: There are several cinemas in each city. There will be many halls in each cinema. Each film will have numerous screenings, and each screening will have multiple bookings. A user may make several reservations. High Level Design At a high level, our web servers will manage user sessions, while application servers will handle all ticket administration, data storage in databases, and reservation processing in collaboration with cache servers. Detailed Component Design First, let\u2019s try to build our service assuming it is being served from a single server. Ticket Booking Workflow: The following would be a typical ticket booking workflow: The user looks for a film to watch. The user chooses a film. The user is shown the movie's available shows. The user chooses a show to watch. The quantity of seats to be reserved is chosen by the user. If the appropriate number of seats are available, the user is presented with a theater map from which to choose seats. Otherwise, the user is directed to'step 8' below. Once the user has chosen a seat, the system will attempt to reserve that seat. If seats cannot be reserved, the following options are available: The show is full, and the user receives an error notice. The user's desired seats are no longer available, but there are alternative seats available, thus the user is returned to the theater map to select different seats. There are no seats available for reservation, but all seats are not yet booked since other users are holding seats in the reservation pool that they have not yet booked. The user will be directed to a website where they can wait for the appropriate seats to become available in the reservation pool. Waiting could lead to the following options: - If enough seats become available, the user is directed to the theater map page, where they can select seats. - While waiting, the user is shown an error message if all seats are booked or there are fewer seats in the reservation pool than the user intended to book. - When the user quits the waiting, they are returned to the movie search page. - A user can wait a maximum of one hour before their session expires and they are returned to the movie search page. If tickets are successfully reserved, the user has five minutes to complete the payment process. The booking is marked finalized after payment is received. If a user does not pay within five minutes, all of their reserved seats become available to other users. How would the server keep track of all the active reservation that haven\u2019t been booked yet? And how would the server keep track of all the waiting customers? Let's call one of the daemon services ActiveReservationService to maintain track of all active bookings and remove any expired reservations from the system. The other service, which we'll call WaitingUserService, will keep track of all the waiting user requests and, as soon as the required number of seats become available, it will tell the (longest waiting) user to choose the seats. a. ActiveReservationsService In addition to keeping all the data in the database, we can keep all the reservations for a'show' in memory in a data structure similar to Linked HashMap or TreeMap. We'll need a linked HashMap-style data structure that allows us to jump to any reservation and remove it after it's finished. Also, because each reservation will have an expiry time, the HashMap's head will always point to the oldest reservation record, allowing the reservation to expire when the timeout is reached. To keep each reservation for each show, we may use a HashTable with the 'ShowID' as the key and the Linked HashMap containing 'BookingID' and the creation 'Timestamp' as the value. The reservation will be stored in the database's 'Booking' table, with the expiration time in the Timestamp field. The 'Status' field will be set to 'Reserved (1),' and if a booking is completed, the system will change the 'Status' to 'Booked (2),' and remove the reservation record from the relevant show's Linked HashMap. We can either erase the reservation from the Booking table or mark it as 'Expired (3)' in addition to removing it from memory when it has expired. In order to process user payments, ActiveReservationsService will collaborate with an external financial service. WaitingUsersService will get a signal if a booking is completed or a reservation expires, allowing any waiting customers to be served. b. WaitingUsersService We can retain all the waiting users of a show in memory in a Linked HashMap or a TreeMap, just like ActiveReservationsService. When a user cancels their request, we require a data structure similar to Linked HashMap so that we can jump to any user and remove them from the HashMap. Also, because we are serving on a first-come, first-serve basis, the Linked HashMap's head would always point to the longest-waiting user, allowing us to serve users fairly whenever seats become available. A HashTable will be used to keep all of the waiting users for each Show. 'ShowID' would be the 'key,' and a Linked HashMap containing 'UserIDs' and their wait-start-time would be the 'value.' Long Polling allows customers to stay informed about the status of their reservations. The server can utilize this request to notify the user when seats become available. Reservation Expiration ActiveReservationsService on the server maintains track of when active reservations expire (based on reservation time). Because the client will see a timer (for the expiration time), which may be out of sync with the server, we may put a five-second buffer on the server to protect against a broken experience, ensuring that the client never times out before the server, preventing a successful transaction. Concurrency How to deal with concurrency so that no two people may book the same seat. To avoid conflicts in SQL databases, we can utilize transactions. If we're using a SQL server, for example, we can use Transaction Isolation Levels to lock records before updating them. Here's an example of code: SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; BEGIN TRANSACTION; -- Suppose we intend to reserve three seats (IDs: 54, 55, 56) for ShowID=99 Select * From Show_Seat where ShowID=99 && ShowSeatID in (54, 55, 56) && Status=0 -- free -- if the number of rows returned by the above statement is three, we can update to -- return success otherwise return failure to the user. update Show_Seat ... update Booking ... COMMIT TRANSACTION; The greatest isolation level is 'Serializable,' which ensures protection from Dirty, Nonrepeatable, and Phantoms readings. One thing to keep in mind here: when we read rows within a transaction, we obtain a write lock on them, which means no one else can update them. We may start tracking the reservation in ActiveReservationService after the above database transaction is complete. Fault Tolerance What happens when ActiveReservationsService or WaitingUsersService crashes? We can read all active reservations from the 'Booking' database whenever ActiveReservationsService dies. Remember that until a reservation is booked, the 'Status' column will remain 'Reserved (1).' Another alternative is to set up a master-slave arrangement so that the slave can take over if the master fails. We don't store the waiting users in the database, therefore unless we have a master-slave configuration, we won't be able to retrieve that data if WaitingUsersService crashes. To make databases fault resilient, we'll use a master-slave configuration. Data Partitioning Database partitioning: If we partition by 'MovieID,' all of a movie's Shows will be on the same server. This could put a lot of strain on the server if the movie is really popular. A better technique would be to divide depending on ShowID, which would distribute the load across multiple servers. ActiveReservationService and WaitingUserService partitioning: - Our web servers will keep track of all active users' sessions and handle all user contact. Based on the 'ShowID,' we may utilize Consistent Hashing to allocate application servers for both ActiveReservationService and WaitingUserService. - This way, a specific set of servers will manage all bookings and waiting users for a specific show. Assume that our Consistent Hashing allocates three servers for each Show for load balancing purposes, therefore when a reservation expires, the server holding that reservation will do the following: Update the 'Show Seats' table in the database to remove the Booking (or mark it as expired) and update the seats' Status. Get rid of the reservation in the Linked HashMap. Inform the user that their reservation has run out of time. To determine the longest waiting user, send a message to all WaitingUserService servers that are holding waiting users for that Show. A reliable hashing algorithm will reveal which servers are hosting these users. If required seats become available, send a message to the WaitingUserService server holding the longest waiting user to execute their request. When a reservation is confirmed, the following events occur: The server holding that reservation sends a message to all servers holding the Show's waiting users, instructing them to expire all waiting users who require more seats than are available. When all servers holding waiting users receive the aforementioned message, they will query the database to see how many open seats are currently available. To run this query only once, a database cache would be really useful. All waiting users who try to reserve more seats than are available will be removed from the system. WaitingUserService must cycle through the Linked HashMap of all the waiting users to do this.","title":"Design Ticketmaster"},{"location":"DesignTicketmaster/#design-ticketmaster","text":"","title":"Design Ticketmaster"},{"location":"DesignTicketmaster/#problem-statement","text":"Let's design an online ticketing system that sells movie tickets like Ticketmaster or BookMyShow. Similar Services: bookmyshow.com, ticketmaster.com Difficulty Level: Hard","title":"Problem Statement"},{"location":"DesignTicketmaster/#what-is-an-online-movie-ticket-booking-system","text":"Customers can acquire theater seats online through a movie ticket reservation system. Customers can browse and book seats for presently playing movies using e-ticketing systems from anywhere at any time.","title":"What is an online movie ticket booking system?"},{"location":"DesignTicketmaster/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesignTicketmaster/#solution","text":"","title":"Solution"},{"location":"DesignTicketmaster/#requirements-and-goals-of-the-system","text":"Our ticket booking service should meet the following requirements: Functional Requirements: Our ticketing service should be able to provide a list of cities in which its affiliated cinemas are located. After the user selects a city, the service should show all movies that have been released in that city. After the user selects a film, the service should list the theaters showing that film as well as show times. The user should be able to select a show and purchase tickets at a certain theatre. The service should be able to show the user the cinema's seating configuration. The user should have the option of selecting multiple seats based on their preferences. The user should be able to tell the difference between available and booked seats. Users should be allowed to place a five-minute hold on seats before making a payment to complete the booking. If there is a potential chance that the seats will become available, such as when other users' holds expire, the user should be permitted to wait. Customers who are in line shall be served in a fair, first-come, first-served manner. Non-Functional Requirements: The system must be extremely concurrent. At any given time, there will be many booking requests for the same seat. This should be handled gracefully and fairly by the service. The service's main feature is ticketing, which entails financial transactions. This means the system must be secure and the database must be ACID-compliant.","title":"Requirements and Goals of the System"},{"location":"DesignTicketmaster/#some-design-considerations","text":"Assume that our service does not require any user authentication for the sake of simplicity. Partial ticket orders will not be accepted by the system. Either the user receives all of the tickets they desire, or they do not. The system is required to be fair. To prevent exploitation of the system, we can limit users to booking no more than ten seats at a time. We should expect crowds to surge for popular/anticipated film releases, with tickets filling up quickly. To keep up with the increased traffic, the system should be scalable and highly available.","title":"Some Design Considerations"},{"location":"DesignTicketmaster/#capacity-estimation","text":"Traffic estimates: Assume that our service receives 3 billion monthly page views and sells 10 million tickets. Storage estimates: Assume there are 500 cities and each city has ten cinemas on average. If each cinema has 2000 seats and there are two shows every day on average. Assume that each seat booking requires 50 bytes in the database (IDs, NumberOfSeats, ShowID, MovieID, SeatNumbers, SeatStatus, Timestamp, and so on). We'd also need to save information on movies and theaters, which we'll suppose will take up 50 bytes. To store all data for a day about all shows in all cinemas in all cities: 500 cities * 10 cinemas * 2000 seats * 2 shows * (50+50) bytes = 2GB / day To store five years of this data, we would need storage of around 3.6TB.","title":"Capacity Estimation"},{"location":"DesignTicketmaster/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs. The APIs to search movie shows and reserve seats may be defined as follows. SearchMovies(api_dev_key, keyword, city, lat_long, radius, start_datetime, end_datetime, postal_code, includeSpellcheck, results_per_page, sorting_order) Parameters: api_dev_key (string): A registered account's API developer key. This will be used to throttle users based on their quota allocation, among other things. keyword (string): Keyword to search with. city (string): City filter to search movies. lat_long (string): Latitude and longitude to filter by. radius (number): Radius of the area in which we want to search for events. start_datetime (string): Filter movies with a starting date and time. end_datetime (string): Filter movies with an ending date and time. postal_code (string): Filter movies by postal code / zipcode. includeSpellcheck (Enum: \u201cyes\u201d or \u201cno\u201d): Yes, to include spell check suggestions in the response. results_per_page (number): Number of results to return per page. Maximum is 30. sorting_order (string): Sorting order of the search result. Some allowable values : \u2018name,asc\u2019, \u2018name,desc\u2019, \u2018date,asc\u2019, \u2018date,desc\u2019, \u2018distance,asc\u2019, \u2018name,date,asc\u2019, \u2018name,date,desc\u2019, \u2018date,name,asc\u2019, \u2018date,name,desc\u2019. Returns: (JSON) Here is a sample list of movies and their shows: [ { \"MovieID\": 1, \"ShowID\": 1, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"14:00\", \"EndTime\": \"16:00\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Almost Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Available\" } ] }, { \"MovieID\": 1, \"ShowID\": 2, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"16:30\", \"EndTime\": \"18:30\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Almost Full\" } ] }, ] ReserveSeats(api_dev_key, session_id, movie_id, show_id, seats_to_reserve[]) Parameters: - api_dev_key (string): same as above - session_id (string): This reservation will be tracked using the user's session ID. When the reservation time expires, this ID will be used to delete the user's reservation from the server. - movie_id (string): Movie to reserve. - show_id (string): Show to reserve. - seats_to_reserve (number): An array containing seat IDs to reserve. Returns: (JSON) Returns the reservation's status, which can be one of the following: 1) \"Successful Reservation\" 2) \"Show Full - Reservation Failed,\" 3) \"Reservation Failed - Please try again as other users have reserved seats.\"","title":"System APIs"},{"location":"DesignTicketmaster/#database-design","text":"Here are a few key observations about the data we are going to store: There are several cinemas in each city. There will be many halls in each cinema. Each film will have numerous screenings, and each screening will have multiple bookings. A user may make several reservations.","title":"Database Design"},{"location":"DesignTicketmaster/#high-level-design","text":"At a high level, our web servers will manage user sessions, while application servers will handle all ticket administration, data storage in databases, and reservation processing in collaboration with cache servers.","title":"High Level Design"},{"location":"DesignTicketmaster/#detailed-component-design","text":"First, let\u2019s try to build our service assuming it is being served from a single server. Ticket Booking Workflow: The following would be a typical ticket booking workflow: The user looks for a film to watch. The user chooses a film. The user is shown the movie's available shows. The user chooses a show to watch. The quantity of seats to be reserved is chosen by the user. If the appropriate number of seats are available, the user is presented with a theater map from which to choose seats. Otherwise, the user is directed to'step 8' below. Once the user has chosen a seat, the system will attempt to reserve that seat. If seats cannot be reserved, the following options are available: The show is full, and the user receives an error notice. The user's desired seats are no longer available, but there are alternative seats available, thus the user is returned to the theater map to select different seats. There are no seats available for reservation, but all seats are not yet booked since other users are holding seats in the reservation pool that they have not yet booked. The user will be directed to a website where they can wait for the appropriate seats to become available in the reservation pool. Waiting could lead to the following options: - If enough seats become available, the user is directed to the theater map page, where they can select seats. - While waiting, the user is shown an error message if all seats are booked or there are fewer seats in the reservation pool than the user intended to book. - When the user quits the waiting, they are returned to the movie search page. - A user can wait a maximum of one hour before their session expires and they are returned to the movie search page. If tickets are successfully reserved, the user has five minutes to complete the payment process. The booking is marked finalized after payment is received. If a user does not pay within five minutes, all of their reserved seats become available to other users. How would the server keep track of all the active reservation that haven\u2019t been booked yet? And how would the server keep track of all the waiting customers? Let's call one of the daemon services ActiveReservationService to maintain track of all active bookings and remove any expired reservations from the system. The other service, which we'll call WaitingUserService, will keep track of all the waiting user requests and, as soon as the required number of seats become available, it will tell the (longest waiting) user to choose the seats. a. ActiveReservationsService In addition to keeping all the data in the database, we can keep all the reservations for a'show' in memory in a data structure similar to Linked HashMap or TreeMap. We'll need a linked HashMap-style data structure that allows us to jump to any reservation and remove it after it's finished. Also, because each reservation will have an expiry time, the HashMap's head will always point to the oldest reservation record, allowing the reservation to expire when the timeout is reached. To keep each reservation for each show, we may use a HashTable with the 'ShowID' as the key and the Linked HashMap containing 'BookingID' and the creation 'Timestamp' as the value. The reservation will be stored in the database's 'Booking' table, with the expiration time in the Timestamp field. The 'Status' field will be set to 'Reserved (1),' and if a booking is completed, the system will change the 'Status' to 'Booked (2),' and remove the reservation record from the relevant show's Linked HashMap. We can either erase the reservation from the Booking table or mark it as 'Expired (3)' in addition to removing it from memory when it has expired. In order to process user payments, ActiveReservationsService will collaborate with an external financial service. WaitingUsersService will get a signal if a booking is completed or a reservation expires, allowing any waiting customers to be served. b. WaitingUsersService We can retain all the waiting users of a show in memory in a Linked HashMap or a TreeMap, just like ActiveReservationsService. When a user cancels their request, we require a data structure similar to Linked HashMap so that we can jump to any user and remove them from the HashMap. Also, because we are serving on a first-come, first-serve basis, the Linked HashMap's head would always point to the longest-waiting user, allowing us to serve users fairly whenever seats become available. A HashTable will be used to keep all of the waiting users for each Show. 'ShowID' would be the 'key,' and a Linked HashMap containing 'UserIDs' and their wait-start-time would be the 'value.' Long Polling allows customers to stay informed about the status of their reservations. The server can utilize this request to notify the user when seats become available. Reservation Expiration ActiveReservationsService on the server maintains track of when active reservations expire (based on reservation time). Because the client will see a timer (for the expiration time), which may be out of sync with the server, we may put a five-second buffer on the server to protect against a broken experience, ensuring that the client never times out before the server, preventing a successful transaction.","title":"Detailed Component Design"},{"location":"DesignTicketmaster/#concurrency","text":"How to deal with concurrency so that no two people may book the same seat. To avoid conflicts in SQL databases, we can utilize transactions. If we're using a SQL server, for example, we can use Transaction Isolation Levels to lock records before updating them. Here's an example of code: SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; BEGIN TRANSACTION; -- Suppose we intend to reserve three seats (IDs: 54, 55, 56) for ShowID=99 Select * From Show_Seat where ShowID=99 && ShowSeatID in (54, 55, 56) && Status=0 -- free -- if the number of rows returned by the above statement is three, we can update to -- return success otherwise return failure to the user. update Show_Seat ... update Booking ... COMMIT TRANSACTION; The greatest isolation level is 'Serializable,' which ensures protection from Dirty, Nonrepeatable, and Phantoms readings. One thing to keep in mind here: when we read rows within a transaction, we obtain a write lock on them, which means no one else can update them. We may start tracking the reservation in ActiveReservationService after the above database transaction is complete.","title":"Concurrency"},{"location":"DesignTicketmaster/#fault-tolerance","text":"What happens when ActiveReservationsService or WaitingUsersService crashes? We can read all active reservations from the 'Booking' database whenever ActiveReservationsService dies. Remember that until a reservation is booked, the 'Status' column will remain 'Reserved (1).' Another alternative is to set up a master-slave arrangement so that the slave can take over if the master fails. We don't store the waiting users in the database, therefore unless we have a master-slave configuration, we won't be able to retrieve that data if WaitingUsersService crashes. To make databases fault resilient, we'll use a master-slave configuration.","title":"Fault Tolerance"},{"location":"DesignTicketmaster/#data-partitioning","text":"Database partitioning: If we partition by 'MovieID,' all of a movie's Shows will be on the same server. This could put a lot of strain on the server if the movie is really popular. A better technique would be to divide depending on ShowID, which would distribute the load across multiple servers. ActiveReservationService and WaitingUserService partitioning: - Our web servers will keep track of all active users' sessions and handle all user contact. Based on the 'ShowID,' we may utilize Consistent Hashing to allocate application servers for both ActiveReservationService and WaitingUserService. - This way, a specific set of servers will manage all bookings and waiting users for a specific show. Assume that our Consistent Hashing allocates three servers for each Show for load balancing purposes, therefore when a reservation expires, the server holding that reservation will do the following: Update the 'Show Seats' table in the database to remove the Booking (or mark it as expired) and update the seats' Status. Get rid of the reservation in the Linked HashMap. Inform the user that their reservation has run out of time. To determine the longest waiting user, send a message to all WaitingUserService servers that are holding waiting users for that Show. A reliable hashing algorithm will reveal which servers are hosting these users. If required seats become available, send a message to the WaitingUserService server holding the longest waiting user to execute their request. When a reservation is confirmed, the following events occur: The server holding that reservation sends a message to all servers holding the Show's waiting users, instructing them to expire all waiting users who require more seats than are available. When all servers holding waiting users receive the aforementioned message, they will query the database to see how many open seats are currently available. To run this query only once, a database cache would be really useful. All waiting users who try to reserve more seats than are available will be removed from the system. WaitingUserService must cycle through the Linked HashMap of all the waiting users to do this.","title":"Data Partitioning"},{"location":"DesignURLShorteningService/","text":"Question: Designing a URL Shortening service like TinyURL Design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy Pratice on full Screen Hints to solve the problem 1. Think about functional Non functional Requirements. 2. Capacity Estimation and Constraints like traffic, bandwidth and stroage estimates. 3. Think about System API's. 4. What about database system design 5. What about Data Partitioning and Replication 6. Think about Cache and Load Balancer","title":"Question: Designing a URL Shortening service like TinyURL"},{"location":"DesignURLShorteningService/#question-designing-a-url-shortening-service-like-tinyurl","text":"Design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy Pratice on full Screen","title":"Question: Designing a URL Shortening service like TinyURL"},{"location":"DesigningAPIRateLimiter/","text":"Designing an API Rate Limiter Problem Statement Let's create an API Rate Limiter that throttles users based on the amount of requests they're sending. Difficulty Level: Medium What is a Rate Limiter? Consider a service that receives a large number of requests but can only handle a certain number of requests per second. To deal with this issue, we'd need some sort of throttling or rate limiting mechanism that would only allow a specific amount of requests per second so that our service could react to them all. At its most basic level, a rate limiter restricts the amount of events a certain object (person, device, IP, etc.) can do in a given time range. For instance: A user can send only one message per second. A user is allowed only three failed credit card transactions per day. A single IP can only create twenty accounts per day. In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks requests once the cap is reached. Why do we need API rate limiting? Rate limiting protects services from abusive behaviors such as denial-of-service (DOS) assaults, brute-force password attempts, and brute-force credit card transactions, among others. These attacks usually consist of a bombardment of HTTP/S requests that appear to be coming from real people but are actually created by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API. Rate restriction is also used to prevent revenue loss, infrastructure costs, spam, and online abuse. The following are some circumstances where rate restriction can improve the reliability of a service (or API): Misbehaving clients/scripts: Some entities can overload a service by sending a huge number of requests, either purposefully or unintentionally. Another instance is when a user makes a large number of low-priority requests and we want to ensure that they do not interfere with high-priority traffic. Users that send a large number of requests for analytics data, for example, should not be permitted to obstruct critical transactions for other users. Security: Limiting the number of second-factor attempts (in 2-factor auth) that users are permitted to make, such as the number of times they are permitted to try with a bad password. To prevent abusive behavior and poor design practices: Without API restrictions, client application developers might resort to sloppy development strategies such as repeatedly asking the same information. To keep costs and resource use in check: Services are typically built for standard input behavior, such as a user making a single post in under a minute. An API may easily push thousands of times per second. The rate limiter gives you more control over service APIs. Revenue: Certain services may wish to limit operations based on the tier of service provided to its customers, resulting in a revenue model based on rate limitation. For any of the APIs that a service provides, there may be default restrictions. To go above and beyond, the user must purchase higher limits. To avoid traffic spikiness, make sure the service is available to everyone else. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Our Rate Limiter should meet the following requirements: Functional Requirements: Limit the number of requests an entity can make to an API in a given time period, such as 15 per second. Because the APIs are accessed via a cluster, the rate restriction must be considered across all servers. When the stated threshold is exceeded inside a single server or across many servers, the user should receive an error message. Non-Functional Requirements: The system should have a high level of availability. Because it protects our service from external threats, the rate limiter should always be enabled. Our rate limitation should not cause significant delays in the user's experience. How to do Rate Limiting? - Consumers can access APIs at different rates and speeds, which is known as rate limiting. - Throttling is the process of restricting customer access to APIs for a set period of time. - Throttling can be set at both the application and API level. The server returns HTTP status \"429 - Too many requests\" when a throttling limit is exceeded. What are different types of throttling? The three most common types of throttling utilized by various services are as follows: Hard Throttling: The maximum number of API queries is limited by the throttling limit. Soft Throttling: We can set the API request limit to a specified percentage in this type. If we have a rate limit of 100 messages per minute and a 10% exceed-limit, our rate limiter will allow us to send up to 110 messages per minute. Elastic or Dynamic Throttling: If the system has some resources available, elastic throttling allows the number of requests to exceed the threshold. For example, if a user is only authorized to send 100 messages per minute, we can allow them to send more if there are free resources available in the system. What are different types of algorithms used for Rate Limiting? Following are the two types of algorithms used for Rate Limiting: Fixed Window Algorithm: The time window is considered in this algorithm from the beginning to the end of the time unit. For example, regardless of the time window in which the API call was made, a period would be regarded 0-60 seconds for a minute. There are two messages between 0-1 second and three messages between 1-2 seconds in the diagram below. This algorithm will only throttle'm5' if the rate limitation is set to two messages per second. Rolling Window Algorithm: The time window is calculated using the fraction of the time the request is made plus the time window length in this procedure. If two messages are sent at the 300th and 400th milliseconds of a second, for example, we'll count them as two messages from the 300th millisecond of that second until the 300th millisecond of the next second. We'll throttle'm3' and'm4' in the figure above, keeping two messages per second. High level design for Rate Limiter The Rate Limiter will be in charge of determining which requests are fulfilled by the API servers and which are rejected. When a new request comes in, the Web Server first asks the Rate Limiter whether it should be served or throttled. The request will be delivered to the API servers if it is not throttled. Basic System Design and Algorithm Let\u2019s take the example where we want to limit the number of requests per user. In this instance, we'd record a count of how many requests each unique user has made, as well as a timestamp for when we started counting the requests. We may store it in a hashtable with the 'UserID' as the key and a structure comprising an integer for the 'Count' and an integer for the Epoch time as the value: Assume that our rate limitation allows three requests per minute per user, thus everytime a new request arrives, our rate limiter will do the following: If the 'UserID' isn't in the hash-table, add it, set the 'Count' to 1, and change the 'StartTime' to the current time (normalized to a minute) before allowing the request. If CurrentTime \u2013 StartTime >= 1 minute, find the record for the 'UserID' and set the 'StartTime' to the current time, 'Count' to 1, and grant the request. If the difference between CurrentTime and StartTime is 1 minute and If \u2018Count < 3\u2019, increment the Count and allow the request. If \u2018Count >= 3\u2019, reject the request. What are some of the problems with our algorithm? Because we're resetting the 'StartTime' at the end of every minute, this is a Fixed Window algorithm , which implies it can possibly enable twice the number of requests every minute. Consider what happens if Kristie submits three requests in the last second of a minute, then sends three more in the first second of the next minute, for a total of six requests in two seconds. A sliding window approach, which we'll describe later, would be the solution to this problem. 2. Atomicity: The \"read-then-write\" practice can cause a race issue in a distributed setting. Consider the case when Kristie's current 'Count' is \"2\" and she makes two more requests. If two distinct processes served each of these requests and read the Count concurrently before either of them modified it, each process would believe Kristie could have one more request and had not reached the rate limit. If we're storing our key-value in Redis, one way to overcome the atomicity issue is to employ a Redis lock for the duration of the read-update operation. However, this would slow down concurrent requests from the same user and add another layer of complexity. We could use Memcached, but it would have similar issues. We can have a custom implementation for 'locking' each record if we use a basic hash-table to handle our atomicity problems. How much memory would we need to store all of the user data? Let's start with a simple solution in which all of the data is stored in a hash table. Assume that 'UserID' is 8 bytes long. Let's also suppose that a two-byte 'Count,' which can count up to 65k, is plenty for our needs. Although the epoch time requires four bytes, we can record only the minute and second parts, which can be stored in two bytes. As a result, storing a user's data requires a total of 12 bytes: 8 + 2 + 2 = 12 bytes Assume that each record in our hash table has a 20-byte overhead. If we need to track one million people at any given time, we'll require 32MB of total memory: (12 + 20) bytes * 1 million => 32MB We'll need 36MB of memory if we think we'll need a 4-byte number to lock each user's record to address our atomicity problems. This can easily fit on a single server, but we don't want to route all of our traffic through that computer. Furthermore, assuming a rate restriction of 10 requests per second, our rate limiter would have 10 million QPS! This is just too much for a single server to handle. In a distributed system, we can probably expect to employ a Redis or Memcached-like solution.All of the data will be stored on remote Redis servers, which all Rate Limiter servers will read (and update) before serving or throttling any requests. Sliding Window algorithm We can keep a sliding window open if we keep track of each user's requests. In our hash-'value' table's column, we may store the timestamp of each request in a Redis Sorted Set. Let's say our rate limiter allows three requests per minute per user. When a new request arrives, the Rate Limiter will do the following: Remove all timestamps older than \"CurrentTime - 1 minute\" from the Sorted Set. Count how many elements there are in the sorted set. If this number exceeds our \"3\" throttling limit, reject the request. Accept the request and add the current time to the sorted set. How much memory would we need to store all of the user data for sliding window? Assume that 'UserID' is 8 bytes long. Each epoch will take up 4 bytes. Let's pretend we need to limit requests to 500 per hour. Assume a hash-table overhead of 20 bytes and a Sorted Set overhead of 20 bytes. To store one user's data, we'd require a maximum of 12KB: 8 + (4 + 20 (sorted set overhead)) * 500 + 20 (hash-table overhead) = 12KB We're reserving 20 bytes each element in this case. We may assume that in a sorted set, we'll need at least two pointers to keep the items in order \u2014 one to the previous element and one to the next. Each pointer on a 64-bit computer will take up 8 bytes. As a result, pointers will require 16 bytes. An extra word (4 bytes) was added to store other overhead. If we need to track one million users at any time, total memory we would need would be 12GB: 12KB * 1 million ~= 12GB Sliding Window Algorithm takes a lot of memory compared to the Fixed Window; this would be a scalability issue. What if we can combine the above two algorithms to optimize our memory usage? Sliding Window with Counters What if we kept track of request counts for each user throughout numerous set time periods, such as 1/60th the size of our rate limit's time window? When we receive a new request, for example, we can retain a count for each minute and calculate the sum of all counters in the previous hour to establish the throttling limit. This would decrease our RAM use. Let's say we set a rate limit of 500 requests per hour, with an extra 10 requests per minute limit. Kristie has surpassed the rate limit when the aggregate of the counters with timestamps in the last hour exceeds the request threshold (500). Furthermore, she is limited to sending ten requests per minute. Because none of the real users would send frequent requests, this would be a sensible and practical consideration. Even if they succeed, retries will be successful because their restrictions are reset every minute. Our counters may be stored in a Redis Hash, which provides highly efficient storage for less than 100 keys. Each request sets the hash to expire an hour later by incrementing a counter in the hash. Each 'time' will be converted to a minute. How much memory we would need to store all the user data for sliding window with counters? Assume that 'UserID' is 8 bytes long. Each epoch will require 4 bytes, whereas the Counter will require 2 bytes. Let's pretend we need to limit requests to 500 per hour. Assume a hash-table overhead of 20 bytes and a Redis hash overhead of 20 bytes. We'll need 60 entries for each user because we'll be keeping a minute-by-minute count. To store one user's data, we'd require a total of 1.6KB: 8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB If we need to track one million users at any time, total memory we would need would be 1.6GB: 1.6KB * 1 million ~= 1.6GB So, our \u2018Sliding Window with Counters\u2019 algorithm uses 86% less memory than the simple sliding window algorithm. Data Sharding and Caching To spread the user's data, we can shard based on the 'UserID.' Consistent Hashing should be used for fault tolerance and replication. We can opt to shard each user per API if we want different throttle limitations for distinct APIs. Consider URL Shortener: each user or IP can have a distinct rate limiter for the createURL() and deleteURL() APIs. If our APIs are partitioned, a separate (slightly smaller) rate restriction for each API shard would be a reasonable idea. Take our URL Shortener, for example, where we wish to limit each user to no more than 100 short URLs each hour. We can rate limit each partition to allow a user to produce no more than three short URLs per minute in addition to 100 short URLs per hour if we utilize Hash-Based Partitioning for our createURL() API. Caching recent active users can provide significant benefits to our system. Before reaching backend servers, application servers can rapidly check if the cache contains the necessary record. By updating all counts and timestamps in cache only, our rate limiter may greatly benefit from the Write-back cache. The persistent storage can be written to at predetermined intervals. We can ensure that the rate limiter adds the least amount of latency to the user's requests this way. When the user has reached their maximum limit and the rate limiter is merely reading data without any updates, the reads can always hit the cache first, which will be tremendously handy. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. Should we rate limit by IP or by user? Let\u2019s discuss the pros and cons of using each one of these schemes: IP: We limit requests per-IP in this approach; while it's not ideal for distinguishing between 'good' and 'bad' actors, it's still better than having no rate limitation as all. When several users share a single public IP, such as in an internet cafe or smartphone users utilizing the same gateway, IP based throttling becomes a major issue. Throttling can be caused by a single bad user. Another issue could develop with caching IP-based limits: because a hacker has access to a large number of IPv6 addresses from just one computer, it's easy to cause a server to run out of memory tracking IPv6 addresses! User: After user authentication, rate limits can be done on APIs. After being authorized, the user will be given a token to pass along with each request. This ensures that we rate limit against a specific API with a valid authentication token. But what if the login API itself has a rate limit? A hacker could launch a denial of service attack against a user by submitting incorrect credentials up to the limit; after that, the actual user will be unable to log in. How about if we combine the above two schemes? Hybrid: A good strategy would be to implement both per-IP and per-user rate limitation, as both have flaws when performed separately. However, this will result in more cache entries with more details per entry, requiring more memory and storage.","title":"Designing an API Rate Limiter"},{"location":"DesigningAPIRateLimiter/#designing-an-api-rate-limiter","text":"","title":"Designing an API Rate Limiter"},{"location":"DesigningAPIRateLimiter/#problem-statement","text":"Let's create an API Rate Limiter that throttles users based on the amount of requests they're sending. Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningAPIRateLimiter/#what-is-a-rate-limiter","text":"Consider a service that receives a large number of requests but can only handle a certain number of requests per second. To deal with this issue, we'd need some sort of throttling or rate limiting mechanism that would only allow a specific amount of requests per second so that our service could react to them all. At its most basic level, a rate limiter restricts the amount of events a certain object (person, device, IP, etc.) can do in a given time range. For instance: A user can send only one message per second. A user is allowed only three failed credit card transactions per day. A single IP can only create twenty accounts per day. In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks requests once the cap is reached.","title":"What is a Rate Limiter?"},{"location":"DesigningAPIRateLimiter/#why-do-we-need-api-rate-limiting","text":"Rate limiting protects services from abusive behaviors such as denial-of-service (DOS) assaults, brute-force password attempts, and brute-force credit card transactions, among others. These attacks usually consist of a bombardment of HTTP/S requests that appear to be coming from real people but are actually created by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API. Rate restriction is also used to prevent revenue loss, infrastructure costs, spam, and online abuse. The following are some circumstances where rate restriction can improve the reliability of a service (or API): Misbehaving clients/scripts: Some entities can overload a service by sending a huge number of requests, either purposefully or unintentionally. Another instance is when a user makes a large number of low-priority requests and we want to ensure that they do not interfere with high-priority traffic. Users that send a large number of requests for analytics data, for example, should not be permitted to obstruct critical transactions for other users. Security: Limiting the number of second-factor attempts (in 2-factor auth) that users are permitted to make, such as the number of times they are permitted to try with a bad password. To prevent abusive behavior and poor design practices: Without API restrictions, client application developers might resort to sloppy development strategies such as repeatedly asking the same information. To keep costs and resource use in check: Services are typically built for standard input behavior, such as a user making a single post in under a minute. An API may easily push thousands of times per second. The rate limiter gives you more control over service APIs. Revenue: Certain services may wish to limit operations based on the tier of service provided to its customers, resulting in a revenue model based on rate limitation. For any of the APIs that a service provides, there may be default restrictions. To go above and beyond, the user must purchase higher limits. To avoid traffic spikiness, make sure the service is available to everyone else.","title":"Why do we need API rate limiting?"},{"location":"DesigningAPIRateLimiter/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningAPIRateLimiter/#solution","text":"","title":"Solution"},{"location":"DesigningAPIRateLimiter/#requirements-and-goals-of-the-system","text":"Our Rate Limiter should meet the following requirements: Functional Requirements: Limit the number of requests an entity can make to an API in a given time period, such as 15 per second. Because the APIs are accessed via a cluster, the rate restriction must be considered across all servers. When the stated threshold is exceeded inside a single server or across many servers, the user should receive an error message. Non-Functional Requirements: The system should have a high level of availability. Because it protects our service from external threats, the rate limiter should always be enabled. Our rate limitation should not cause significant delays in the user's experience. How to do Rate Limiting? - Consumers can access APIs at different rates and speeds, which is known as rate limiting. - Throttling is the process of restricting customer access to APIs for a set period of time. - Throttling can be set at both the application and API level. The server returns HTTP status \"429 - Too many requests\" when a throttling limit is exceeded. What are different types of throttling? The three most common types of throttling utilized by various services are as follows: Hard Throttling: The maximum number of API queries is limited by the throttling limit. Soft Throttling: We can set the API request limit to a specified percentage in this type. If we have a rate limit of 100 messages per minute and a 10% exceed-limit, our rate limiter will allow us to send up to 110 messages per minute. Elastic or Dynamic Throttling: If the system has some resources available, elastic throttling allows the number of requests to exceed the threshold. For example, if a user is only authorized to send 100 messages per minute, we can allow them to send more if there are free resources available in the system.","title":"Requirements and Goals of the System"},{"location":"DesigningAPIRateLimiter/#what-are-different-types-of-algorithms-used-for-rate-limiting","text":"Following are the two types of algorithms used for Rate Limiting: Fixed Window Algorithm: The time window is considered in this algorithm from the beginning to the end of the time unit. For example, regardless of the time window in which the API call was made, a period would be regarded 0-60 seconds for a minute. There are two messages between 0-1 second and three messages between 1-2 seconds in the diagram below. This algorithm will only throttle'm5' if the rate limitation is set to two messages per second. Rolling Window Algorithm: The time window is calculated using the fraction of the time the request is made plus the time window length in this procedure. If two messages are sent at the 300th and 400th milliseconds of a second, for example, we'll count them as two messages from the 300th millisecond of that second until the 300th millisecond of the next second. We'll throttle'm3' and'm4' in the figure above, keeping two messages per second.","title":"What are different types of algorithms used for Rate Limiting?"},{"location":"DesigningAPIRateLimiter/#high-level-design-for-rate-limiter","text":"The Rate Limiter will be in charge of determining which requests are fulfilled by the API servers and which are rejected. When a new request comes in, the Web Server first asks the Rate Limiter whether it should be served or throttled. The request will be delivered to the API servers if it is not throttled.","title":"High level design for Rate Limiter"},{"location":"DesigningAPIRateLimiter/#basic-system-design-and-algorithm","text":"Let\u2019s take the example where we want to limit the number of requests per user. In this instance, we'd record a count of how many requests each unique user has made, as well as a timestamp for when we started counting the requests. We may store it in a hashtable with the 'UserID' as the key and a structure comprising an integer for the 'Count' and an integer for the Epoch time as the value: Assume that our rate limitation allows three requests per minute per user, thus everytime a new request arrives, our rate limiter will do the following: If the 'UserID' isn't in the hash-table, add it, set the 'Count' to 1, and change the 'StartTime' to the current time (normalized to a minute) before allowing the request. If CurrentTime \u2013 StartTime >= 1 minute, find the record for the 'UserID' and set the 'StartTime' to the current time, 'Count' to 1, and grant the request. If the difference between CurrentTime and StartTime is 1 minute and If \u2018Count < 3\u2019, increment the Count and allow the request. If \u2018Count >= 3\u2019, reject the request. What are some of the problems with our algorithm? Because we're resetting the 'StartTime' at the end of every minute, this is a Fixed Window algorithm , which implies it can possibly enable twice the number of requests every minute. Consider what happens if Kristie submits three requests in the last second of a minute, then sends three more in the first second of the next minute, for a total of six requests in two seconds. A sliding window approach, which we'll describe later, would be the solution to this problem. 2. Atomicity: The \"read-then-write\" practice can cause a race issue in a distributed setting. Consider the case when Kristie's current 'Count' is \"2\" and she makes two more requests. If two distinct processes served each of these requests and read the Count concurrently before either of them modified it, each process would believe Kristie could have one more request and had not reached the rate limit. If we're storing our key-value in Redis, one way to overcome the atomicity issue is to employ a Redis lock for the duration of the read-update operation. However, this would slow down concurrent requests from the same user and add another layer of complexity. We could use Memcached, but it would have similar issues. We can have a custom implementation for 'locking' each record if we use a basic hash-table to handle our atomicity problems. How much memory would we need to store all of the user data? Let's start with a simple solution in which all of the data is stored in a hash table. Assume that 'UserID' is 8 bytes long. Let's also suppose that a two-byte 'Count,' which can count up to 65k, is plenty for our needs. Although the epoch time requires four bytes, we can record only the minute and second parts, which can be stored in two bytes. As a result, storing a user's data requires a total of 12 bytes: 8 + 2 + 2 = 12 bytes Assume that each record in our hash table has a 20-byte overhead. If we need to track one million people at any given time, we'll require 32MB of total memory: (12 + 20) bytes * 1 million => 32MB We'll need 36MB of memory if we think we'll need a 4-byte number to lock each user's record to address our atomicity problems. This can easily fit on a single server, but we don't want to route all of our traffic through that computer. Furthermore, assuming a rate restriction of 10 requests per second, our rate limiter would have 10 million QPS! This is just too much for a single server to handle. In a distributed system, we can probably expect to employ a Redis or Memcached-like solution.All of the data will be stored on remote Redis servers, which all Rate Limiter servers will read (and update) before serving or throttling any requests.","title":"Basic System Design and Algorithm"},{"location":"DesigningAPIRateLimiter/#sliding-window-algorithm","text":"We can keep a sliding window open if we keep track of each user's requests. In our hash-'value' table's column, we may store the timestamp of each request in a Redis Sorted Set. Let's say our rate limiter allows three requests per minute per user. When a new request arrives, the Rate Limiter will do the following: Remove all timestamps older than \"CurrentTime - 1 minute\" from the Sorted Set. Count how many elements there are in the sorted set. If this number exceeds our \"3\" throttling limit, reject the request. Accept the request and add the current time to the sorted set. How much memory would we need to store all of the user data for sliding window? Assume that 'UserID' is 8 bytes long. Each epoch will take up 4 bytes. Let's pretend we need to limit requests to 500 per hour. Assume a hash-table overhead of 20 bytes and a Sorted Set overhead of 20 bytes. To store one user's data, we'd require a maximum of 12KB: 8 + (4 + 20 (sorted set overhead)) * 500 + 20 (hash-table overhead) = 12KB We're reserving 20 bytes each element in this case. We may assume that in a sorted set, we'll need at least two pointers to keep the items in order \u2014 one to the previous element and one to the next. Each pointer on a 64-bit computer will take up 8 bytes. As a result, pointers will require 16 bytes. An extra word (4 bytes) was added to store other overhead. If we need to track one million users at any time, total memory we would need would be 12GB: 12KB * 1 million ~= 12GB Sliding Window Algorithm takes a lot of memory compared to the Fixed Window; this would be a scalability issue. What if we can combine the above two algorithms to optimize our memory usage?","title":"Sliding Window algorithm"},{"location":"DesigningAPIRateLimiter/#sliding-window-with-counters","text":"What if we kept track of request counts for each user throughout numerous set time periods, such as 1/60th the size of our rate limit's time window? When we receive a new request, for example, we can retain a count for each minute and calculate the sum of all counters in the previous hour to establish the throttling limit. This would decrease our RAM use. Let's say we set a rate limit of 500 requests per hour, with an extra 10 requests per minute limit. Kristie has surpassed the rate limit when the aggregate of the counters with timestamps in the last hour exceeds the request threshold (500). Furthermore, she is limited to sending ten requests per minute. Because none of the real users would send frequent requests, this would be a sensible and practical consideration. Even if they succeed, retries will be successful because their restrictions are reset every minute. Our counters may be stored in a Redis Hash, which provides highly efficient storage for less than 100 keys. Each request sets the hash to expire an hour later by incrementing a counter in the hash. Each 'time' will be converted to a minute. How much memory we would need to store all the user data for sliding window with counters? Assume that 'UserID' is 8 bytes long. Each epoch will require 4 bytes, whereas the Counter will require 2 bytes. Let's pretend we need to limit requests to 500 per hour. Assume a hash-table overhead of 20 bytes and a Redis hash overhead of 20 bytes. We'll need 60 entries for each user because we'll be keeping a minute-by-minute count. To store one user's data, we'd require a total of 1.6KB: 8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB If we need to track one million users at any time, total memory we would need would be 1.6GB: 1.6KB * 1 million ~= 1.6GB So, our \u2018Sliding Window with Counters\u2019 algorithm uses 86% less memory than the simple sliding window algorithm.","title":"Sliding Window with Counters"},{"location":"DesigningAPIRateLimiter/#data-sharding-and-caching","text":"To spread the user's data, we can shard based on the 'UserID.' Consistent Hashing should be used for fault tolerance and replication. We can opt to shard each user per API if we want different throttle limitations for distinct APIs. Consider URL Shortener: each user or IP can have a distinct rate limiter for the createURL() and deleteURL() APIs. If our APIs are partitioned, a separate (slightly smaller) rate restriction for each API shard would be a reasonable idea. Take our URL Shortener, for example, where we wish to limit each user to no more than 100 short URLs each hour. We can rate limit each partition to allow a user to produce no more than three short URLs per minute in addition to 100 short URLs per hour if we utilize Hash-Based Partitioning for our createURL() API. Caching recent active users can provide significant benefits to our system. Before reaching backend servers, application servers can rapidly check if the cache contains the necessary record. By updating all counts and timestamps in cache only, our rate limiter may greatly benefit from the Write-back cache. The persistent storage can be written to at predetermined intervals. We can ensure that the rate limiter adds the least amount of latency to the user's requests this way. When the user has reached their maximum limit and the rate limiter is merely reading data without any updates, the reads can always hit the cache first, which will be tremendously handy. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. Should we rate limit by IP or by user? Let\u2019s discuss the pros and cons of using each one of these schemes: IP: We limit requests per-IP in this approach; while it's not ideal for distinguishing between 'good' and 'bad' actors, it's still better than having no rate limitation as all. When several users share a single public IP, such as in an internet cafe or smartphone users utilizing the same gateway, IP based throttling becomes a major issue. Throttling can be caused by a single bad user. Another issue could develop with caching IP-based limits: because a hacker has access to a large number of IPv6 addresses from just one computer, it's easy to cause a server to run out of memory tracking IPv6 addresses! User: After user authentication, rate limits can be done on APIs. After being authorized, the user will be given a token to pass along with each request. This ensures that we rate limit against a specific API with a valid authentication token. But what if the login API itself has a rate limit? A hacker could launch a denial of service attack against a user by submitting incorrect credentials up to the limit; after that, the actual user will be unable to log in. How about if we combine the above two schemes? Hybrid: A good strategy would be to implement both per-IP and per-user rate limitation, as both have flaws when performed separately. However, this will result in more cache entries with more details per entry, requiring more memory and storage.","title":"Data Sharding and Caching"},{"location":"DesigningDropbox/","text":"Designing Dropbox Problem Statement Create a file hosting service similar to Dropbox or Google Drive. Users can save their data on faraway servers using cloud file storage. Typically, cloud storage providers manage these servers and make them available to consumers over a network (typically through the Internet). Users pay a monthly fee for their cloud data storage. Similar Services: OneDrive, Google Drive Difficulty Level: Medium Why Use Cloud Storage? Cloud file storage services have recently grown in popularity as they make it easier to store and share digital files across various devices. The massive popularity of cloud storage services is thought to be due to the change from single personal computers to multiple devices with diverse platforms and operating systems, such as smartphones and tablets, each with portable access from various geographical places at any time. The following are some of the most significant advantages of such services: Availability: Cloud storage services promote data accessibility from anywhere, at any time. Users can access their files/photos from any device, at any time and from any location. Reliability and Durability: Another advantage of cloud storage is that it guarantees data security and longevity. Cloud storage ensures that users' data is never lost by storing several copies of the data on many servers across the globe. Scalability: There will never be a shortage of storage capacity for users. You have infinite storage with cloud storage if you are willing to pay for it. If you haven't used dropbox.com before, we highly recommend opening an account and uploading/editing a file, as well as exploring the various options available. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System At the start of the interview, you should always outline criteria. Ask questions to determine the extent of the system that the interviewer is considering. Users should have the ability to upload and download files/photos from any device. Users should have the ability to share files and folders with other users. Our service should allow automated device synchronization, which means that when a file is updated on one device, it should be updated on all devices. The system should be able to store huge files of up to a GB in size. Acidity is necessary. All file operations should be assured to be atomic, consistent, isolated, and durable. Offline editing should be possible in our system. Users should be able to add, delete, and alter files when offline, and their modifications should be synced to remote servers and other online devices as soon as they reconnect. Extended Requirements The system should support snapshotting of the data, so that users can go back to any version of the files. Some Design Considerations Huge read and write volumes are to be expected. The read-to-write ratio should be about equal. Internally, files can be saved in small chunks (for example, 4MB); this has a number of advantages, such as the fact that all failed operations will only be retried for smaller portions of a file. Only the failing chunk will be retried if a user fails to upload a file. By simply sharing updated chunks, we may limit the quantity of data transmission. We can save storage space and bandwidth by deleting duplicate pieces. Keeping a local copy of the metadata (file name, size, etc.) on the client can save us a lot of server trips. Clients can intelligently upload the diffs instead of the entire chunk for minor changes. Capacity Estimation and Constraints Assume we have 500 million total users and 100 million daily active users (DAU). Assume that each user connects from three distinct devices on average. If each user has 200 files/photos, we will have a total of 100 billion files. Assuming that the average file size is 100KB, total storage would be ten petabytes. 10PB = 100B * 100KB Assume that there will be one million active connections per minute. High Level Design On their device, the user will designate a folder as their workspace. Any file/photo/folder saved in this folder will be uploaded to the cloud, and any changes or deletions to the file will be reflected in the cloud storage as well. The user can create similar workspaces on all of their devices, and any changes made on one device will be replicated on all other devices, ensuring that everyone sees the same workplace. We need to store files and metadata information such as File Name, File Size, Directory, and who this file is shared with at a high level. As a result, we'll need some servers to assist clients in uploading and downloading files to Cloud Storage, as well as servers to update information about files and users. We also require a technique to alert all clients when an update occurs, allowing them to synchronize their data. Block servers will interact with clients to upload/download files from cloud storage, and Metadata servers will maintain file metadata updated in a SQL or NoSQL database, as depicted in the figure below. Synchronization servers will handle the process of notifying all clients about synchronization updates. Component Design Let\u2019s go through the major components of our system one by one: a. Client The Client Application keeps track of the user's workspace folder and syncs all files and folders in it with the remote Cloud Storage. The client application will communicate with the storage servers to upload, download, and edit files in the backend Cloud Storage. The client also communicates with the distant Synchronization Service to handle any changes in file metadata, such as file name, size, or modification date. Here are some of the client's critical operations: Download and upload files. Monitor changes to files in the workspace folder. Deal with conflicts caused by offline or simultaneous updates. How do we efficiently transfer files? As previously stated, we can partition each file into smaller chunks so that only the updated chunks are transferred rather than the entire file. Let's imagine each file is divided into four 4MB parts. Based on 1) Storage devices we utilize in the cloud to optimize space utilization and input/output operations per second, we can statically determine what might be an appropriate chunk size (IOPS) 2) Internet bandwidth 3) Average file size in storage, and so forth. We should additionally keep a record of each file and the portions that make it up in our metadata. Do we need to keep a copy of the metadata with the client? Keeping a local copy of metadata allows us to make offline updates while also reducing the number of round trips required to update remote metadata. How can clients effectively listen to developments among their peers? One approach could be for the clients to check with the server on a regular basis to see if anything has changed. The downside with this technique is that changes will take longer to reflect locally because clients would check for updates on a regular basis rather than a server reporting anytime something changes. If the client checks the server frequently for changes, it will not only waste bandwidth by returning an empty response the majority of the time, but it will also keep the server busy. This method of gathering data is not scalable. The above problem could be solved by using HTTP long polling. Long polling is when a client requests information from a server knowing that the server may not respond right away. Instead of returning an empty response if the server has no fresh data for the client when the poll is received, the server keeps the request open and waits for response information to become available. The server provides an HTTP/S response to the client as soon as it receives fresh information, completing the open HTTP/S Request. The client can instantly send another server request for future updates after receiving the server answer. Based on the above considerations, we can divide our client into following four parts: I. Internal Metadata Database will keep track of all files, chunks, versions, and file system locations. II. Chunker will split the files down into smaller bits. It will also be in charge of reassembling a file from its fragments. Our chunking algorithm will detect portions of files that have been updated by the user and only send those portions to Cloud Storage, saving bandwidth and synchronization time. III. Watcher will keep an eye on the local workspace folders and alert the Indexer (described below) of any user actions, such as creating, deleting, or updating files or folders. Watcher also listens for any updates on other clients that the Synchronization service broadcasts. IV. Indexer The Watcher's events will be processed by the Indexer, who will update the internal metadata database with information about the chunks of updated files. The Indexer will communicate with the remote Synchronization Service to advertise changes to other clients and update the remote metadata database after the chunks have been successfully submitted/downloaded to Cloud Storage. How should customers deal with slow servers? If the server is busy or not responding, clients should back off significantly. Clients should delay their retries if a server is too sluggish to respond, and this delay should grow exponentially. Should mobile clients automatically sync remote changes? Mobile clients, unlike desktop or web clients, sync on demand to conserve bandwidth and storage space. b. Metadata Database Versioning and metadata information on files/chunks, users, and workspaces are maintained by the Metadata Database. A relational database, such as MySQL, or a NoSQL database service, such as DynamoDB, can be used as the Metadata Database. Regardless of the database type, the Synchronization Service should be able to give a consistent view of the files stored in the database, particularly if multiple users are working on the same file at the same time. Because NoSQL data stores prioritize scalability and performance over ACID properties, we'll need to programmatically include support for ACID properties in the logic of our Synchronization Service if we choose this type of database. However, using a relational database can simplify the implementation of the Synchronization Service as they natively support ACID properties. The metadata Database should be storing information about following objects: Chunks Files User Devices Workspace (sync folders) c. Synchronization Service The Synchronization Service is the component that processes a client's file updates and distributes them to other subscribers. It also synchronizes the information held in the distant Metadata DB with the local databases of clients. Because of its crucial function in managing metadata and synchronizing users' files, the Synchronization Service is the most important aspect of the system design. Desktop clients use the Synchronization Service to get updates from Cloud Storage or transfer files and updates to Cloud Storage and, perhaps, other users. If a client has been offline for an extended period of time, it will poll the system for new updates as soon as they are available. When the Synchronization Service receives an update request, it checks with the Metadata Database for consistency and then proceeds with the update. Subsequently, a notification is sent to all subscribed users or devices to report the file update. To achieve a faster response time, the Synchronization Service should be built to transport less data between clients and the Cloud Storage. The Synchronization Service can use a differencing method to reduce the quantity of data that needs to be synced to satisfy this design goal. We can send the difference between two versions of a file instead of sending whole files from clients to the server or vice versa. As a result, just the altered portion of the file is transferred. For the end user, this reduces bandwidth consumption and cloud data storage. As previously said, we will divide our files into 4MB chunks and only transfer the updated parts. To determine whether or not to update the local copy of a chunk, the server and clients can compute a hash (e.g., SHA-256). We don't need to build a new chunk on the server if we already have one with a comparable hash (even from another user). This is covered in further depth under Data Deduplication. We can use a communication middleware between clients and the Synchronization Service to create an efficient and scalable synchronization mechanism. To support a large number of clients utilizing pull or push techniques, the messaging middleware should provide scalable message queuing and change notifications. Multiple Synchronization Service instances can accept requests from a global request Queue in this manner, allowing the communication middleware to balance its load. d. Message Queuing Service A messaging middleware, which should be able to manage a large number of requests, is an important aspect of our system. Our application requires a scalable Message Queuing Service that provides asynchronous message-based communication between clients and the Synchronization Service. Asynchronous and loosely coupled message-based communication between dispersed system components is supported by the Message Queuing Service. The Message Queuing Service should be able to hold any number of messages in a highly available, scalable queue. In our system, the Message Queuing Service will implement two types of queues. The Request Queue is a global queue that is shared by all clients. Client requests to update the Metadata Database will first be sent to the Request Queue, where they will be processed by the Synchronization Service. The update messages are delivered to each client by the Response Queues that correspond to individual subscribed clients. We need to construct distinct Response Queues for each subscribed client to exchange update messages because a message will be erased from the queue after it is received by a client. e. Cloud/Block Storage Cloud/Block Storage saves portions of files that users upload. To send and receive things from the storage, clients interact directly with it. We may use any storage, whether in the cloud or on-premises, because the metadata is separated from the storage. File Processing Workflow The following sequence depicts the interaction of the application's components in a case where Client A modifies a file that is shared with Clients B and C, and they should also receive the update. If the other clients aren't online at the time of the update, the Message Queuing Service retains their update alerts in separate response queues until they do. Client A saves chunks to the cloud. Client A commits modifications and updates metadata. Client A receives confirmation, and Clients B and C are notified of the modifications. Clients B and C are notified of metadata changes and are prompted to download updated chunks. Data Deduplication Data deduplication is a storage optimization technique that eliminates redundant copies of data. It can also be used to minimize the number of bytes that must be delivered over a network. We can create a hash for each new arriving chunk and compare it to all the hashes of previous chunks to check if we already have the same chunk in our storage. In our system, we have two options for deduplication: a. Post-process deduplication New chunks are initially put on the storage device with post-process deduplication, and then a mechanism checks the data for duplication. The advantage is that clients will not have to wait for the hash computation or lookup to finish before storing the data, guaranteeing that storage performance is not harmed. The disadvantages of this technique are that 1) duplicate data will be stored unnecessarily, although for a short period, and 2) duplicate data will be transported, using bandwidth. b. In-line deduplication Deduplication hash calculations can also be performed in real time when customers submit data on their devices. If our system recognizes a chunk that it already possesses, the metadata will just provide a reference to the existing chunk rather than a full duplicate of the chunk. This strategy will allow us to make the most of our network and storage resources. Metadata Partitioning To scale the metadata database, we must split it so that it can contain data for millions of users and billions of files/chunks. We need to devise a partitioning strategy that will divide and store our data across multiple database servers. 1. Vertical Partitioning: We can divide our database so that tables pertaining to a single feature are stored on a single server. For example, all user-related tables can be stored in one database, while all files/chunks-related tables can be stored in another. Although this method is simple to apply, it has certain drawbacks: Will there be scaling issues? What if we need to store trillions of chunks and our database isn't designed to handle such a large amount of records? How would we segment such tables further? When two tables from two different databases are joined, performance and consistency difficulties can arise. How often must we link the user and file tables? 2. Range Based Partitioning: What if we partition files and chunks according to the first letter of the File Path? In that situation, we save all files beginning with the letter 'A' in one partition, files beginning with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. Even less commonly occurring letters can be combined into a single database split. This partitioning scheme should be created statically so that we can always store/find a file in a predictable manner. The biggest disadvantage of this method is that it can result in unbalanced servers. For example, suppose we decide to place all files beginning with the letter 'E' into a DB partition, only to discover later that we have far too many files beginning with the letter 'E' to fit into one DB partition. 3. Hash-Based Partitioning: In this scheme, we create a hash of the item we're storing and use that hash to determine which DB partition this object belongs in. In our example, we can use the hash of the File object's 'FileID' to figure out which partition the file would be placed on. Our hashing function will distribute objects into different partitions at random, for example, any ID can be mapped to a number between [1...256], and this number will be the division in which we will put our object. This approach can still result in overcrowded partitions, which can be avoided by employing Consistent Hashing. Caching In our system, we can have two types of caches. We can use a cache for Block storage to deal with hot files/chunks. We can use an off-the-shelf solution like Memcached to store complete chunks with their corresponding IDs/Hashes, and Block servers can rapidly verify if the cache has the necessary chunk before contacting Block storage. We can figure out how many cache servers we need based on client usage patterns. A high-end commercial server can have up to 144GB of memory, with 36K chunks cached. Which cache replacement policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to replace a chunk with a newer/hotter chunk? For our system, LRU (Least Recently Used) can be a suitable policy. We discard the chunk that has been used the least recently first. Load We can also have a cache for Metadata DB. Load Balancer (LB) The load balancing layer can be added to our system in two places: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers. At first, a basic Round Robin technique that evenly distributes incoming requests among backend servers can be used. This LB is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, LB will remove it from the rotation and stop transmitting traffic to it. Round Robin LB has the drawback of not taking server load into account. The LB will not cease delivering new requests to a server that is overloaded or slow. To address this, a more intelligent LB solution can be implemented, which queries the backend server about their load on a regular basis and adjusts traffic accordingly. Security, Permissions and File Sharing One of the main worries customers would have when saving their files in the cloud is data privacy and security, especially since our system allows users to share their files with other users or even make them public for everyone to see. To deal with this, we'll store each file's rights in our metadata database to indicate which files are visible or changeable by any user.","title":"Designing Dropbox"},{"location":"DesigningDropbox/#designing-dropbox","text":"","title":"Designing Dropbox"},{"location":"DesigningDropbox/#problem-statement","text":"Create a file hosting service similar to Dropbox or Google Drive. Users can save their data on faraway servers using cloud file storage. Typically, cloud storage providers manage these servers and make them available to consumers over a network (typically through the Internet). Users pay a monthly fee for their cloud data storage. Similar Services: OneDrive, Google Drive Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningDropbox/#why-use-cloud-storage","text":"Cloud file storage services have recently grown in popularity as they make it easier to store and share digital files across various devices. The massive popularity of cloud storage services is thought to be due to the change from single personal computers to multiple devices with diverse platforms and operating systems, such as smartphones and tablets, each with portable access from various geographical places at any time. The following are some of the most significant advantages of such services: Availability: Cloud storage services promote data accessibility from anywhere, at any time. Users can access their files/photos from any device, at any time and from any location. Reliability and Durability: Another advantage of cloud storage is that it guarantees data security and longevity. Cloud storage ensures that users' data is never lost by storing several copies of the data on many servers across the globe. Scalability: There will never be a shortage of storage capacity for users. You have infinite storage with cloud storage if you are willing to pay for it. If you haven't used dropbox.com before, we highly recommend opening an account and uploading/editing a file, as well as exploring the various options available.","title":"Why Use Cloud Storage?"},{"location":"DesigningDropbox/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningDropbox/#solution","text":"","title":"Solution"},{"location":"DesigningDropbox/#requirements-and-goals-of-the-system","text":"At the start of the interview, you should always outline criteria. Ask questions to determine the extent of the system that the interviewer is considering. Users should have the ability to upload and download files/photos from any device. Users should have the ability to share files and folders with other users. Our service should allow automated device synchronization, which means that when a file is updated on one device, it should be updated on all devices. The system should be able to store huge files of up to a GB in size. Acidity is necessary. All file operations should be assured to be atomic, consistent, isolated, and durable. Offline editing should be possible in our system. Users should be able to add, delete, and alter files when offline, and their modifications should be synced to remote servers and other online devices as soon as they reconnect. Extended Requirements The system should support snapshotting of the data, so that users can go back to any version of the files.","title":"Requirements and Goals of the System"},{"location":"DesigningDropbox/#some-design-considerations","text":"Huge read and write volumes are to be expected. The read-to-write ratio should be about equal. Internally, files can be saved in small chunks (for example, 4MB); this has a number of advantages, such as the fact that all failed operations will only be retried for smaller portions of a file. Only the failing chunk will be retried if a user fails to upload a file. By simply sharing updated chunks, we may limit the quantity of data transmission. We can save storage space and bandwidth by deleting duplicate pieces. Keeping a local copy of the metadata (file name, size, etc.) on the client can save us a lot of server trips. Clients can intelligently upload the diffs instead of the entire chunk for minor changes.","title":"Some Design Considerations"},{"location":"DesigningDropbox/#capacity-estimation-and-constraints","text":"Assume we have 500 million total users and 100 million daily active users (DAU). Assume that each user connects from three distinct devices on average. If each user has 200 files/photos, we will have a total of 100 billion files. Assuming that the average file size is 100KB, total storage would be ten petabytes. 10PB = 100B * 100KB Assume that there will be one million active connections per minute.","title":"Capacity Estimation and Constraints"},{"location":"DesigningDropbox/#high-level-design","text":"On their device, the user will designate a folder as their workspace. Any file/photo/folder saved in this folder will be uploaded to the cloud, and any changes or deletions to the file will be reflected in the cloud storage as well. The user can create similar workspaces on all of their devices, and any changes made on one device will be replicated on all other devices, ensuring that everyone sees the same workplace. We need to store files and metadata information such as File Name, File Size, Directory, and who this file is shared with at a high level. As a result, we'll need some servers to assist clients in uploading and downloading files to Cloud Storage, as well as servers to update information about files and users. We also require a technique to alert all clients when an update occurs, allowing them to synchronize their data. Block servers will interact with clients to upload/download files from cloud storage, and Metadata servers will maintain file metadata updated in a SQL or NoSQL database, as depicted in the figure below. Synchronization servers will handle the process of notifying all clients about synchronization updates.","title":"High Level Design"},{"location":"DesigningDropbox/#component-design","text":"Let\u2019s go through the major components of our system one by one: a. Client The Client Application keeps track of the user's workspace folder and syncs all files and folders in it with the remote Cloud Storage. The client application will communicate with the storage servers to upload, download, and edit files in the backend Cloud Storage. The client also communicates with the distant Synchronization Service to handle any changes in file metadata, such as file name, size, or modification date. Here are some of the client's critical operations: Download and upload files. Monitor changes to files in the workspace folder. Deal with conflicts caused by offline or simultaneous updates. How do we efficiently transfer files? As previously stated, we can partition each file into smaller chunks so that only the updated chunks are transferred rather than the entire file. Let's imagine each file is divided into four 4MB parts. Based on 1) Storage devices we utilize in the cloud to optimize space utilization and input/output operations per second, we can statically determine what might be an appropriate chunk size (IOPS) 2) Internet bandwidth 3) Average file size in storage, and so forth. We should additionally keep a record of each file and the portions that make it up in our metadata. Do we need to keep a copy of the metadata with the client? Keeping a local copy of metadata allows us to make offline updates while also reducing the number of round trips required to update remote metadata. How can clients effectively listen to developments among their peers? One approach could be for the clients to check with the server on a regular basis to see if anything has changed. The downside with this technique is that changes will take longer to reflect locally because clients would check for updates on a regular basis rather than a server reporting anytime something changes. If the client checks the server frequently for changes, it will not only waste bandwidth by returning an empty response the majority of the time, but it will also keep the server busy. This method of gathering data is not scalable. The above problem could be solved by using HTTP long polling. Long polling is when a client requests information from a server knowing that the server may not respond right away. Instead of returning an empty response if the server has no fresh data for the client when the poll is received, the server keeps the request open and waits for response information to become available. The server provides an HTTP/S response to the client as soon as it receives fresh information, completing the open HTTP/S Request. The client can instantly send another server request for future updates after receiving the server answer. Based on the above considerations, we can divide our client into following four parts: I. Internal Metadata Database will keep track of all files, chunks, versions, and file system locations. II. Chunker will split the files down into smaller bits. It will also be in charge of reassembling a file from its fragments. Our chunking algorithm will detect portions of files that have been updated by the user and only send those portions to Cloud Storage, saving bandwidth and synchronization time. III. Watcher will keep an eye on the local workspace folders and alert the Indexer (described below) of any user actions, such as creating, deleting, or updating files or folders. Watcher also listens for any updates on other clients that the Synchronization service broadcasts. IV. Indexer The Watcher's events will be processed by the Indexer, who will update the internal metadata database with information about the chunks of updated files. The Indexer will communicate with the remote Synchronization Service to advertise changes to other clients and update the remote metadata database after the chunks have been successfully submitted/downloaded to Cloud Storage. How should customers deal with slow servers? If the server is busy or not responding, clients should back off significantly. Clients should delay their retries if a server is too sluggish to respond, and this delay should grow exponentially. Should mobile clients automatically sync remote changes? Mobile clients, unlike desktop or web clients, sync on demand to conserve bandwidth and storage space. b. Metadata Database Versioning and metadata information on files/chunks, users, and workspaces are maintained by the Metadata Database. A relational database, such as MySQL, or a NoSQL database service, such as DynamoDB, can be used as the Metadata Database. Regardless of the database type, the Synchronization Service should be able to give a consistent view of the files stored in the database, particularly if multiple users are working on the same file at the same time. Because NoSQL data stores prioritize scalability and performance over ACID properties, we'll need to programmatically include support for ACID properties in the logic of our Synchronization Service if we choose this type of database. However, using a relational database can simplify the implementation of the Synchronization Service as they natively support ACID properties. The metadata Database should be storing information about following objects: Chunks Files User Devices Workspace (sync folders) c. Synchronization Service The Synchronization Service is the component that processes a client's file updates and distributes them to other subscribers. It also synchronizes the information held in the distant Metadata DB with the local databases of clients. Because of its crucial function in managing metadata and synchronizing users' files, the Synchronization Service is the most important aspect of the system design. Desktop clients use the Synchronization Service to get updates from Cloud Storage or transfer files and updates to Cloud Storage and, perhaps, other users. If a client has been offline for an extended period of time, it will poll the system for new updates as soon as they are available. When the Synchronization Service receives an update request, it checks with the Metadata Database for consistency and then proceeds with the update. Subsequently, a notification is sent to all subscribed users or devices to report the file update. To achieve a faster response time, the Synchronization Service should be built to transport less data between clients and the Cloud Storage. The Synchronization Service can use a differencing method to reduce the quantity of data that needs to be synced to satisfy this design goal. We can send the difference between two versions of a file instead of sending whole files from clients to the server or vice versa. As a result, just the altered portion of the file is transferred. For the end user, this reduces bandwidth consumption and cloud data storage. As previously said, we will divide our files into 4MB chunks and only transfer the updated parts. To determine whether or not to update the local copy of a chunk, the server and clients can compute a hash (e.g., SHA-256). We don't need to build a new chunk on the server if we already have one with a comparable hash (even from another user). This is covered in further depth under Data Deduplication. We can use a communication middleware between clients and the Synchronization Service to create an efficient and scalable synchronization mechanism. To support a large number of clients utilizing pull or push techniques, the messaging middleware should provide scalable message queuing and change notifications. Multiple Synchronization Service instances can accept requests from a global request Queue in this manner, allowing the communication middleware to balance its load. d. Message Queuing Service A messaging middleware, which should be able to manage a large number of requests, is an important aspect of our system. Our application requires a scalable Message Queuing Service that provides asynchronous message-based communication between clients and the Synchronization Service. Asynchronous and loosely coupled message-based communication between dispersed system components is supported by the Message Queuing Service. The Message Queuing Service should be able to hold any number of messages in a highly available, scalable queue. In our system, the Message Queuing Service will implement two types of queues. The Request Queue is a global queue that is shared by all clients. Client requests to update the Metadata Database will first be sent to the Request Queue, where they will be processed by the Synchronization Service. The update messages are delivered to each client by the Response Queues that correspond to individual subscribed clients. We need to construct distinct Response Queues for each subscribed client to exchange update messages because a message will be erased from the queue after it is received by a client. e. Cloud/Block Storage Cloud/Block Storage saves portions of files that users upload. To send and receive things from the storage, clients interact directly with it. We may use any storage, whether in the cloud or on-premises, because the metadata is separated from the storage.","title":"Component Design"},{"location":"DesigningDropbox/#file-processing-workflow","text":"The following sequence depicts the interaction of the application's components in a case where Client A modifies a file that is shared with Clients B and C, and they should also receive the update. If the other clients aren't online at the time of the update, the Message Queuing Service retains their update alerts in separate response queues until they do. Client A saves chunks to the cloud. Client A commits modifications and updates metadata. Client A receives confirmation, and Clients B and C are notified of the modifications. Clients B and C are notified of metadata changes and are prompted to download updated chunks.","title":"File Processing Workflow"},{"location":"DesigningDropbox/#data-deduplication","text":"Data deduplication is a storage optimization technique that eliminates redundant copies of data. It can also be used to minimize the number of bytes that must be delivered over a network. We can create a hash for each new arriving chunk and compare it to all the hashes of previous chunks to check if we already have the same chunk in our storage. In our system, we have two options for deduplication: a. Post-process deduplication New chunks are initially put on the storage device with post-process deduplication, and then a mechanism checks the data for duplication. The advantage is that clients will not have to wait for the hash computation or lookup to finish before storing the data, guaranteeing that storage performance is not harmed. The disadvantages of this technique are that 1) duplicate data will be stored unnecessarily, although for a short period, and 2) duplicate data will be transported, using bandwidth. b. In-line deduplication Deduplication hash calculations can also be performed in real time when customers submit data on their devices. If our system recognizes a chunk that it already possesses, the metadata will just provide a reference to the existing chunk rather than a full duplicate of the chunk. This strategy will allow us to make the most of our network and storage resources.","title":"Data Deduplication"},{"location":"DesigningDropbox/#metadata-partitioning","text":"To scale the metadata database, we must split it so that it can contain data for millions of users and billions of files/chunks. We need to devise a partitioning strategy that will divide and store our data across multiple database servers. 1. Vertical Partitioning: We can divide our database so that tables pertaining to a single feature are stored on a single server. For example, all user-related tables can be stored in one database, while all files/chunks-related tables can be stored in another. Although this method is simple to apply, it has certain drawbacks: Will there be scaling issues? What if we need to store trillions of chunks and our database isn't designed to handle such a large amount of records? How would we segment such tables further? When two tables from two different databases are joined, performance and consistency difficulties can arise. How often must we link the user and file tables? 2. Range Based Partitioning: What if we partition files and chunks according to the first letter of the File Path? In that situation, we save all files beginning with the letter 'A' in one partition, files beginning with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. Even less commonly occurring letters can be combined into a single database split. This partitioning scheme should be created statically so that we can always store/find a file in a predictable manner. The biggest disadvantage of this method is that it can result in unbalanced servers. For example, suppose we decide to place all files beginning with the letter 'E' into a DB partition, only to discover later that we have far too many files beginning with the letter 'E' to fit into one DB partition. 3. Hash-Based Partitioning: In this scheme, we create a hash of the item we're storing and use that hash to determine which DB partition this object belongs in. In our example, we can use the hash of the File object's 'FileID' to figure out which partition the file would be placed on. Our hashing function will distribute objects into different partitions at random, for example, any ID can be mapped to a number between [1...256], and this number will be the division in which we will put our object. This approach can still result in overcrowded partitions, which can be avoided by employing Consistent Hashing.","title":"Metadata Partitioning"},{"location":"DesigningDropbox/#caching","text":"In our system, we can have two types of caches. We can use a cache for Block storage to deal with hot files/chunks. We can use an off-the-shelf solution like Memcached to store complete chunks with their corresponding IDs/Hashes, and Block servers can rapidly verify if the cache has the necessary chunk before contacting Block storage. We can figure out how many cache servers we need based on client usage patterns. A high-end commercial server can have up to 144GB of memory, with 36K chunks cached. Which cache replacement policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to replace a chunk with a newer/hotter chunk? For our system, LRU (Least Recently Used) can be a suitable policy. We discard the chunk that has been used the least recently first. Load We can also have a cache for Metadata DB.","title":"Caching"},{"location":"DesigningDropbox/#load-balancer-lb","text":"The load balancing layer can be added to our system in two places: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers. At first, a basic Round Robin technique that evenly distributes incoming requests among backend servers can be used. This LB is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, LB will remove it from the rotation and stop transmitting traffic to it. Round Robin LB has the drawback of not taking server load into account. The LB will not cease delivering new requests to a server that is overloaded or slow. To address this, a more intelligent LB solution can be implemented, which queries the backend server about their load on a regular basis and adjusts traffic accordingly.","title":"Load Balancer (LB)"},{"location":"DesigningDropbox/#security-permissions-and-file-sharing","text":"One of the main worries customers would have when saving their files in the cloud is data privacy and security, especially since our system allows users to share their files with other users or even make them public for everyone to see. To deal with this, we'll store each file's rights in our metadata database to indicate which files are visible or changeable by any user.","title":"Security, Permissions and File Sharing"},{"location":"DesigningFacebookMessenger/","text":"Designing Facebook Messenger Problem Statement Let's create a service similar to Facebook Messenger that allows users to send text messages to one another via online and mobile interfaces. Difficulty Level: Medium What is Facebook Messenger, exactly? Facebook Messenger is a software application that allows users to send text-based instant messages. Messenger allows users to communicate with their Facebook friends from both their phones and the Facebook Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Our Messenger should meet the following requirements: Functional Requirements: Messenger should allow users to have one-on-one talks. Messenger should maintain track of its users' online and offline statuses. Messenger should be able to save chat history indefinitely. Non-functional Requirements: Users should be able to chat in real time with minimal latency. Our system should be extremely consistent, with consumers seeing the same conversation history across all of their devices. We prefer Messenger's high availability, but we can live with reduced availability for the sake of consistency. Extended Requirements: Group Chats: Messenger should allow several users to converse in a group setting. Push notifications: When users are offline, Messenger should be able to alert them of new messages. Capacity Estimation and Constraints Assume we have 500 million daily active users, each of whom sends 40 messages each day, for a total of 20 billion messages every day. Storage Estimation: Assuming that a message is 100 bytes on average, we'd need 2TB of storage to store all of the communications for a single day. 20 billion messages * 100 bytes => 2 TB/day To store five years of chat history, we would need 3.6 petabytes of storage. 2 TB * 365 days * 5 years ~= 3.6 PB We would also need to keep user information and message metadata in addition to the chat messages (ID, Timestamp, etc.). Not to mention the fact that the above computation ignores data compression and replication. Bandwidth Estimation: If our service is getting 2TB of data every day, this will give us 25MB of incoming data for each second. 2 TB / 86400 sec ~= 25 MB/s Since each incoming message needs to go out to another user, we will need the same amount of bandwidth 25MB/s for both upload and download. High level estimates: Total messages 20 billion per day Storage for each day 2TB Storage for 5 years 3.6PB Incomming data 25MB/s Outgoing data 25MB/s High Level Design On a high level, we'll require a chat server to function as the central hub, orchestrating all user communications. When a user wants to send a message to another user, they connect to the chat server and transmit the message to the server, which then sends it to the other user and stores it in the database. The detailed workflow would be as follows: User-A uses the chat server to send a message to User-B. The server receives the message and responds to User-A with an acknowledgement. The message is stored in the server's database and sent to User-B. User-B receives the message and sends the server an acknowledgment. The server informs User-A that the message was successfully sent to User-B. Detailed Component Design Let's start with a simple approach that runs everything on a single server. Our system must be able to handle the following use scenarios at a high level: Receive and deliver incoming and outgoing messages. Messages are stored and retrieved from a database. Keep track of which users are online and which have gone offline, and inform all relevant users of these changes. Let's go over each scenario one by one: a. Messages Handling How would we efficiently send/receive messages? A user must connect to the server and post messages for other users in order to send messages. The user has two options for receiving a message from the server: Pull model: Users can periodically query the server to see whether they have any new messages. Push model: Users can keep their connection to the server open and rely on the server to notify them when new messages arrive. If we apply the first strategy, the server will need to keep track of messages that have yet to be delivered, and when the receiving user connects to the server to request a new message, the server will be able to return all of the pending messages. To reduce latency for the user, they must check the server frequently, and if there are no outstanding messages, they will most likely receive an empty response. This is a waste of resources and does not appear to be an effective solution. If we apply our second strategy, in which all active users maintain a connection with the server, the server can transmit messages to the intended user as soon as they are received. Because the messages are delivered promptly on the opened connection, the server does not need to maintain track of the outstanding messages, and we have minimal latency. How will clients keep their connection to the server open? HTTP Long Polling or WebSockets are two options. Clients can request information from the server using extended polling with the understanding that the server may not respond immediately. Instead of returning an empty response if the server has no fresh data for the client when the poll is received, the server keeps the request open and waits for response information to become available. The server provides the response to the client as soon as it receives fresh information, completing the open request. The client can instantly send another server request for future updates after receiving the server answer. This results in significant gains in latency, throughput, and overall performance. The long polling request may time out or be disconnected from the server, in which case the client must start over. How can the server keep track of all open connections in order to efficiently relay messages to users? The server can keep a hash table with the UserID as the key and the connection object as the value. As a result, if the server receives a message for a user, it searches the hash table for that user's connection object and transmits the message on the open request. What happens if a message for a user who has gone offline is received by the server? The server can notify the sender of a delivery failure if the recipient has disconnected. If the connection is temporary, such as when the receiver's long-poll request timed out, we should expect the user to reconnect. In that instance, we can request that the sender transmit the message again. This retry might be built into the client's logic, eliminating the need for users to retype their messages. The server can also save the message and transmit it again whenever the receiver reconnects. How many chat servers are we going to need? Let's assume there are 500 million connections at any given time. We'd need 10K such servers if a modern server can handle 50K concurrent connections at any given moment. How do we know which server is responsible for which user's connection? In front of our chat servers, we can put a software load balancer in place that can map each UserID to a server and redirect the request.. How should a 'deliver message' request be handled by the server? When the server receives a new message, it must do the following: 1) Save the message in the database 2) Send the message to the recipient 3) Send the sender an acknowledgment The chat server will first locate the server that holds the recipient's connection and transmit the message to that server, which will then send the message to the receiver. We don't have to wait for the message to be stored in the database before the chat server sends the acknowledgement to the sender (this can happen in the background). The next section discusses storing the message. How does the messenger keep track of the messages' sequence? With each message, we can save a timestamp, which is the moment the message was received by the server. This will still not assure that clients receive messages in the correct order. The following is a scenario in which the server timestamp cannot establish the exact order of messages: User-1 sends the server the message M1 for User-2. At T1, the server gets M1. In the meantime, User-2 sends User-1 a message M2 to the server. At T2, the server gets the message M2, and T2 > T1. The server sends messages M1 and M2 to User-2 and User-1, respectively. As a result, User-1 will view M1 first, followed by M2, whereas User-2 will see M2 first, followed by M1. To fix this, we need to assign a sequence number to each message sent to each client. This number will define the exact order in which messages are delivered to each user. Both clients will see a different perspective of the message sequence using this solution, but it will be consistent across all devices. b. Message storage and retrieval from the database When a new message is received, the chat server must save it in the database. We have two alternatives for doing so: Create a new thread that will interact with the database to save the message. Make an asynchronous database request to save the message. While designing our database, we must keep the following in mind: How to work with the database connection pool effectively. How to retry requests that have failed. Where should requests that failed after several retries be logged? After all the issues have been rectified, how can I retry these logged requests (that failed after the retry)? How should we organize our storage? We need a database that can handle a high rate of tiny updates while also retrieving a large number of records rapidly. This is necessary because we have a large number of small messages to insert into the database, and users are more interested in sequentially accessing the messages when querying. Because we can't afford to read/write a row from the database every time a user receives/sends a message, we can't utilize RDBMS like MySQL or NoSQL like MongoDB. This will cause not just excessive latency in our service's core functions, but also a massive pressure on databases. A wide-column database solution like HBase can easily meet both of our requirements. HBase is a column-oriented key-value NoSQL database that may store numerous values in different columns for a single key. HBase is based on the Google BigTable database and runs on top of the Hadoop Distributed File System (HDFS). HBase groups data together to store new data in a memory buffer, which it then dumps to disk once the buffer is full. This kind of storing not only allows you to store a large amount of little data fast, but it also allows you to retrieve rows by key or scan ranges of rows. HBase is an efficient database for storing variable-sized data, which our business also requires. How should clients get data from the server quickly? When retrieving data from the server, clients should use pagination. The size of the page may vary depending on the client; for example, cell phones have smaller screens, therefore we need fewer messages/conversations in the viewport. c. Managing user\u2019s status We need to maintain track of each user's online/offline state and notify all affected users when that status changes. We can easily determine the user's current status because we keep a connection object on the server for all active users. With 500 million active users at any given time, broadcasting each status change to all relevant active users would use a significant amount of resources. Around this, we can apply the following optimization: When a user first opens the app, it can see the current state of all of their pals. We can send a failure to the sender and change the status on the client whenever a user sends a message to another user who has gone offline. When a user logs in, the server can broadcast that information with a delay of a few seconds to determine if the person logs out immediately. Clients can get the status of those users who are visible in the user's viewport from the server. This shouldn't be done frequently because the server broadcasts users' online status, and we can live with the stale offline status of users for a time. We can pull the status whenever the client starts a new chat with another user. Design Summary: Clients will establish a connection with the chat server in order to deliver a message, which the server will then forward to the appropriate user. To receive messages, all active users will maintain a connection with the server. On the lengthy poll request, the chat server will push new messages to the receiving user whenever a new message arrives. HBase, which offers speedy tiny updates and range-based searches, can be used to store messages. The servers can advertise a user's online status to other users who are interested. Clients can get fewer frequent status updates for people who are visible in the client's viewport. Data partitioning We'll need to divide the data over numerous database servers because we'll be storing a lot of it (3.6PB for five years). What is our plan for partitioning? Partitioning based on UserID: - Let's pretend we split based on the UserID hash so we can keep all of a user's messages in the same database. We'll have \"3.6PB/4TB = 900\" shards for five years if each DB shard is 4TB. - Let's assume we maintain 1K shards for the sake of simplicity. - So we'll use \"hash(UserID) percent 1000\" to obtain the shard number, and then store/retrieve the data from there. This segmentation approach will also make retrieving conversation history for any user very quick. - We can start with fewer database servers and several shards on a single physical server in the beginning. - We can simply store several partitions on a single server because we may have multiple database instances on it. - To map many logical partitions on one physical server, our hash function must grasp this logical partitioning scheme. - We can start with a large number of logical partitions that are mapped to fewer physical servers because we will keep an indefinite history of messages, and as our storage requirement grows, we can add more physical servers to disperse our logical partitions. Partitioning based on MessageID: We should not utilize this technique since getting a range of messages from a chat would be exceedingly sluggish if we store different messages of a user on various database shards. Cache We can cache a subset of recent messages (say, the last 15) in a subset of recent conversations displayed in a user's viewport (say last 5). Because we choose to store all of a user's communications on a single shard, the user's cache should also be on a single machine. Load balancing In front of our chat servers, we'll need a load balancer that can map each UserID to a server that has the user's connection and then divert requests to that server. A load balancer would also be required for our caching servers. Fault tolerance and Replication What happens if a chat server goes down? Our chat servers are maintaining user connections. Should we create a system to transfer connections to another server if one goes down? It's difficult to failover TCP connections to different servers; a simpler solution is to have clients reconnect automatically if the connection is lost. Should several copies of user messages be kept? We can't have only one copy of a user's data since we won't be able to restore it if the server that has it crashes or goes down permanently. We can either store several copies of the data on various servers or distribute and replicate it using techniques like Reed-Solomon encoding. Extended Requirements a. Group chat Separate group-chat objects can be created in our system and saved on the chat servers. GroupChatID is used to identify a group-chat object, which also keeps track of who is in the conversation. Our load balancer can send each group chat message to the server that handles that group chat based on GroupChatID, and the server that handles that group chat can iterate over all of the users in the chat to determine the server that handles each user's connection to deliver the message. All group conversations can be stored in a distinct table partitioned by GroupChatID in databases. b. Push notifications Users can only send messages to active users in our present architecture, and if the receiving user is offline, we transmit a failure to the sending user. Our system will be able to send messages to people who are not online thanks to push notifications. Each user can opt-in to receive push notifications from their device (or a web browser) whenever a new message or event is received. Each manufacturer has its own set of servers that handle the distribution of these notifications to users. To implement push notifications, we'll need to build up a Notification server that will collect messages for offline users and transmit them to the manufacturer's push notification server, which will then send them to the user's device.","title":"Designing Facebook Messenger"},{"location":"DesigningFacebookMessenger/#designing-facebook-messenger","text":"","title":"Designing Facebook Messenger"},{"location":"DesigningFacebookMessenger/#problem-statement","text":"Let's create a service similar to Facebook Messenger that allows users to send text messages to one another via online and mobile interfaces. Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningFacebookMessenger/#what-is-facebook-messenger-exactly","text":"Facebook Messenger is a software application that allows users to send text-based instant messages. Messenger allows users to communicate with their Facebook friends from both their phones and the Facebook","title":"What is Facebook Messenger, exactly?"},{"location":"DesigningFacebookMessenger/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningFacebookMessenger/#solution","text":"","title":"Solution"},{"location":"DesigningFacebookMessenger/#requirements-and-goals-of-the-system","text":"Our Messenger should meet the following requirements: Functional Requirements: Messenger should allow users to have one-on-one talks. Messenger should maintain track of its users' online and offline statuses. Messenger should be able to save chat history indefinitely. Non-functional Requirements: Users should be able to chat in real time with minimal latency. Our system should be extremely consistent, with consumers seeing the same conversation history across all of their devices. We prefer Messenger's high availability, but we can live with reduced availability for the sake of consistency. Extended Requirements: Group Chats: Messenger should allow several users to converse in a group setting. Push notifications: When users are offline, Messenger should be able to alert them of new messages.","title":"Requirements and Goals of the System"},{"location":"DesigningFacebookMessenger/#capacity-estimation-and-constraints","text":"Assume we have 500 million daily active users, each of whom sends 40 messages each day, for a total of 20 billion messages every day. Storage Estimation: Assuming that a message is 100 bytes on average, we'd need 2TB of storage to store all of the communications for a single day. 20 billion messages * 100 bytes => 2 TB/day To store five years of chat history, we would need 3.6 petabytes of storage. 2 TB * 365 days * 5 years ~= 3.6 PB We would also need to keep user information and message metadata in addition to the chat messages (ID, Timestamp, etc.). Not to mention the fact that the above computation ignores data compression and replication. Bandwidth Estimation: If our service is getting 2TB of data every day, this will give us 25MB of incoming data for each second. 2 TB / 86400 sec ~= 25 MB/s Since each incoming message needs to go out to another user, we will need the same amount of bandwidth 25MB/s for both upload and download. High level estimates: Total messages 20 billion per day Storage for each day 2TB Storage for 5 years 3.6PB Incomming data 25MB/s Outgoing data 25MB/s","title":"Capacity Estimation and Constraints"},{"location":"DesigningFacebookMessenger/#high-level-design","text":"On a high level, we'll require a chat server to function as the central hub, orchestrating all user communications. When a user wants to send a message to another user, they connect to the chat server and transmit the message to the server, which then sends it to the other user and stores it in the database. The detailed workflow would be as follows: User-A uses the chat server to send a message to User-B. The server receives the message and responds to User-A with an acknowledgement. The message is stored in the server's database and sent to User-B. User-B receives the message and sends the server an acknowledgment. The server informs User-A that the message was successfully sent to User-B.","title":"High Level Design"},{"location":"DesigningFacebookMessenger/#detailed-component-design","text":"Let's start with a simple approach that runs everything on a single server. Our system must be able to handle the following use scenarios at a high level: Receive and deliver incoming and outgoing messages. Messages are stored and retrieved from a database. Keep track of which users are online and which have gone offline, and inform all relevant users of these changes. Let's go over each scenario one by one: a. Messages Handling How would we efficiently send/receive messages? A user must connect to the server and post messages for other users in order to send messages. The user has two options for receiving a message from the server: Pull model: Users can periodically query the server to see whether they have any new messages. Push model: Users can keep their connection to the server open and rely on the server to notify them when new messages arrive. If we apply the first strategy, the server will need to keep track of messages that have yet to be delivered, and when the receiving user connects to the server to request a new message, the server will be able to return all of the pending messages. To reduce latency for the user, they must check the server frequently, and if there are no outstanding messages, they will most likely receive an empty response. This is a waste of resources and does not appear to be an effective solution. If we apply our second strategy, in which all active users maintain a connection with the server, the server can transmit messages to the intended user as soon as they are received. Because the messages are delivered promptly on the opened connection, the server does not need to maintain track of the outstanding messages, and we have minimal latency. How will clients keep their connection to the server open? HTTP Long Polling or WebSockets are two options. Clients can request information from the server using extended polling with the understanding that the server may not respond immediately. Instead of returning an empty response if the server has no fresh data for the client when the poll is received, the server keeps the request open and waits for response information to become available. The server provides the response to the client as soon as it receives fresh information, completing the open request. The client can instantly send another server request for future updates after receiving the server answer. This results in significant gains in latency, throughput, and overall performance. The long polling request may time out or be disconnected from the server, in which case the client must start over. How can the server keep track of all open connections in order to efficiently relay messages to users? The server can keep a hash table with the UserID as the key and the connection object as the value. As a result, if the server receives a message for a user, it searches the hash table for that user's connection object and transmits the message on the open request. What happens if a message for a user who has gone offline is received by the server? The server can notify the sender of a delivery failure if the recipient has disconnected. If the connection is temporary, such as when the receiver's long-poll request timed out, we should expect the user to reconnect. In that instance, we can request that the sender transmit the message again. This retry might be built into the client's logic, eliminating the need for users to retype their messages. The server can also save the message and transmit it again whenever the receiver reconnects. How many chat servers are we going to need? Let's assume there are 500 million connections at any given time. We'd need 10K such servers if a modern server can handle 50K concurrent connections at any given moment. How do we know which server is responsible for which user's connection? In front of our chat servers, we can put a software load balancer in place that can map each UserID to a server and redirect the request.. How should a 'deliver message' request be handled by the server? When the server receives a new message, it must do the following: 1) Save the message in the database 2) Send the message to the recipient 3) Send the sender an acknowledgment The chat server will first locate the server that holds the recipient's connection and transmit the message to that server, which will then send the message to the receiver. We don't have to wait for the message to be stored in the database before the chat server sends the acknowledgement to the sender (this can happen in the background). The next section discusses storing the message. How does the messenger keep track of the messages' sequence? With each message, we can save a timestamp, which is the moment the message was received by the server. This will still not assure that clients receive messages in the correct order. The following is a scenario in which the server timestamp cannot establish the exact order of messages: User-1 sends the server the message M1 for User-2. At T1, the server gets M1. In the meantime, User-2 sends User-1 a message M2 to the server. At T2, the server gets the message M2, and T2 > T1. The server sends messages M1 and M2 to User-2 and User-1, respectively. As a result, User-1 will view M1 first, followed by M2, whereas User-2 will see M2 first, followed by M1. To fix this, we need to assign a sequence number to each message sent to each client. This number will define the exact order in which messages are delivered to each user. Both clients will see a different perspective of the message sequence using this solution, but it will be consistent across all devices. b. Message storage and retrieval from the database When a new message is received, the chat server must save it in the database. We have two alternatives for doing so: Create a new thread that will interact with the database to save the message. Make an asynchronous database request to save the message. While designing our database, we must keep the following in mind: How to work with the database connection pool effectively. How to retry requests that have failed. Where should requests that failed after several retries be logged? After all the issues have been rectified, how can I retry these logged requests (that failed after the retry)? How should we organize our storage? We need a database that can handle a high rate of tiny updates while also retrieving a large number of records rapidly. This is necessary because we have a large number of small messages to insert into the database, and users are more interested in sequentially accessing the messages when querying. Because we can't afford to read/write a row from the database every time a user receives/sends a message, we can't utilize RDBMS like MySQL or NoSQL like MongoDB. This will cause not just excessive latency in our service's core functions, but also a massive pressure on databases. A wide-column database solution like HBase can easily meet both of our requirements. HBase is a column-oriented key-value NoSQL database that may store numerous values in different columns for a single key. HBase is based on the Google BigTable database and runs on top of the Hadoop Distributed File System (HDFS). HBase groups data together to store new data in a memory buffer, which it then dumps to disk once the buffer is full. This kind of storing not only allows you to store a large amount of little data fast, but it also allows you to retrieve rows by key or scan ranges of rows. HBase is an efficient database for storing variable-sized data, which our business also requires. How should clients get data from the server quickly? When retrieving data from the server, clients should use pagination. The size of the page may vary depending on the client; for example, cell phones have smaller screens, therefore we need fewer messages/conversations in the viewport. c. Managing user\u2019s status We need to maintain track of each user's online/offline state and notify all affected users when that status changes. We can easily determine the user's current status because we keep a connection object on the server for all active users. With 500 million active users at any given time, broadcasting each status change to all relevant active users would use a significant amount of resources. Around this, we can apply the following optimization: When a user first opens the app, it can see the current state of all of their pals. We can send a failure to the sender and change the status on the client whenever a user sends a message to another user who has gone offline. When a user logs in, the server can broadcast that information with a delay of a few seconds to determine if the person logs out immediately. Clients can get the status of those users who are visible in the user's viewport from the server. This shouldn't be done frequently because the server broadcasts users' online status, and we can live with the stale offline status of users for a time. We can pull the status whenever the client starts a new chat with another user. Design Summary: Clients will establish a connection with the chat server in order to deliver a message, which the server will then forward to the appropriate user. To receive messages, all active users will maintain a connection with the server. On the lengthy poll request, the chat server will push new messages to the receiving user whenever a new message arrives. HBase, which offers speedy tiny updates and range-based searches, can be used to store messages. The servers can advertise a user's online status to other users who are interested. Clients can get fewer frequent status updates for people who are visible in the client's viewport.","title":"Detailed Component Design"},{"location":"DesigningFacebookMessenger/#data-partitioning","text":"We'll need to divide the data over numerous database servers because we'll be storing a lot of it (3.6PB for five years). What is our plan for partitioning? Partitioning based on UserID: - Let's pretend we split based on the UserID hash so we can keep all of a user's messages in the same database. We'll have \"3.6PB/4TB = 900\" shards for five years if each DB shard is 4TB. - Let's assume we maintain 1K shards for the sake of simplicity. - So we'll use \"hash(UserID) percent 1000\" to obtain the shard number, and then store/retrieve the data from there. This segmentation approach will also make retrieving conversation history for any user very quick. - We can start with fewer database servers and several shards on a single physical server in the beginning. - We can simply store several partitions on a single server because we may have multiple database instances on it. - To map many logical partitions on one physical server, our hash function must grasp this logical partitioning scheme. - We can start with a large number of logical partitions that are mapped to fewer physical servers because we will keep an indefinite history of messages, and as our storage requirement grows, we can add more physical servers to disperse our logical partitions. Partitioning based on MessageID: We should not utilize this technique since getting a range of messages from a chat would be exceedingly sluggish if we store different messages of a user on various database shards.","title":"Data partitioning"},{"location":"DesigningFacebookMessenger/#cache","text":"We can cache a subset of recent messages (say, the last 15) in a subset of recent conversations displayed in a user's viewport (say last 5). Because we choose to store all of a user's communications on a single shard, the user's cache should also be on a single machine.","title":"Cache"},{"location":"DesigningFacebookMessenger/#load-balancing","text":"In front of our chat servers, we'll need a load balancer that can map each UserID to a server that has the user's connection and then divert requests to that server. A load balancer would also be required for our caching servers.","title":"Load balancing"},{"location":"DesigningFacebookMessenger/#fault-tolerance-and-replication","text":"What happens if a chat server goes down? Our chat servers are maintaining user connections. Should we create a system to transfer connections to another server if one goes down? It's difficult to failover TCP connections to different servers; a simpler solution is to have clients reconnect automatically if the connection is lost. Should several copies of user messages be kept? We can't have only one copy of a user's data since we won't be able to restore it if the server that has it crashes or goes down permanently. We can either store several copies of the data on various servers or distribute and replicate it using techniques like Reed-Solomon encoding.","title":"Fault tolerance and Replication"},{"location":"DesigningFacebookMessenger/#extended-requirements","text":"a. Group chat Separate group-chat objects can be created in our system and saved on the chat servers. GroupChatID is used to identify a group-chat object, which also keeps track of who is in the conversation. Our load balancer can send each group chat message to the server that handles that group chat based on GroupChatID, and the server that handles that group chat can iterate over all of the users in the chat to determine the server that handles each user's connection to deliver the message. All group conversations can be stored in a distinct table partitioned by GroupChatID in databases. b. Push notifications Users can only send messages to active users in our present architecture, and if the receiving user is offline, we transmit a failure to the sending user. Our system will be able to send messages to people who are not online thanks to push notifications. Each user can opt-in to receive push notifications from their device (or a web browser) whenever a new message or event is received. Each manufacturer has its own set of servers that handle the distribution of these notifications to users. To implement push notifications, we'll need to build up a Notification server that will collect messages for offline users and transmit them to the manufacturer's push notification server, which will then send them to the user's device.","title":"Extended Requirements"},{"location":"DesigningFacebookNewsfeed/","text":"Designing Facebook\u2019s Newsfeed Problem Statement Let's design Facebook's Newsfeed, which would contain posts, photos, videos, and status updates from all the people and pages a user follows. Similar Services: Twitter Newsfeed, Instagram Newsfeed, Quora Newsfeed Difficulty Level: Hard What is Facebook\u2019s newsfeed? The constantly updated list of stories in the centre of Facebook's site is known as a Newsfeed. It contains status updates, photographs, videos, links, app activity, and 'likes' from Facebook friends, pages, and groups. In other words, it's a collection of images, videos, locations, status updates, and other activities that form a comprehensive scrollable version of your friends' and your life story. You'll need a newsfeed system to display updates from friends and followers on any social media site you create, whether it's Twitter, Instagram, or Facebook. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Let\u2019s design a newsfeed for Facebook with the following requirements: Functional requirements: A user's newsfeed will be based on posts from the individuals, pages, and groups that he or she follows. A user may have a huge number of friends and follow numerous pages/groups. Images, videos, and text can all be found in feeds. For all active users, our service should allow adding new posts to the newsfeed as they arrive. Non-functional requirements: Our system should be capable of generating any user's newsfeed in real-time, with a maximum latency of 2 seconds visible to the end user. If a new newsfeed request comes in, a post should appear in a user's feed in less than 5 seconds. Capacity Estimation and Constraints Let\u2019s assume on average a user has approximately 290-300 friends and follows 190-200 pages. Traffic estimates: Assume 300 million daily active users, with each user fetching their timeline five times each day on average. This will result in 1.5 billion daily newsfeed requests, or about 17,500 requests every second. Storage estimates: Let's imagine that each user's feed has roughly 500 posts that we wish to save in memory for a quick fetch request. Let's also assume that each post is 1KB in size on average. This means that each user will require approximately 500KB of data storage. We'd need 150TB of memory to store all of this info for all of the active users. We'd need roughly 1500 machines to retain the top 500 posts in memory for all active users if a server can hold 100GB. System APIs To expose the functionality of our service, we can use SOAP or REST APIs. The API for retrieving the newsfeed could be defined as follows: getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replies) Parameters: api_dev_key (string): The API developer key of a registered user can be used to throttle users based on their quota allocation, among other things. user_id (number): The ID of the user for whom the system will generate the newsfeed. since_id (number): Optional; returns results with an ID higher than (that is, more recent than) the specified ID. count (number): Optional; specifies the number of feed items to try and retrieve up to a maximum of 200 per distinct request. max_id (number): Optional; returns results with an ID less than (that is, older than) or equal to the specified ID. exclude_replies(boolean): Optional; this parameter will prevent replies from appearing in the returned timeline. Returns: (JSON) Returns a JSON object containing a list of feed items. Database Design User, Entity (e.g. page, group, etc.) and FeedItem are the three main objects (or Post). Here are some observations about these entities' relationships: A user can follow and become friends with other entities. FeedItems, which can comprise text, images, or videos, can be posted by both users and entities. Each FeedItem will be assigned a UserID that will refer to the person who generated it. Let's pretend that just users can produce feed items for the sake of simplicity, while Facebook Pages can also post feed items. Each FeedItem can have an EntityID that points to the page or group where the post was created as an option. If we're working with a relational database, we'll need to model two relationships: User-Entity and FeedItem-Media. We can keep this relationship in a separate table because each user can be friends with a lot of individuals and follow a lot of things. In \"UserFollow,\" the \"Type\" column indicates whether the entity being followed is a User or an Entity. Similarly, a table for FeedMedia relations can be created. High Level System Design At a high level this problem can be divided into two parts: Feed generation: The posts (or feed items) of persons and entities (pages and groups) that a user follows are used to create the newsfeed. So, whenever our system receives a request to build a feed for a certain user (let's say Jane), we'll do the following: Get the IDs of all the users and entities Jane is following. For those IDs, get the most recent, popular, and relevant posts. These are the possible posts to display on Jane's newsfeed. Determine the importance of these jobs to Jane. This is Jane's most recent feed. Save this feed in the cache and present the top posts (say 20) on Jane's feed. When Jane reaches the end of her current feed, she can use the front-end to fetch the following 20 posts from the server, and so on. It's worth noting that we just generated the feed once and saved it in the cache. What about fresh posts from Jane's friends and followers? We should be able to rate and add those new posts to Jane's feed if she is online. We can repeat the above processes every five minutes or so to rate and add the latest posts to her feed. Jane can then be notified when newer items in her feed become available for her to fetch. Feed publishing: Jane must request and pull feed articles from the server every time she loads her newsfeed page. She can pull new data from the server when she reaches the end of her current stream. For newer things, the server can either notify Jane, who can then pull or push these new posts, or the server can notify Jane, who can then pull or push these new posts. We'll go through these alternatives in further detail later. Our Newsfeed service will require the following components at a high level: 1. Web servers: Maintaining the user's connection. Data will be transferred between the user and the server over this connection. 2. Application server: To carry out workflows for saving new postings in database servers. To retrieve and push the newsfeed to the end user, we'll also need some application servers. 3. Metadata database and cache: To store the metadata about Users, Pages, and Groups. 4. Posts database and cache: To store metadata about posts and their contents. 5. Video and photo storage, and cache: Blob storage, to store all the media included in the posts. 6. Newsfeed generation service: To gather and rank all relevant posts for a user's newsfeed generation and cache storage. This service will also receive real-time updates, and these updated feed items will be added to any user's timeline. 7. Feed notification service: To alert the user that newer items have been added to their newsfeed. Our system's high-level architecture diagram is shown below. User A is being followed by User B and C. Detailed Component Design Let\u2019s discuss different components of our system in detail. a. Feed generation Take, for example, the newsfeed creation service retrieving the most recent postings from all the persons and entities Jane follows; the query would be: (SELECT FeedItemID FROM FeedItem WHERE UserID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 0(user)) ) UNION (SELECT FeedItemID FROM FeedItem WHERE EntityID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 1(entity)) ) ORDER BY CreationDate DESC LIMIT 100 This concept for the feed generating service has the following flaws: Users with a large number of friends/followers will notice a significant slowdown because we must sift, merge, and rank a large number of postings. When a user loads their page, we construct the timeline. This would be extremely slow and have a significant amount of delay. Each status update will result in feed updates for all followers for live updates. This may cause significant delays in our Newsfeed Generation Service. For live updates, the server pushing (or notifying users about) newer postings could result in extremely high loads, particularly for people or sites with a large following. We can pre-generate the timeline and save it in memory to increase efficiency. Offline generation for newsfeed: We can have dedicated servers that are constantly creating and storing users' newsfeeds in memory. As a result, anytime a user requests new entries for their feed, we may simply provide them from the previously generated, cached location. Users' newsfeeds are not compiled on demand with this scheme, but rather on a regular basis and returned to them whenever they request it. When these servers need to build a feed for a user, they'll first check to see when the feed was last generated for that user. From that point on, new feed data would be generated. We may store this information in a hash table with UserID as the \"key\" and STRUCT as the \"value\" as seen below: Struct { LinkedHashMap<FeedItemID, FeedItem> feedItems; DateTime lastGenerated; } FeedItemIDs can be stored in a data structure similar to Linked HashMap or TreeMap, allowing us to simply jump to any feed item as well as loop across the map. Users can send the last FeedItemID they see in their newsfeed whenever they want more feed items, and we can jump to that FeedItemID in our hash-map and retrieve the next batch/page of feed items from there. How many feed items should we store in memory for a user\u2019s feed? We can choose to save 500 feed items per user at first, but this quantity can be changed later based on usage patterns. For example, if one page of a user's feed has 20 items and most users never visit more than ten pages of their feed, we can keep just 200 posts per user. We can always query backend servers for every user who wishes to see more posts (than what is stored in memory). Should we generate (and keep in memory) newsfeeds for all users? There will be many users who do not log in on a regular basis. Here are several options for dealing with this: 1) A simpler solution could be to employ an LRU-based cache to remove users from memory who haven't seen their newsfeed in a long time. 2) A smarter system can figure out a user's login pattern to pre-generate their newsfeed, such as when a user is active and on which days of the week they read their newsfeed. etc. In the next section, we'll look at various solutions to our \"live updates\" issues. b. Feed publishing A fanout is the process of sending a post to all of your followers. The push strategy is known as fanout-on-write, whereas the pull approach is known as fanout-on-load. Let's look at the many possibilities for distributing feed data to users. \u201cPull\u201d model or Fan-out-on-load: This strategy involves storing all recent feed data in memory so that users can retrieve it whenever they need it from the server. Clients can pull data from the stream on a regular basis or whenever they require it manually. This strategy has the following drawbacks: a) Users may not see fresh data until they send a pull request; b) It's difficult to determine the correct pull cadence, as most pull requests would return an empty response if there is no new data, wasting resources. \u201cPush\u201d model or Fan-out-on-write: With a push system, if a user publishes a post, it can be instantaneously pushed to all followers. The benefit is that you don't have to go through your buddy list and get feeds for each of them when fetching feeds. It decreases read operations greatly. Users must keep a Long Poll request with the server to receive updates in an effective manner. A potential flaw in this strategy is that when a user (a superstar) has millions of followers, the server must push changes to a large number of individuals. Hybrid: A hybrid strategy to handling feed data, combining fan-out-on-write and fan-out-on-load, could be used as an alternative. We can, for example, stop pushing posts from users with a large number of followers (celebrities) and only send data to people with a few hundred (or thousand) followers. We can allow the followers of celebrities pull the updates. We can save a lot of resources by blocking fanout for users who have a lot of friends or follows because the push operation can be very expensive for them. Another option is to limit the fanout to only her online friends after a user writes a post. A mix of 'push to notify' and 'pull for serving' end users is also a wonderful method to get the most out of both approaches. A model that is solely push or pull is less versatile. How many feed items can we return to the client in each request? We should set a limit on how many items a user can retrieve in a single request (say 20). However, we should allow the client to define the amount of feed items they want with each request because the user may wish to fetch a varied number of posts depending on the device (mobile vs. desktop). Should we always notify users if there are new posts available for their newsfeed? Users may find it beneficial to be notified when new data becomes available. On mobile devices, however, when data usage is relatively expensive, it can waste bandwidth. As a result, we can opt not to push data to mobile devices and instead allow users to \"Pull to Refresh\" for new postings. Feed Ranking The simplest straightforward approach to rank items in a newsfeed is by their creation time, but today's ranking algorithms go far further to ensure that \"important\" messages are prioritized. The high-level goal behind ranking is to find out how to integrate crucial \"signals\" that make a post relevant before calculating a final ranking score. More specifically, we can choose features that are related to the importance of any feed item, such as the number of likes, comments, shares, time of the update, whether the article contains images/videos, and so on, and then use these features to create a score. This is usually sufficient for a basic ranking system. By regularly monitoring if we are making progress in terms of user stickiness, retention, ad income, and so on, a better ranking system may greatly enhance itself. Data Partitioning a. Sharding posts and metadata We need to divide our data across numerous machines so that we can read and write it quickly because we have a large number of new postings every day and our read load is also incredibly high. We can use a scheme similar to the one described in Designing Twitter for sharding our databases that store posts and their metadata. b. Sharding feed data We can partition feed data that is being kept in memory based on UserID. We can try storing all of a user's data on a single server. We can supply the UserID to our hash function when storing, and it will map the user to a cache server where the user's feed objects will be stored. Also, because we don't anticipate to keep more than 500 FeedItmeIDs per user, we won't come into a situation where a person's feed data won't fit on a single server. We would always have to contact only one server to retrieve a user's feed. Consistent Hashing is required for future development and replication.","title":"Designing Facebook\u2019s Newsfeed"},{"location":"DesigningFacebookNewsfeed/#designing-facebooks-newsfeed","text":"","title":"Designing Facebook\u2019s Newsfeed"},{"location":"DesigningFacebookNewsfeed/#problem-statement","text":"Let's design Facebook's Newsfeed, which would contain posts, photos, videos, and status updates from all the people and pages a user follows. Similar Services: Twitter Newsfeed, Instagram Newsfeed, Quora Newsfeed Difficulty Level: Hard","title":"Problem Statement"},{"location":"DesigningFacebookNewsfeed/#what-is-facebooks-newsfeed","text":"The constantly updated list of stories in the centre of Facebook's site is known as a Newsfeed. It contains status updates, photographs, videos, links, app activity, and 'likes' from Facebook friends, pages, and groups. In other words, it's a collection of images, videos, locations, status updates, and other activities that form a comprehensive scrollable version of your friends' and your life story. You'll need a newsfeed system to display updates from friends and followers on any social media site you create, whether it's Twitter, Instagram, or Facebook.","title":"What is Facebook\u2019s newsfeed?"},{"location":"DesigningFacebookNewsfeed/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningFacebookNewsfeed/#solution","text":"","title":"Solution"},{"location":"DesigningFacebookNewsfeed/#requirements-and-goals-of-the-system","text":"Let\u2019s design a newsfeed for Facebook with the following requirements: Functional requirements: A user's newsfeed will be based on posts from the individuals, pages, and groups that he or she follows. A user may have a huge number of friends and follow numerous pages/groups. Images, videos, and text can all be found in feeds. For all active users, our service should allow adding new posts to the newsfeed as they arrive. Non-functional requirements: Our system should be capable of generating any user's newsfeed in real-time, with a maximum latency of 2 seconds visible to the end user. If a new newsfeed request comes in, a post should appear in a user's feed in less than 5 seconds.","title":"Requirements and Goals of the System"},{"location":"DesigningFacebookNewsfeed/#capacity-estimation-and-constraints","text":"Let\u2019s assume on average a user has approximately 290-300 friends and follows 190-200 pages. Traffic estimates: Assume 300 million daily active users, with each user fetching their timeline five times each day on average. This will result in 1.5 billion daily newsfeed requests, or about 17,500 requests every second. Storage estimates: Let's imagine that each user's feed has roughly 500 posts that we wish to save in memory for a quick fetch request. Let's also assume that each post is 1KB in size on average. This means that each user will require approximately 500KB of data storage. We'd need 150TB of memory to store all of this info for all of the active users. We'd need roughly 1500 machines to retain the top 500 posts in memory for all active users if a server can hold 100GB.","title":"Capacity Estimation and Constraints"},{"location":"DesigningFacebookNewsfeed/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs. The API for retrieving the newsfeed could be defined as follows: getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replies) Parameters: api_dev_key (string): The API developer key of a registered user can be used to throttle users based on their quota allocation, among other things. user_id (number): The ID of the user for whom the system will generate the newsfeed. since_id (number): Optional; returns results with an ID higher than (that is, more recent than) the specified ID. count (number): Optional; specifies the number of feed items to try and retrieve up to a maximum of 200 per distinct request. max_id (number): Optional; returns results with an ID less than (that is, older than) or equal to the specified ID. exclude_replies(boolean): Optional; this parameter will prevent replies from appearing in the returned timeline. Returns: (JSON) Returns a JSON object containing a list of feed items.","title":"System APIs"},{"location":"DesigningFacebookNewsfeed/#database-design","text":"User, Entity (e.g. page, group, etc.) and FeedItem are the three main objects (or Post). Here are some observations about these entities' relationships: A user can follow and become friends with other entities. FeedItems, which can comprise text, images, or videos, can be posted by both users and entities. Each FeedItem will be assigned a UserID that will refer to the person who generated it. Let's pretend that just users can produce feed items for the sake of simplicity, while Facebook Pages can also post feed items. Each FeedItem can have an EntityID that points to the page or group where the post was created as an option. If we're working with a relational database, we'll need to model two relationships: User-Entity and FeedItem-Media. We can keep this relationship in a separate table because each user can be friends with a lot of individuals and follow a lot of things. In \"UserFollow,\" the \"Type\" column indicates whether the entity being followed is a User or an Entity. Similarly, a table for FeedMedia relations can be created.","title":"Database Design"},{"location":"DesigningFacebookNewsfeed/#high-level-system-design","text":"At a high level this problem can be divided into two parts: Feed generation: The posts (or feed items) of persons and entities (pages and groups) that a user follows are used to create the newsfeed. So, whenever our system receives a request to build a feed for a certain user (let's say Jane), we'll do the following: Get the IDs of all the users and entities Jane is following. For those IDs, get the most recent, popular, and relevant posts. These are the possible posts to display on Jane's newsfeed. Determine the importance of these jobs to Jane. This is Jane's most recent feed. Save this feed in the cache and present the top posts (say 20) on Jane's feed. When Jane reaches the end of her current feed, she can use the front-end to fetch the following 20 posts from the server, and so on. It's worth noting that we just generated the feed once and saved it in the cache. What about fresh posts from Jane's friends and followers? We should be able to rate and add those new posts to Jane's feed if she is online. We can repeat the above processes every five minutes or so to rate and add the latest posts to her feed. Jane can then be notified when newer items in her feed become available for her to fetch. Feed publishing: Jane must request and pull feed articles from the server every time she loads her newsfeed page. She can pull new data from the server when she reaches the end of her current stream. For newer things, the server can either notify Jane, who can then pull or push these new posts, or the server can notify Jane, who can then pull or push these new posts. We'll go through these alternatives in further detail later. Our Newsfeed service will require the following components at a high level: 1. Web servers: Maintaining the user's connection. Data will be transferred between the user and the server over this connection. 2. Application server: To carry out workflows for saving new postings in database servers. To retrieve and push the newsfeed to the end user, we'll also need some application servers. 3. Metadata database and cache: To store the metadata about Users, Pages, and Groups. 4. Posts database and cache: To store metadata about posts and their contents. 5. Video and photo storage, and cache: Blob storage, to store all the media included in the posts. 6. Newsfeed generation service: To gather and rank all relevant posts for a user's newsfeed generation and cache storage. This service will also receive real-time updates, and these updated feed items will be added to any user's timeline. 7. Feed notification service: To alert the user that newer items have been added to their newsfeed. Our system's high-level architecture diagram is shown below. User A is being followed by User B and C.","title":"High Level System Design"},{"location":"DesigningFacebookNewsfeed/#detailed-component-design","text":"Let\u2019s discuss different components of our system in detail. a. Feed generation Take, for example, the newsfeed creation service retrieving the most recent postings from all the persons and entities Jane follows; the query would be: (SELECT FeedItemID FROM FeedItem WHERE UserID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 0(user)) ) UNION (SELECT FeedItemID FROM FeedItem WHERE EntityID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 1(entity)) ) ORDER BY CreationDate DESC LIMIT 100 This concept for the feed generating service has the following flaws: Users with a large number of friends/followers will notice a significant slowdown because we must sift, merge, and rank a large number of postings. When a user loads their page, we construct the timeline. This would be extremely slow and have a significant amount of delay. Each status update will result in feed updates for all followers for live updates. This may cause significant delays in our Newsfeed Generation Service. For live updates, the server pushing (or notifying users about) newer postings could result in extremely high loads, particularly for people or sites with a large following. We can pre-generate the timeline and save it in memory to increase efficiency. Offline generation for newsfeed: We can have dedicated servers that are constantly creating and storing users' newsfeeds in memory. As a result, anytime a user requests new entries for their feed, we may simply provide them from the previously generated, cached location. Users' newsfeeds are not compiled on demand with this scheme, but rather on a regular basis and returned to them whenever they request it. When these servers need to build a feed for a user, they'll first check to see when the feed was last generated for that user. From that point on, new feed data would be generated. We may store this information in a hash table with UserID as the \"key\" and STRUCT as the \"value\" as seen below: Struct { LinkedHashMap<FeedItemID, FeedItem> feedItems; DateTime lastGenerated; } FeedItemIDs can be stored in a data structure similar to Linked HashMap or TreeMap, allowing us to simply jump to any feed item as well as loop across the map. Users can send the last FeedItemID they see in their newsfeed whenever they want more feed items, and we can jump to that FeedItemID in our hash-map and retrieve the next batch/page of feed items from there. How many feed items should we store in memory for a user\u2019s feed? We can choose to save 500 feed items per user at first, but this quantity can be changed later based on usage patterns. For example, if one page of a user's feed has 20 items and most users never visit more than ten pages of their feed, we can keep just 200 posts per user. We can always query backend servers for every user who wishes to see more posts (than what is stored in memory). Should we generate (and keep in memory) newsfeeds for all users? There will be many users who do not log in on a regular basis. Here are several options for dealing with this: 1) A simpler solution could be to employ an LRU-based cache to remove users from memory who haven't seen their newsfeed in a long time. 2) A smarter system can figure out a user's login pattern to pre-generate their newsfeed, such as when a user is active and on which days of the week they read their newsfeed. etc. In the next section, we'll look at various solutions to our \"live updates\" issues. b. Feed publishing A fanout is the process of sending a post to all of your followers. The push strategy is known as fanout-on-write, whereas the pull approach is known as fanout-on-load. Let's look at the many possibilities for distributing feed data to users. \u201cPull\u201d model or Fan-out-on-load: This strategy involves storing all recent feed data in memory so that users can retrieve it whenever they need it from the server. Clients can pull data from the stream on a regular basis or whenever they require it manually. This strategy has the following drawbacks: a) Users may not see fresh data until they send a pull request; b) It's difficult to determine the correct pull cadence, as most pull requests would return an empty response if there is no new data, wasting resources. \u201cPush\u201d model or Fan-out-on-write: With a push system, if a user publishes a post, it can be instantaneously pushed to all followers. The benefit is that you don't have to go through your buddy list and get feeds for each of them when fetching feeds. It decreases read operations greatly. Users must keep a Long Poll request with the server to receive updates in an effective manner. A potential flaw in this strategy is that when a user (a superstar) has millions of followers, the server must push changes to a large number of individuals. Hybrid: A hybrid strategy to handling feed data, combining fan-out-on-write and fan-out-on-load, could be used as an alternative. We can, for example, stop pushing posts from users with a large number of followers (celebrities) and only send data to people with a few hundred (or thousand) followers. We can allow the followers of celebrities pull the updates. We can save a lot of resources by blocking fanout for users who have a lot of friends or follows because the push operation can be very expensive for them. Another option is to limit the fanout to only her online friends after a user writes a post. A mix of 'push to notify' and 'pull for serving' end users is also a wonderful method to get the most out of both approaches. A model that is solely push or pull is less versatile. How many feed items can we return to the client in each request? We should set a limit on how many items a user can retrieve in a single request (say 20). However, we should allow the client to define the amount of feed items they want with each request because the user may wish to fetch a varied number of posts depending on the device (mobile vs. desktop). Should we always notify users if there are new posts available for their newsfeed? Users may find it beneficial to be notified when new data becomes available. On mobile devices, however, when data usage is relatively expensive, it can waste bandwidth. As a result, we can opt not to push data to mobile devices and instead allow users to \"Pull to Refresh\" for new postings.","title":"Detailed Component Design"},{"location":"DesigningFacebookNewsfeed/#feed-ranking","text":"The simplest straightforward approach to rank items in a newsfeed is by their creation time, but today's ranking algorithms go far further to ensure that \"important\" messages are prioritized. The high-level goal behind ranking is to find out how to integrate crucial \"signals\" that make a post relevant before calculating a final ranking score. More specifically, we can choose features that are related to the importance of any feed item, such as the number of likes, comments, shares, time of the update, whether the article contains images/videos, and so on, and then use these features to create a score. This is usually sufficient for a basic ranking system. By regularly monitoring if we are making progress in terms of user stickiness, retention, ad income, and so on, a better ranking system may greatly enhance itself.","title":"Feed Ranking"},{"location":"DesigningFacebookNewsfeed/#data-partitioning","text":"a. Sharding posts and metadata We need to divide our data across numerous machines so that we can read and write it quickly because we have a large number of new postings every day and our read load is also incredibly high. We can use a scheme similar to the one described in Designing Twitter for sharding our databases that store posts and their metadata. b. Sharding feed data We can partition feed data that is being kept in memory based on UserID. We can try storing all of a user's data on a single server. We can supply the UserID to our hash function when storing, and it will map the user to a cache server where the user's feed objects will be stored. Also, because we don't anticipate to keep more than 500 FeedItmeIDs per user, we won't come into a situation where a person's feed data won't fit on a single server. We would always have to contact only one server to retrieve a user's feed. Consistent Hashing is required for future development and replication.","title":"Data Partitioning"},{"location":"DesigningInstagram/","text":"Designing Instagram Problem Statement Let's create a photo-sharing site similar to Instagram, where users can post photographs to share with others. Similar Services: Flickr, Picasa Difficulty Level: Medium What exactly is Instagram? Instagram is a social media platform that allows users to upload and share photographs and videos with other people. Users can opt to publish information publicly or privately on Instagram. Any user can see anything published publicly, whereas privately shared items can only be accessed by a limited number of people. Instagram also allows users to share via a variety of other social media platforms, including Facebook, Twitter, Flickr, and Tumblr. We want to create a simplified version of Instagram for this experiment, where users can share photographs and follow other users. Each user's 'News Feed' will comprise of the top photos of everyone they follow. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System While designing the Instagram, we'll concentrate on the following set of requirements: Functional Requirements Photos should be able to be uploaded, downloaded, and viewed. Users can do searches based on the titles of photos and videos. Users can follow each other. The system should be able to create and show a user's News Feed, which includes the best photos from everyone the user follows. Non-functional Requirements Our service must be really accessible. For News Feed creation, the system's allowable latency is 200ms. If a user doesn't see a photo for a time, consistency may suffer (in the interest of availability); it should be acceptable. The system should be extremely dependable; no photo or video uploaded should ever be deleted. Not in scope: Adding tags to photos, searching photos on tags, commenting on photos, tagging users to photos, who to follow, etc. Some Design Considerations We'll focus on designing a system that can fetch photographs quickly because the system will be read-heavy. Users can practically upload as many photographs as they like. Storage management should be a key consideration when constructing this system. When viewing photos, expect low latency. Data should be completely trustworthy. When a user submits a photograph, the system ensures that it is never lost. Capacity Estimation and Constraints Let\u2019s assume we have 500M total users, with 1M daily active users. 2M new photos every day, 23 new photos every second. Average photo file size => 200KB Total space required for 1 day of photos 2M * 200KB => 400 GB Total space required for 10 years: 400GB * 365 (days a year) * 10 (years) ~= 1425TB High Level System Design We need to enable two situations at a high level: one for uploading images and another for viewing/searching photos. Our service would require some object storage servers for photographs and database servers for metadata information. Database Schema \ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. We need to keep track of users' uploaded photos and the people they follow. The Photo table will record all information about a photo; we'll need an index on (PhotoID, CreationDate) because we'll want to get the most recent photos first. Because we need joins, a straight simple approach for storing the above schema would be to utilize an RDBMS like MYSQL, however relational databases have their own set of problems. especially when they need to be scaled Photos can be stored in a distributed file system like HDFS or S3. To take use of NoSQL's features, we can store the above schema in a distributed key-value store. All photo metadata can be stored in a table with the 'PhotoID' as the key and an object including PhotoLocation, UserLocation, CreationTimestamp, and so on as the value. To know who owns which photo, we need to store relationships between users and photos. We also need to keep track of who a user follows. We can use a wide-column datastore like Cassandra for both of these tables. The 'key' for the 'UserPhoto' table would be 'UserID,' and the 'value' would be the user's list of 'PhotoIDs,' kept in distinct columns. The 'UserFollow' table will follow a similar pattern. Cassandra, like all key-value stores, keeps a set number of replicas to ensure reliability. Furthermore, in such data storage, deletes are not applied immediately; data is held for a specified number of days (to allow for undeleting) before being completely erased from the system. Data Size Estimation Let\u2019s estimate how much data will be going into each table and how much total storage we will need for 10 years. User: Assuming each \u201cint\u201d and \u201cdateTime\u201d is four bytes, each row in the User\u2019s table will be of 68 bytes: UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes If we have 500 million users, we will need 32GB of total storage. 500 million * 68 ~= 32GB Photo: Each row in Photo\u2019s table will be of 284 bytes: PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes If 2M new photos get uploaded every day, we will need 0.5GB of storage for one day: 2M * 284 bytes ~= 0.5GB per day For 10 years we will need 1.88TB of storage. UserFollow: Each row in the UserFollow table will consist of 8 bytes. If we have 500 million users and on average each user follows 500 users. We would need 1.82TB of storage for the UserFollow table: 500 million users * 500 followers * 8 bytes ~= 1.82TB Total space required for all tables for 10 years will be 3.7TB: 32GB + 1.88TB + 1.82TB ~= 3.7TB Component Design Photo uploads (or writes) are slow because they must write to the disk, while reads are faster, especially if they are served from cache. Because uploading is a slow procedure, uploading users can absorb all available connections. This means that if the system becomes overburdened with write requests,'reads' will not be served. Before developing our system, we should keep in mind that web servers have a connection restriction. We can't have more than 500 concurrent uploads or reads if we think a web server can only handle 500 connections at any given moment. We can split reading and writes into distinct services to alleviate this bottleneck. To guarantee that uploads do not clog the system, we will have dedicated servers for readers and separate servers for writes. Separating read and write requests for photos will allow us to scale and optimize each of these activities separately. Reliability and Redundancy Our service does not allow you to lose files. As a result, we'll keep numerous copies of each picture so that if one storage server fails, we can still access the photo from another storage server. The same idea applies to the system's other components. If we want the system to be highly available, we must have several clones of services running in the system, so that even if a few services fail, the system remains operational. Redundancy eliminates the system's single point of failure. If only one instance of a service is required to run at any one time, we can deploy a redundant secondary copy that is not providing any traffic but can assume control after the original fails. Adding redundancy to a system can eliminate single points of failure and provide backup or emergency functionality. If two instances of the same service are running in production and one of them fails or degrades, the system can failover to the healthy copy. Failover can occur automatically or with the need for user intervention. Data Sharding Let's look at several possible metadata sharding schemes: a. Partitioning based on UserID Assume we shard based on the 'UserID' in order to maintain all of a user's images on the same shard. We'll need four DB shards to store 3.7TB of data if each DB shard is 1TB. Assume we keep 10 shards for improved performance and scalability. So we'll use UserID percent 10 to discover the shard number and then save the data there. We can attach a shard number to each PhotoID to uniquely identify any photo in our system. How can we generate PhotoIDs? Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system. What are the different issues with this partitioning scheme? How would we deal with unhappy users? Many people follow such popular users, and many more people see whatever photo they post. Some users will have more photos than others, resulting in a non-uniform storage distribution. What if we can't fit all of a user's photos onto one shard? Will distributing a user's photographs across several shards result in longer latencies? Storing all of a user's images on a single shard might lead to concerns such as the loss of all of the user's data if that shard goes down, or increased latency if it is serving a large load, among other things. b. Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through \u201cPhotoID % 10\u201d, the above problems will have been solved. We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system. How can we generate PhotoIDs? Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo. Wouldn\u2019t this key generating DB be a single point of failure? Yes, it would be. A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered. For the MySQL, the following script can define such sequences: KeyGeneratingServer1: auto-increment-increment = 2 auto-increment-offset = 1 KeyGeneratingServer2: auto-increment-increment = 2 auto-increment-offset = 2 We can place a load balancer in front of each of these databases to ensure round robin and minimize downtime. Both servers may be out of sync, with one producing more keys than the other, but this will not affect our system. This architecture can be extended by creating distinct ID tables for Users, Photo-Comments, and other items in our system. Alternatively, we can use a 'key' creation technique like the one described in Designing a URL Shortening Service Like TinyURL. How can we prepare for our system's future expansion? To support future data growth, we can have a high number of logical partitions, so that several logical partitions can live on a single physical database server at first. We can have distinct databases for each logical partition on any server since each database server can have numerous database instances. So, whenever we suspect a database server is overburdened with data, we can move some logical partitions to another server. We can keep a configuration file (or a separate database) that maps our logical partitions to database servers, allowing us to quickly move partitions around. We only have to shift a partition when we need to. Ranking and News Feed Generation To make a user's News Feed, we need to get the most recent, popular, and relevant photographs from the people they follow. Let's pretend we need to get the top 100 photographs for a user's News Feed for the sake of simplicity. Our application server will first obtain a list of persons the user follows, and then retrieve metadata information for the user's most recent 100 images. The server will then submit all of these photographs to our ranking system, which will select the top 100 photos (based on recency, likeness, and other factors) and return them to the user. Because we must query many tables and conduct sorting, merging, and ranking on the results, this strategy may have increased latency. We can pre-generate the News Feed and store it in a separate table to increase efficiency. Generating the News Feed in Advance: We can have dedicated servers that generate users' News Feeds on a regular basis and store them in a 'UserNewsFeed' database. So, whenever a user wants the most recent photos for their News Feed, we'll just query this table and provide them the results. When these servers need to generate a user's News Feed, they first look in the UserNewsFeed database to see when the user's News Feed was last generated. From that point forward, new News Feed data will be generated (following the steps mentioned above). What are the different approaches for sending News Feed contents to the users? 1. Pull: Clients can get the contents of the News Feed from the server on a regular basis or whenever they require it manually. This strategy has the following drawbacks: a) New data may not be visible to users until clients initiate a pull request; b) Most pull requests will return an empty response if there is no new data. 2. Push: As soon as fresh data becomes available, servers can push it to users. Users must keep a Long Poll request with the server to receive updates in an effective manner. A person with a large number of followers or a superstar with millions of followers could be a problem with this strategy because the server would have to push changes often. 3. Hybrid: A hybrid approach is possible. We can switch all users with a lot of followers to a pull-based paradigm and just push data to those with a few hundred (or thousands) followers. Another option is for the server to transmit changes to all users at a set frequency, allowing those with a lot of follows/updates to pull data on a regular basis. Designing Facebook's Newsfeed contains a detailed description of News Feed production. News Feed Creation with Sharded Data One of the most significant requirements for creating a user's News Feed is to get the most recent photos from everyone the user follows. We'll need a way to sort images according to when they were taken. We can do this more efficiently by including photo production time in the PhotoID. We will be able to find the most recent PhotoIDs quickly since we will have a primary index on PhotoID. For this, we can utilize epoch time. Let's pretend our PhotoID has two parts: the first represents the epoch time, and the second is an auto-incrementing sequence. To create a new PhotoID, we can use the current epoch time and an auto-incrementing ID from our key-generating database. From this PhotoID (PhotoID percent 10) we can calculate the shard number and place the photo there. What is the maximum size of our PhotoID? If our epoch begins today, how many bits would be required to store the number of seconds over the following 50 years? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds This number would require 31 bits to store. We can devote 9 bits to store the auto incremented sequence because we predict 23 new images each second on average. So we can store (29 => 512) fresh photographs every second. Every second, we can reset our auto incrementing sequence. In Designing Twitter, we'll go over this concept in further depth under the heading 'Data Sharding.' Cache and Load balancing To serve the worldwide spread consumers, our service would require a large-scale photo delivery infrastructure. Our service should leverage a large number of geographically distributed photo cache servers and CDNs to bring its material closer to the user (for details see Caching). To cache hot database rows, we can add a cache to metadata servers. We can use Memcache to cache the data, and application servers can quickly check if the cache contains the needed rows before reaching the database. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. The least recently viewed row is discarded first under this policy. How can we build more intelligent cache? If we go with 80-20 rule, i.e., 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people read them. This dictates that we can try caching 20% of daily read volume of photos and metadata.","title":"Designing Instagram"},{"location":"DesigningInstagram/#designing-instagram","text":"","title":"Designing Instagram"},{"location":"DesigningInstagram/#problem-statement","text":"Let's create a photo-sharing site similar to Instagram, where users can post photographs to share with others. Similar Services: Flickr, Picasa Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningInstagram/#what-exactly-is-instagram","text":"Instagram is a social media platform that allows users to upload and share photographs and videos with other people. Users can opt to publish information publicly or privately on Instagram. Any user can see anything published publicly, whereas privately shared items can only be accessed by a limited number of people. Instagram also allows users to share via a variety of other social media platforms, including Facebook, Twitter, Flickr, and Tumblr. We want to create a simplified version of Instagram for this experiment, where users can share photographs and follow other users. Each user's 'News Feed' will comprise of the top photos of everyone they follow.","title":"What exactly is Instagram?"},{"location":"DesigningInstagram/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningInstagram/#solution","text":"","title":"Solution"},{"location":"DesigningInstagram/#requirements-and-goals-of-the-system","text":"While designing the Instagram, we'll concentrate on the following set of requirements: Functional Requirements Photos should be able to be uploaded, downloaded, and viewed. Users can do searches based on the titles of photos and videos. Users can follow each other. The system should be able to create and show a user's News Feed, which includes the best photos from everyone the user follows. Non-functional Requirements Our service must be really accessible. For News Feed creation, the system's allowable latency is 200ms. If a user doesn't see a photo for a time, consistency may suffer (in the interest of availability); it should be acceptable. The system should be extremely dependable; no photo or video uploaded should ever be deleted. Not in scope: Adding tags to photos, searching photos on tags, commenting on photos, tagging users to photos, who to follow, etc.","title":"Requirements and Goals of the System"},{"location":"DesigningInstagram/#some-design-considerations","text":"We'll focus on designing a system that can fetch photographs quickly because the system will be read-heavy. Users can practically upload as many photographs as they like. Storage management should be a key consideration when constructing this system. When viewing photos, expect low latency. Data should be completely trustworthy. When a user submits a photograph, the system ensures that it is never lost.","title":"Some Design Considerations"},{"location":"DesigningInstagram/#capacity-estimation-and-constraints","text":"Let\u2019s assume we have 500M total users, with 1M daily active users. 2M new photos every day, 23 new photos every second. Average photo file size => 200KB Total space required for 1 day of photos 2M * 200KB => 400 GB Total space required for 10 years: 400GB * 365 (days a year) * 10 (years) ~= 1425TB","title":"Capacity Estimation and Constraints"},{"location":"DesigningInstagram/#high-level-system-design","text":"We need to enable two situations at a high level: one for uploading images and another for viewing/searching photos. Our service would require some object storage servers for photographs and database servers for metadata information.","title":"High Level System Design"},{"location":"DesigningInstagram/#database-schema","text":"\ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. We need to keep track of users' uploaded photos and the people they follow. The Photo table will record all information about a photo; we'll need an index on (PhotoID, CreationDate) because we'll want to get the most recent photos first. Because we need joins, a straight simple approach for storing the above schema would be to utilize an RDBMS like MYSQL, however relational databases have their own set of problems. especially when they need to be scaled Photos can be stored in a distributed file system like HDFS or S3. To take use of NoSQL's features, we can store the above schema in a distributed key-value store. All photo metadata can be stored in a table with the 'PhotoID' as the key and an object including PhotoLocation, UserLocation, CreationTimestamp, and so on as the value. To know who owns which photo, we need to store relationships between users and photos. We also need to keep track of who a user follows. We can use a wide-column datastore like Cassandra for both of these tables. The 'key' for the 'UserPhoto' table would be 'UserID,' and the 'value' would be the user's list of 'PhotoIDs,' kept in distinct columns. The 'UserFollow' table will follow a similar pattern. Cassandra, like all key-value stores, keeps a set number of replicas to ensure reliability. Furthermore, in such data storage, deletes are not applied immediately; data is held for a specified number of days (to allow for undeleting) before being completely erased from the system.","title":"Database Schema"},{"location":"DesigningInstagram/#data-size-estimation","text":"Let\u2019s estimate how much data will be going into each table and how much total storage we will need for 10 years. User: Assuming each \u201cint\u201d and \u201cdateTime\u201d is four bytes, each row in the User\u2019s table will be of 68 bytes: UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes If we have 500 million users, we will need 32GB of total storage. 500 million * 68 ~= 32GB Photo: Each row in Photo\u2019s table will be of 284 bytes: PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes If 2M new photos get uploaded every day, we will need 0.5GB of storage for one day: 2M * 284 bytes ~= 0.5GB per day For 10 years we will need 1.88TB of storage. UserFollow: Each row in the UserFollow table will consist of 8 bytes. If we have 500 million users and on average each user follows 500 users. We would need 1.82TB of storage for the UserFollow table: 500 million users * 500 followers * 8 bytes ~= 1.82TB Total space required for all tables for 10 years will be 3.7TB: 32GB + 1.88TB + 1.82TB ~= 3.7TB","title":"Data Size Estimation"},{"location":"DesigningInstagram/#component-design","text":"Photo uploads (or writes) are slow because they must write to the disk, while reads are faster, especially if they are served from cache. Because uploading is a slow procedure, uploading users can absorb all available connections. This means that if the system becomes overburdened with write requests,'reads' will not be served. Before developing our system, we should keep in mind that web servers have a connection restriction. We can't have more than 500 concurrent uploads or reads if we think a web server can only handle 500 connections at any given moment. We can split reading and writes into distinct services to alleviate this bottleneck. To guarantee that uploads do not clog the system, we will have dedicated servers for readers and separate servers for writes. Separating read and write requests for photos will allow us to scale and optimize each of these activities separately.","title":"Component Design"},{"location":"DesigningInstagram/#reliability-and-redundancy","text":"Our service does not allow you to lose files. As a result, we'll keep numerous copies of each picture so that if one storage server fails, we can still access the photo from another storage server. The same idea applies to the system's other components. If we want the system to be highly available, we must have several clones of services running in the system, so that even if a few services fail, the system remains operational. Redundancy eliminates the system's single point of failure. If only one instance of a service is required to run at any one time, we can deploy a redundant secondary copy that is not providing any traffic but can assume control after the original fails. Adding redundancy to a system can eliminate single points of failure and provide backup or emergency functionality. If two instances of the same service are running in production and one of them fails or degrades, the system can failover to the healthy copy. Failover can occur automatically or with the need for user intervention.","title":"Reliability and Redundancy"},{"location":"DesigningInstagram/#data-sharding","text":"Let's look at several possible metadata sharding schemes: a. Partitioning based on UserID Assume we shard based on the 'UserID' in order to maintain all of a user's images on the same shard. We'll need four DB shards to store 3.7TB of data if each DB shard is 1TB. Assume we keep 10 shards for improved performance and scalability. So we'll use UserID percent 10 to discover the shard number and then save the data there. We can attach a shard number to each PhotoID to uniquely identify any photo in our system. How can we generate PhotoIDs? Each DB shard can have its own auto-increment sequence for PhotoIDs and since we will append ShardID with each PhotoID, it will make it unique throughout our system. What are the different issues with this partitioning scheme? How would we deal with unhappy users? Many people follow such popular users, and many more people see whatever photo they post. Some users will have more photos than others, resulting in a non-uniform storage distribution. What if we can't fit all of a user's photos onto one shard? Will distributing a user's photographs across several shards result in longer latencies? Storing all of a user's images on a single shard might lead to concerns such as the loss of all of the user's data if that shard goes down, or increased latency if it is serving a large load, among other things. b. Partitioning based on PhotoID If we can generate unique PhotoIDs first and then find a shard number through \u201cPhotoID % 10\u201d, the above problems will have been solved. We would not need to append ShardID with PhotoID in this case as PhotoID will itself be unique throughout the system. How can we generate PhotoIDs? Here we cannot have an auto-incrementing sequence in each shard to define PhotoID because we need to know PhotoID first to find the shard where it will be stored. One solution could be that we dedicate a separate database instance to generate auto-incrementing IDs. If our PhotoID can fit into 64 bits, we can define a table containing only a 64 bit ID field. So whenever we would like to add a photo in our system, we can insert a new row in this table and take that ID to be our PhotoID of the new photo. Wouldn\u2019t this key generating DB be a single point of failure? Yes, it would be. A workaround for that could be defining two such databases with one generating even numbered IDs and the other odd numbered. For the MySQL, the following script can define such sequences: KeyGeneratingServer1: auto-increment-increment = 2 auto-increment-offset = 1 KeyGeneratingServer2: auto-increment-increment = 2 auto-increment-offset = 2 We can place a load balancer in front of each of these databases to ensure round robin and minimize downtime. Both servers may be out of sync, with one producing more keys than the other, but this will not affect our system. This architecture can be extended by creating distinct ID tables for Users, Photo-Comments, and other items in our system. Alternatively, we can use a 'key' creation technique like the one described in Designing a URL Shortening Service Like TinyURL. How can we prepare for our system's future expansion? To support future data growth, we can have a high number of logical partitions, so that several logical partitions can live on a single physical database server at first. We can have distinct databases for each logical partition on any server since each database server can have numerous database instances. So, whenever we suspect a database server is overburdened with data, we can move some logical partitions to another server. We can keep a configuration file (or a separate database) that maps our logical partitions to database servers, allowing us to quickly move partitions around. We only have to shift a partition when we need to.","title":"Data Sharding"},{"location":"DesigningInstagram/#ranking-and-news-feed-generation","text":"To make a user's News Feed, we need to get the most recent, popular, and relevant photographs from the people they follow. Let's pretend we need to get the top 100 photographs for a user's News Feed for the sake of simplicity. Our application server will first obtain a list of persons the user follows, and then retrieve metadata information for the user's most recent 100 images. The server will then submit all of these photographs to our ranking system, which will select the top 100 photos (based on recency, likeness, and other factors) and return them to the user. Because we must query many tables and conduct sorting, merging, and ranking on the results, this strategy may have increased latency. We can pre-generate the News Feed and store it in a separate table to increase efficiency. Generating the News Feed in Advance: We can have dedicated servers that generate users' News Feeds on a regular basis and store them in a 'UserNewsFeed' database. So, whenever a user wants the most recent photos for their News Feed, we'll just query this table and provide them the results. When these servers need to generate a user's News Feed, they first look in the UserNewsFeed database to see when the user's News Feed was last generated. From that point forward, new News Feed data will be generated (following the steps mentioned above). What are the different approaches for sending News Feed contents to the users? 1. Pull: Clients can get the contents of the News Feed from the server on a regular basis or whenever they require it manually. This strategy has the following drawbacks: a) New data may not be visible to users until clients initiate a pull request; b) Most pull requests will return an empty response if there is no new data. 2. Push: As soon as fresh data becomes available, servers can push it to users. Users must keep a Long Poll request with the server to receive updates in an effective manner. A person with a large number of followers or a superstar with millions of followers could be a problem with this strategy because the server would have to push changes often. 3. Hybrid: A hybrid approach is possible. We can switch all users with a lot of followers to a pull-based paradigm and just push data to those with a few hundred (or thousands) followers. Another option is for the server to transmit changes to all users at a set frequency, allowing those with a lot of follows/updates to pull data on a regular basis. Designing Facebook's Newsfeed contains a detailed description of News Feed production.","title":"Ranking and News Feed Generation"},{"location":"DesigningInstagram/#news-feed-creation-with-sharded-data","text":"One of the most significant requirements for creating a user's News Feed is to get the most recent photos from everyone the user follows. We'll need a way to sort images according to when they were taken. We can do this more efficiently by including photo production time in the PhotoID. We will be able to find the most recent PhotoIDs quickly since we will have a primary index on PhotoID. For this, we can utilize epoch time. Let's pretend our PhotoID has two parts: the first represents the epoch time, and the second is an auto-incrementing sequence. To create a new PhotoID, we can use the current epoch time and an auto-incrementing ID from our key-generating database. From this PhotoID (PhotoID percent 10) we can calculate the shard number and place the photo there. What is the maximum size of our PhotoID? If our epoch begins today, how many bits would be required to store the number of seconds over the following 50 years? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6 billion seconds This number would require 31 bits to store. We can devote 9 bits to store the auto incremented sequence because we predict 23 new images each second on average. So we can store (29 => 512) fresh photographs every second. Every second, we can reset our auto incrementing sequence. In Designing Twitter, we'll go over this concept in further depth under the heading 'Data Sharding.'","title":"News Feed Creation with Sharded Data"},{"location":"DesigningInstagram/#cache-and-load-balancing","text":"To serve the worldwide spread consumers, our service would require a large-scale photo delivery infrastructure. Our service should leverage a large number of geographically distributed photo cache servers and CDNs to bring its material closer to the user (for details see Caching). To cache hot database rows, we can add a cache to metadata servers. We can use Memcache to cache the data, and application servers can quickly check if the cache contains the needed rows before reaching the database. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. The least recently viewed row is discarded first under this policy. How can we build more intelligent cache? If we go with 80-20 rule, i.e., 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people read them. This dictates that we can try caching 20% of daily read volume of photos and metadata.","title":"Cache and Load balancing"},{"location":"DesigningPastebin/","text":"Designing Pastebin Problem Statement Let's create a web service that works like Pastebin and allows users to save plain text. Users will submit a text fragment and receive a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy What is Pastebin, exactly? Pastebin-style services allow users to upload plain text or images to a network (usually the Internet) and generate unique URLs to retrieve the data. Users can also utilize such services to swiftly distribute data over the network by just passing the URL to other users. If you haven't used pastebin.com before, you should try making a new 'Paste' there and spending some time exploring the various choices available. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System The following requirements should be met by our Pastebin service: Functional Requirements: Users should be able to upload or \"paste\" their data and receive a unique URL with which to view it. Users will be limited to text uploads. Data and links will automatically expire after a set period of time; users should also be allowed to set the 4. expiration time. Users should be allowed to choose a custom alias for their paste as an option. Non-Functional Requirements: The system must be extremely dependable, and any data uploaded must not be lost. The system should have a high level of availability. This is necessary because users will be unable to access their Pastes if our service is unavailable. Users should have real-time access to their Pastes with minimal delay. Paste links should be impossible to guess (not predictable). Extended Requirements: Analytics, such as the number of times a paste was accessed. Other services should be able to access our service via REST APIs. Some Design Considerations Although Pastebin and URL Shortening share similar criteria, there are also extra design considerations to keep in mind. What should the maximum quantity of text a user can paste at one time be? To prevent misuse of the service, we can limit users' Paste sizes to no more than 10MB. Should we limit the size of custom URLs? Users can use whatever URL they want because our service supports custom URLs, however giving a custom URL is not required. However, imposing a size limit on custom URLs is reasonable (and frequently desirable) in order to maintain a consistent URL database. Our services will be heavily read-heavy, with more read requests than new Pastes production. We can assume a read-to-write ratio of 5:1. Traffic estimates Pastebin services are not expected to have the same level of traffic as Twitter or Facebook, so let's pretend we get one million new pastes every day. This gives us a total of five million daily reads. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec Storage estimates: Users can upload up to 10MB of data; Pastebin-like services are frequently used to share source code, configurations, and logs. Because such texts aren't particularly large, let's say that each paste is 10KB on average. We'll be storing 10GB of data per day at this rate. 1M * 10KB => 10 GB/day We would require a total storage capacity of 36TB to store this data for ten years. In ten years, we will have 3.6 billion pastes if 1 million pastes are produced every day. To uniquely identify these pastes, we need to generate and store keys. We'd need six letters strings if we used base64 encoding ([A-Z, a-z, 0-9,., -]): 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB. Bandwidth estimates: For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service. Memory estimates: Some of the most frequently accessed hot pastes can be cached. We'd like to cache these 20% pastes based on the 80-20 rule, which states that 20% of hot pastes produce 80% of traffic. With 5 million read requests per day, caching 20% of these queries would require: 0.2 * 5M * 10KB ~= 10 GB System APIs To expose the functionality of our service, we can use SOAP or REST APIs. The APIs to create/retrieve/delete Pastes could be defined as follows: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. paste_data (string): Textual data of the paste. custom_url (string): Optional custom URL. user_name (string): Optional user name to be used to generate URL. paste_name (string): Optional name of the paste expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion will return the URL to which the paste can be accessible; otherwise, an error code will be returned. We can also have Paste APIs that retrieve and remove data: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019. Database Design A couple more observations on the data we're storing: We require billions of documents to be stored. Each metadata object that we store will be small (less than 1KB). Each paste object we store can be of average size (it can be a few MB). Except for storing which user generated which Paste, there are no linkages between entries. Our service requires extensive reading. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. 'URlHash' is the URL counterpart of the TinyURL, and 'ContentKey' is a reference to an external object that stores the paste contents; we'll go over external paste storage later in the chapter. High Level Design A high-level application layer is required to handle all read and write requests. To store and retrieve data, the application layer will communicate with the storage layer. Our storage layer can be divided into two databases, one keeping metadata about each paste, users, and so on, and the other storing the paste contents in some object storage (like Amazon S3). This data split will also allow us to grow them separately. Component Design Application layer All incoming and outgoing requests will be processed by our application layer. To serve the requests, the application servers will communicate with the backend data storage components. How should a write request be handled? When our application server receives a write request, it generates a six-letter random string that will be used as the paste key (if the user has not provided a custom key). The contents of the paste, as well as the created key, will be saved in the database by the application server. The server can return the key to the user after successful insertion. One potential issue here is that the insertion fails due to a duplicate key. Because we're producing a random key, it's possible that the freshly produced key will match an existing one. In that scenario, we should try again after generating a new key. We should keep retrying until the duplicate key does not cause failure. If the user's custom key already exists in our database, we should provide them an error message. A standalone Key Generation Service (KGS) that creates random six-letter strings and saves them in a database (let's name it key-DB) could be used to solve the aforementioned problem. We'll just use one of the previously produced keys whenever we want to save a new paste. Because we won't have to worry about duplications or collisions, this strategy will make things very simple and quick. KGS will ensure that all of the keys in key-DB are unique. KGS can keep keys in two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys to the used keys table as soon as they are sent to an application server. KGS can maintain some keys in memory at all times so that it can rapidly provide them to a server when it needs them. As soon as KGS loads certain keys into memory, it can move them to the utilized keys table, ensuring that each server has its own set of keys. We will be squandering those keys if KGS dies before using all of the keys loaded in memory. We can disregard these keys because there are so many of them. Doesn't KGS represent a single point of failure? It certainly is. To overcome this, we may create a standby replica of KGS that will take over key generation and distribution when the original server fails. Are some keys from key-DB cacheable on each app server? Yes, this will undoubtedly expedite things. However, if the application server dies before all the keys have been consumed, we will lose those keys. This may be acceptable because we have 68B unique six-letter keys, which is far more than we need. How does a paste read request get handled? The application service layer contacts the datastore when it receives a read paste request. The datastore looks for the key and returns the paste's contents if it is found. An error code is returned otherwise. Datastore layer We can divide our datastore layer into two: Metadata database: A relational database, such as MySQL, or a Distributed Key-Value Store, such as Dynamo or Cassandra, can be used. Object storage: We can save our files in Object Storage, such as Amazon's S3. We can easily enhance our content storage capacity whenever we feel like it by adding more servers. 9. Purging or DB Cleanup Please see Designing a URL Shortening service. 10. Data Partitioning and Replication Please see Designing a URL Shortening service. 11. Cache and Load Balancer Please see Designing a URL Shortening service. 12. Security and Permissions Please see Designing a URL Shortening service.","title":"Designing Pastebin"},{"location":"DesigningPastebin/#designing-pastebin","text":"","title":"Designing Pastebin"},{"location":"DesigningPastebin/#problem-statement","text":"Let's create a web service that works like Pastebin and allows users to save plain text. Users will submit a text fragment and receive a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy","title":"Problem Statement"},{"location":"DesigningPastebin/#what-is-pastebin-exactly","text":"Pastebin-style services allow users to upload plain text or images to a network (usually the Internet) and generate unique URLs to retrieve the data. Users can also utilize such services to swiftly distribute data over the network by just passing the URL to other users. If you haven't used pastebin.com before, you should try making a new 'Paste' there and spending some time exploring the various choices available.","title":"What is Pastebin, exactly?"},{"location":"DesigningPastebin/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningPastebin/#solution","text":"","title":"Solution"},{"location":"DesigningPastebin/#requirements-and-goals-of-the-system","text":"The following requirements should be met by our Pastebin service: Functional Requirements: Users should be able to upload or \"paste\" their data and receive a unique URL with which to view it. Users will be limited to text uploads. Data and links will automatically expire after a set period of time; users should also be allowed to set the 4. expiration time. Users should be allowed to choose a custom alias for their paste as an option. Non-Functional Requirements: The system must be extremely dependable, and any data uploaded must not be lost. The system should have a high level of availability. This is necessary because users will be unable to access their Pastes if our service is unavailable. Users should have real-time access to their Pastes with minimal delay. Paste links should be impossible to guess (not predictable). Extended Requirements: Analytics, such as the number of times a paste was accessed. Other services should be able to access our service via REST APIs.","title":"Requirements and Goals of the System"},{"location":"DesigningPastebin/#some-design-considerations","text":"Although Pastebin and URL Shortening share similar criteria, there are also extra design considerations to keep in mind. What should the maximum quantity of text a user can paste at one time be? To prevent misuse of the service, we can limit users' Paste sizes to no more than 10MB. Should we limit the size of custom URLs? Users can use whatever URL they want because our service supports custom URLs, however giving a custom URL is not required. However, imposing a size limit on custom URLs is reasonable (and frequently desirable) in order to maintain a consistent URL database. Our services will be heavily read-heavy, with more read requests than new Pastes production. We can assume a read-to-write ratio of 5:1. Traffic estimates Pastebin services are not expected to have the same level of traffic as Twitter or Facebook, so let's pretend we get one million new pastes every day. This gives us a total of five million daily reads. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec Storage estimates: Users can upload up to 10MB of data; Pastebin-like services are frequently used to share source code, configurations, and logs. Because such texts aren't particularly large, let's say that each paste is 10KB on average. We'll be storing 10GB of data per day at this rate. 1M * 10KB => 10 GB/day We would require a total storage capacity of 36TB to store this data for ten years. In ten years, we will have 3.6 billion pastes if 1 million pastes are produced every day. To uniquely identify these pastes, we need to generate and store keys. We'd need six letters strings if we used base64 encoding ([A-Z, a-z, 0-9,., -]): 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB. Bandwidth estimates: For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service. Memory estimates: Some of the most frequently accessed hot pastes can be cached. We'd like to cache these 20% pastes based on the 80-20 rule, which states that 20% of hot pastes produce 80% of traffic. With 5 million read requests per day, caching 20% of these queries would require: 0.2 * 5M * 10KB ~= 10 GB","title":"Some Design Considerations"},{"location":"DesigningPastebin/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs. The APIs to create/retrieve/delete Pastes could be defined as follows: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. paste_data (string): Textual data of the paste. custom_url (string): Optional custom URL. user_name (string): Optional user name to be used to generate URL. paste_name (string): Optional name of the paste expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion will return the URL to which the paste can be accessible; otherwise, an error code will be returned. We can also have Paste APIs that retrieve and remove data: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019.","title":"System APIs"},{"location":"DesigningPastebin/#database-design","text":"A couple more observations on the data we're storing: We require billions of documents to be stored. Each metadata object that we store will be small (less than 1KB). Each paste object we store can be of average size (it can be a few MB). Except for storing which user generated which Paste, there are no linkages between entries. Our service requires extensive reading. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. 'URlHash' is the URL counterpart of the TinyURL, and 'ContentKey' is a reference to an external object that stores the paste contents; we'll go over external paste storage later in the chapter.","title":"Database Design"},{"location":"DesigningPastebin/#high-level-design","text":"A high-level application layer is required to handle all read and write requests. To store and retrieve data, the application layer will communicate with the storage layer. Our storage layer can be divided into two databases, one keeping metadata about each paste, users, and so on, and the other storing the paste contents in some object storage (like Amazon S3). This data split will also allow us to grow them separately.","title":"High Level Design"},{"location":"DesigningPastebin/#component-design","text":"Application layer All incoming and outgoing requests will be processed by our application layer. To serve the requests, the application servers will communicate with the backend data storage components. How should a write request be handled? When our application server receives a write request, it generates a six-letter random string that will be used as the paste key (if the user has not provided a custom key). The contents of the paste, as well as the created key, will be saved in the database by the application server. The server can return the key to the user after successful insertion. One potential issue here is that the insertion fails due to a duplicate key. Because we're producing a random key, it's possible that the freshly produced key will match an existing one. In that scenario, we should try again after generating a new key. We should keep retrying until the duplicate key does not cause failure. If the user's custom key already exists in our database, we should provide them an error message. A standalone Key Generation Service (KGS) that creates random six-letter strings and saves them in a database (let's name it key-DB) could be used to solve the aforementioned problem. We'll just use one of the previously produced keys whenever we want to save a new paste. Because we won't have to worry about duplications or collisions, this strategy will make things very simple and quick. KGS will ensure that all of the keys in key-DB are unique. KGS can keep keys in two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys to the used keys table as soon as they are sent to an application server. KGS can maintain some keys in memory at all times so that it can rapidly provide them to a server when it needs them. As soon as KGS loads certain keys into memory, it can move them to the utilized keys table, ensuring that each server has its own set of keys. We will be squandering those keys if KGS dies before using all of the keys loaded in memory. We can disregard these keys because there are so many of them. Doesn't KGS represent a single point of failure? It certainly is. To overcome this, we may create a standby replica of KGS that will take over key generation and distribution when the original server fails. Are some keys from key-DB cacheable on each app server? Yes, this will undoubtedly expedite things. However, if the application server dies before all the keys have been consumed, we will lose those keys. This may be acceptable because we have 68B unique six-letter keys, which is far more than we need. How does a paste read request get handled? The application service layer contacts the datastore when it receives a read paste request. The datastore looks for the key and returns the paste's contents if it is found. An error code is returned otherwise. Datastore layer We can divide our datastore layer into two: Metadata database: A relational database, such as MySQL, or a Distributed Key-Value Store, such as Dynamo or Cassandra, can be used. Object storage: We can save our files in Object Storage, such as Amazon's S3. We can easily enhance our content storage capacity whenever we feel like it by adding more servers.","title":"Component Design"},{"location":"DesigningPastebin/#9-purging-or-db-cleanup","text":"Please see Designing a URL Shortening service.","title":"9. Purging or DB Cleanup"},{"location":"DesigningPastebin/#10-data-partitioning-and-replication","text":"Please see Designing a URL Shortening service.","title":"10. Data Partitioning and Replication"},{"location":"DesigningPastebin/#11-cache-and-load-balancer","text":"Please see Designing a URL Shortening service.","title":"11. Cache and Load Balancer"},{"location":"DesigningPastebin/#12-security-and-permissions","text":"Please see Designing a URL Shortening service.","title":"12. Security and Permissions"},{"location":"DesigningTwitter/","text":"Designing Twitter Problem Statement Let's create a social networking service similar to Twitter. Users will be able to submit tweets, follow other users, and favorite tweets on the service. Difficulty Level: Medium What is Twitter? Twitter is a social media platform that allows users to send and receive 140-character messages known as \"tweets.\" Only registered users can post and read tweets; non-registered users can only read them. Twitter can be accessed by the internet, SMS, or mobile app. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System We'll make a simpler version of Twitter that meets the following criteria: Functional Requirements New tweets should be able to be posted by users. Other users should be able to follow them. Tweets should be able to be marked as favorites by users. The service should be able to construct and show a user's timeline, which includes the most recent tweets from everyone the user follows. Photos and movies can be included in tweets. Non-functional Requirements Our service must be really accessible. For timeline creation, the system's acceptable latency is 200ms. Consistency may suffer (in the interest of availability); it's fine if a person doesn't see a tweet for a time. Extended Requirements Searching for tweets. Replying to a tweet. Trending topics \u2013 current hot topics/searches. Tagging other users. Tweet Notification. Who to follow? Suggestions? Moments. Capacity Estimation and Constraints Assume we have a total of one billion members, with 200 million daily active users (DAU). Assume there are 100 million new tweets every day, and each user follows 200 people on average. How many favorites are there each day? If each user favorites five tweets every day on average, we will have: 200M users * 5 favorites => 1B favorites How many total tweet-views will our system generate? Assume a user sees their timeline two times each day on average and five other people's pages. If a user sees 20 tweets on each page, our method will create 28 billion tweet views each day: 200M DAU * ((2 + 5) * 20 tweets) => 28B/day Estimates of Storage Let's imagine each tweet is 140 characters long and each character requires two bytes to store without compression. Assume that each tweet requires 30 bytes to store metadata (like ID, timestamp, user ID, etc.). Total storage space required: 100M * (280 + 30) bytes => 30GB/day What kind of storage would we require in five years? How much storage would we require for user data, preferences, and follows? This will be left for the exercise. Not all tweets will contain media; assume that every fifth tweet contains a photo and every tenth contains a video. Assume that a photo is 200KB and a video is 2MB on average. This will result in 24TB of fresh media being created every day. (100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day Estimates of Bandwidth This translates to 290MB/sec based on total ingress of 24TB per day. Keep in mind that we get 28 billion tweet views per day. We must display every tweet's photo (if it has one), but let's assume that consumers view every third video in their timeline. Total egress will thus be: (28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s Total ~= 35GB/s System APIs \ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The API for posting a new tweet may be defined as follows: tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. tweet_data (string): The text of the tweet, typically up to 140 characters. tweet_location (string): Optional location (longitude, latitude) this Tweet refers to. user_location (string): Optional location (longitude, latitude) of the user adding the tweet. media_ids (number[]): Optional list of media_ids to be associated with the Tweet. (all the media photo, video, etc. need to be uploaded separately). Returns: (string) A successful post will return the URL to access that tweet. Otherwise, an appropriate HTTP error is returned. High Level System Design We need a system that can store all new tweets efficiently (100M/86400s => 1150 tweets per second) and read them quickly (28B/86400s => 325K tweets per second). The specifications indicate that this will be a read-intensive system. To service all of these requests, we'll need many application servers with load balancers in front of them for traffic distribution. On the backend, we'll need a database that can handle a large number of reads while storing all of the new tweets. We'll also require some file storage for photographs and movies. Although we anticipate a daily writing load of 100 million tweets and a daily read load of 28 billion. This means our system will receive approximately 1160 new tweets and 325K read requests every second on average. This traffic will be distributed unevenly throughout the day, but we may expect at least a few thousand write requests and roughly 1 million read requests per second during peak hours. This is something we should bear in mind when creating our system's architecture. Database Schema We'll need to keep track of users' tweets, favorite tweets, and people they follow. See 'Database schema' under Designing Instagram for information on choosing between SQL and NoSQL databases to store the above schema. Data Sharding We need to divide our data across numerous machines so that we can read and write it quickly because we receive a large number of new tweets every day and our read load is also incredibly high. Let's go over the various choices for sharding our data one by one: Sharding based on UserID: We can try storing all of a user's data on a single server. We can provide the UserID to our hash function to map the user to a database server where all of the user's tweets, favorites, and followers will be stored. While searching for a user's tweets, follows, or favorites, we may ask our hash function where we can find the user's data and then read it from there. This strategy has several flaws: What happens if a user becomes too hot? On the server where the user is stored, there could be a lot of inquiries. This heavy load will have an impact on our service's performance. In comparison to others, certain users may accumulate a large number of tweets or followers over time. - It's challenging to keep a consistent distribution of rising user data. We must either repartition/redistribute our data or utilize consistent hashing to recover from these instances. Sharding based on TweetID: Each TweetID will be mapped to a random server where it will be stored by our hash algorithm. We must query all servers to find tweets, and each server will return a set of tweets. These results will be compiled and returned to the user by a centralized server. Consider the following example of timeline generation: To build a user's timeline, our system must complete the following steps: All of the persons the user follows will be found by our application (app) server. To find tweets from these users, the app server will issue a query to all database servers. Each database server will locate each user's tweets, organize them by recency, and return the most recent tweets. The app server will combine all of the results and sort them again to present the user with the best options. This solution overcomes the problem of hot users, however unlike sharding by UserID, we must query all database partitions to find a user's tweets, which can lead to longer latency. We can increase our performance even further by putting a cache in front of the database servers to store hot tweets. Sharding based on Tweet creation time: Storing tweets according to when they were created will allow us to swiftly retrieve all of the top tweets while just querying a limited number of servers. The issue here is that the traffic burden will not be spread; for example, while writing, all new tweets will travel to one server, leaving the other servers inactive. Similarly, when reading, the server with the most recent data will have a much higher burden than those with older data. What if we could combine TweetID sharding with Tweet creation time sharding? We can get the benefits of both techniques if we don't store tweet creation time separately and instead use TweetID to reflect it. It will be much easier to locate the most recent Tweets this way. To do this, we must make each TweetID in our system universally unique, and each TweetID should also have a timestamp. For this, we can utilize epoch time. Let's pretend our TweetID is made up of two parts: the first represents epoch seconds, and the second is an auto-incrementing sequence. To create a new TweetID, we simply append an auto-incrementing integer to the current epoch time. From this TweetID, we can calculate the shard number and save it. What is the maximum size of our TweetID? How many bits would it take to hold the number of seconds for the next 50 years if our epoch time began today? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6B This number would require 31 bits to store. We can devote 17 bits to keep auto incremented sequence because we predict 1150 new tweets every second on average; this will make our TweetID 48 bits long. So we can store (217 => 130K) fresh tweets every second. Every second, we can reset our auto incrementing sequence. We can have two database servers generate auto-incrementing keys for us, one generating even numbered keys and the other generating odd numbered keys, for fault tolerance and improved performance. Our TweetID will look like this if we assume our current epoch seconds are \"1483228800.\" 1483228800 000001 1483228800 000002 1483228800 000003 1483228800 000004 \u2026 We can easily store tweets for the next 100 years and for mili-second granularity if we make our TweetID 64bits (8 bytes) long. We still have to query all of the servers for timeline generation in the aforementioned technique, but our reads (and writes) will be much faster. Because there is no secondary index (at the time of creation), our write latency will be reduced. We don't need to filter on creation-time when reading because our primary key includes the epoch time. Cache We can utilize a database server cache to cache popular tweets and users. We can use a commercially available solution like Memcache to store the entire tweet object. Before reaching the database, application servers can rapidly check if the cache has the requested tweets. We can figure out how many cache servers we need based on client usage patterns. Which cache replacement policy would be most appropriate for our requirements? How would we chose a newer/hotter tweet to replace an old one when the cache is full? For our system, LRU (Least Recently Used) can be a suitable policy. The most recently viewed tweet gets discarded first under this strategy. How can we make a cache that is more intelligent? According to the 80-20 rule, 20% of tweets generate 80% of read traffic, implying that some tweets are so popular that the majority of people read them. This means that each shard should be able to cache 20% of the daily read volume. How about caching the most recent data? This method could be beneficial to our service. If 80% of our users only view tweets from the previous three days, we can try to cache all of the tweets from the previous three days. Assume we have dedicated cache servers that store all tweets from all users for the previous three days. Every day, we receive 100 million new tweets or 30GB of new data, as estimated above (without photos and videos). We'll need less than 100GB of memory to keep all of the tweets from the last three days. Although this data can readily fit on a single server, we should replicate it across numerous servers to spread out the read traffic and lessen the stress on cache servers. So, whenever we're creating a user's timeline, we can check to see if the cache servers have all of that user's recent tweets. If so, we may just return the entire cached data. If there aren't enough tweets in the cache, we'll have to query the backend server for more. We can try caching photographs and videos from the previous three days using a similar strategy. Our cache would be similar to a hash table, with 'OwnerID' as the key and a doubly linked list holding all of that user's tweets from the previous three days as the value. We may always enter new tweets at the head of the linked list, which means all previous tweets will be near the tail of the linked list, since we want to retrieve the most current data first. As a result, tweets in the tail can be removed to make room for fresh tweets. Timeline Generation For a detailed discussion about timeline generation, take a look at Designing Facebook\u2019s Newsfeed. Replication and Fault Tolerance We can have numerous secondary database servers for each DB partition because our system is read-heavy. Only read traffic will be sent to secondary servers. All writes will go to the primary server first, and then to the subsidiary servers. This system also provides fault tolerance, as we may failover to a secondary server if the original server fails. Load Balancing We may add a load balancing layer to our system in three places: 1) between clients and application servers, 2) between clients and application servers, and 3) between clients and application servers. 4) Between Aggregation servers and Cache servers. A simple Round Robin technique can be used at first, which evenly distributes incoming requests among servers. This Load Balancer is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, Load Balancer removes it from the rotation and stops transmitting traffic to it. Round Robin LB has the disadvantage of not taking server load into account. The Load Balancer will not cease delivering new requests to a server that is overloaded or slow. To address this, a more intelligent Load Balancer solution can be implemented, which queries the backend server about their load on a regular basis and adjusts traffic accordingly. Monitoring It is critical to be able to monitor our systems. We should collect data on a regular basis to get a quick picture of how our system is performing. To acquire a better knowledge of our service's performance, we can collect the following metrics/counters: What is the daily high of new tweets per day/second? Stats on timeline delivery, such as how many tweets are delivered each day/second by our service. The user's average latency when refreshing the timeline. We can determine whether we need more replication, load balancing, or caching by monitoring these counters. Extended Requirements How do we serve feeds? Get all of the most recent tweets from the people you follow and arrange them by time. To fetch/show tweets, use pagination. Only get the top N tweets from everyone you're following. This N will be determined by the client's Viewport, as we show fewer tweets on mobile than on a Web client. To save time, we can also cache the following top tweets. Alternatively, we can pre-generate the feed to save time; see 'Ranking and timeline production' under Designing Instagram for more information. Retweet: We can store the ID of the original Tweet on each Tweet object in the database, but no content on this retweet object. Trending Topics: In the last N seconds, we can store the most frequently occurring hashtags or search queries and update them every M seconds. The frequency of tweets, search searches, retweets, and likes can be used to rank hot topics. We can attach more weight to things that are seen by a larger number of individuals. Who to follow? How to give suggestions? This feature will increase user interaction. Friends of persons someone follows can be suggested. To locate prominent persons for the suggestions, we can go two or three tiers down. People with more followers can be given priority. Because only a few ideas may be made at a time, shuffle and re-prioritize using Machine Learning (ML). People with recently increased follow-ship, common followers if the other person is following this user, common location or interests, and so on are examples of ML signals. Moments: Using ML \u2013 supervised learning or Clustering, obtain top news for various websites over the last 1 or 2 hours, identify related tweets, prioritize them, and categorize them (news, support, financial, entertainment, etc.). These articles can then be displayed as trending topics in Moments. Search: Search involves Indexing, Ranking, and Retrieval of tweets. A similar solution is discussed in our next problem Design Twitter Search.","title":"Designing Twitter"},{"location":"DesigningTwitter/#designing-twitter","text":"","title":"Designing Twitter"},{"location":"DesigningTwitter/#problem-statement","text":"Let's create a social networking service similar to Twitter. Users will be able to submit tweets, follow other users, and favorite tweets on the service. Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningTwitter/#what-is-twitter","text":"Twitter is a social media platform that allows users to send and receive 140-character messages known as \"tweets.\" Only registered users can post and read tweets; non-registered users can only read them. Twitter can be accessed by the internet, SMS, or mobile app.","title":"What is Twitter?"},{"location":"DesigningTwitter/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningTwitter/#solution","text":"","title":"Solution"},{"location":"DesigningTwitter/#requirements-and-goals-of-the-system","text":"We'll make a simpler version of Twitter that meets the following criteria: Functional Requirements New tweets should be able to be posted by users. Other users should be able to follow them. Tweets should be able to be marked as favorites by users. The service should be able to construct and show a user's timeline, which includes the most recent tweets from everyone the user follows. Photos and movies can be included in tweets. Non-functional Requirements Our service must be really accessible. For timeline creation, the system's acceptable latency is 200ms. Consistency may suffer (in the interest of availability); it's fine if a person doesn't see a tweet for a time. Extended Requirements Searching for tweets. Replying to a tweet. Trending topics \u2013 current hot topics/searches. Tagging other users. Tweet Notification. Who to follow? Suggestions? Moments.","title":"Requirements and Goals of the System"},{"location":"DesigningTwitter/#capacity-estimation-and-constraints","text":"Assume we have a total of one billion members, with 200 million daily active users (DAU). Assume there are 100 million new tweets every day, and each user follows 200 people on average. How many favorites are there each day? If each user favorites five tweets every day on average, we will have: 200M users * 5 favorites => 1B favorites How many total tweet-views will our system generate? Assume a user sees their timeline two times each day on average and five other people's pages. If a user sees 20 tweets on each page, our method will create 28 billion tweet views each day: 200M DAU * ((2 + 5) * 20 tweets) => 28B/day Estimates of Storage Let's imagine each tweet is 140 characters long and each character requires two bytes to store without compression. Assume that each tweet requires 30 bytes to store metadata (like ID, timestamp, user ID, etc.). Total storage space required: 100M * (280 + 30) bytes => 30GB/day What kind of storage would we require in five years? How much storage would we require for user data, preferences, and follows? This will be left for the exercise. Not all tweets will contain media; assume that every fifth tweet contains a photo and every tenth contains a video. Assume that a photo is 200KB and a video is 2MB on average. This will result in 24TB of fresh media being created every day. (100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day Estimates of Bandwidth This translates to 290MB/sec based on total ingress of 24TB per day. Keep in mind that we get 28 billion tweet views per day. We must display every tweet's photo (if it has one), but let's assume that consumers view every third video in their timeline. Total egress will thus be: (28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s Total ~= 35GB/s","title":"Capacity Estimation and Constraints"},{"location":"DesigningTwitter/#system-apis","text":"\ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The API for posting a new tweet may be defined as follows: tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. tweet_data (string): The text of the tweet, typically up to 140 characters. tweet_location (string): Optional location (longitude, latitude) this Tweet refers to. user_location (string): Optional location (longitude, latitude) of the user adding the tweet. media_ids (number[]): Optional list of media_ids to be associated with the Tweet. (all the media photo, video, etc. need to be uploaded separately). Returns: (string) A successful post will return the URL to access that tweet. Otherwise, an appropriate HTTP error is returned.","title":"System APIs"},{"location":"DesigningTwitter/#high-level-system-design","text":"We need a system that can store all new tweets efficiently (100M/86400s => 1150 tweets per second) and read them quickly (28B/86400s => 325K tweets per second). The specifications indicate that this will be a read-intensive system. To service all of these requests, we'll need many application servers with load balancers in front of them for traffic distribution. On the backend, we'll need a database that can handle a large number of reads while storing all of the new tweets. We'll also require some file storage for photographs and movies. Although we anticipate a daily writing load of 100 million tweets and a daily read load of 28 billion. This means our system will receive approximately 1160 new tweets and 325K read requests every second on average. This traffic will be distributed unevenly throughout the day, but we may expect at least a few thousand write requests and roughly 1 million read requests per second during peak hours. This is something we should bear in mind when creating our system's architecture.","title":"High Level System Design"},{"location":"DesigningTwitter/#database-schema","text":"We'll need to keep track of users' tweets, favorite tweets, and people they follow. See 'Database schema' under Designing Instagram for information on choosing between SQL and NoSQL databases to store the above schema.","title":"Database Schema"},{"location":"DesigningTwitter/#data-sharding","text":"We need to divide our data across numerous machines so that we can read and write it quickly because we receive a large number of new tweets every day and our read load is also incredibly high. Let's go over the various choices for sharding our data one by one: Sharding based on UserID: We can try storing all of a user's data on a single server. We can provide the UserID to our hash function to map the user to a database server where all of the user's tweets, favorites, and followers will be stored. While searching for a user's tweets, follows, or favorites, we may ask our hash function where we can find the user's data and then read it from there. This strategy has several flaws: What happens if a user becomes too hot? On the server where the user is stored, there could be a lot of inquiries. This heavy load will have an impact on our service's performance. In comparison to others, certain users may accumulate a large number of tweets or followers over time. - It's challenging to keep a consistent distribution of rising user data. We must either repartition/redistribute our data or utilize consistent hashing to recover from these instances. Sharding based on TweetID: Each TweetID will be mapped to a random server where it will be stored by our hash algorithm. We must query all servers to find tweets, and each server will return a set of tweets. These results will be compiled and returned to the user by a centralized server. Consider the following example of timeline generation: To build a user's timeline, our system must complete the following steps: All of the persons the user follows will be found by our application (app) server. To find tweets from these users, the app server will issue a query to all database servers. Each database server will locate each user's tweets, organize them by recency, and return the most recent tweets. The app server will combine all of the results and sort them again to present the user with the best options. This solution overcomes the problem of hot users, however unlike sharding by UserID, we must query all database partitions to find a user's tweets, which can lead to longer latency. We can increase our performance even further by putting a cache in front of the database servers to store hot tweets. Sharding based on Tweet creation time: Storing tweets according to when they were created will allow us to swiftly retrieve all of the top tweets while just querying a limited number of servers. The issue here is that the traffic burden will not be spread; for example, while writing, all new tweets will travel to one server, leaving the other servers inactive. Similarly, when reading, the server with the most recent data will have a much higher burden than those with older data. What if we could combine TweetID sharding with Tweet creation time sharding? We can get the benefits of both techniques if we don't store tweet creation time separately and instead use TweetID to reflect it. It will be much easier to locate the most recent Tweets this way. To do this, we must make each TweetID in our system universally unique, and each TweetID should also have a timestamp. For this, we can utilize epoch time. Let's pretend our TweetID is made up of two parts: the first represents epoch seconds, and the second is an auto-incrementing sequence. To create a new TweetID, we simply append an auto-incrementing integer to the current epoch time. From this TweetID, we can calculate the shard number and save it. What is the maximum size of our TweetID? How many bits would it take to hold the number of seconds for the next 50 years if our epoch time began today? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6B This number would require 31 bits to store. We can devote 17 bits to keep auto incremented sequence because we predict 1150 new tweets every second on average; this will make our TweetID 48 bits long. So we can store (217 => 130K) fresh tweets every second. Every second, we can reset our auto incrementing sequence. We can have two database servers generate auto-incrementing keys for us, one generating even numbered keys and the other generating odd numbered keys, for fault tolerance and improved performance. Our TweetID will look like this if we assume our current epoch seconds are \"1483228800.\" 1483228800 000001 1483228800 000002 1483228800 000003 1483228800 000004 \u2026 We can easily store tweets for the next 100 years and for mili-second granularity if we make our TweetID 64bits (8 bytes) long. We still have to query all of the servers for timeline generation in the aforementioned technique, but our reads (and writes) will be much faster. Because there is no secondary index (at the time of creation), our write latency will be reduced. We don't need to filter on creation-time when reading because our primary key includes the epoch time.","title":"Data Sharding"},{"location":"DesigningTwitter/#cache","text":"We can utilize a database server cache to cache popular tweets and users. We can use a commercially available solution like Memcache to store the entire tweet object. Before reaching the database, application servers can rapidly check if the cache has the requested tweets. We can figure out how many cache servers we need based on client usage patterns. Which cache replacement policy would be most appropriate for our requirements? How would we chose a newer/hotter tweet to replace an old one when the cache is full? For our system, LRU (Least Recently Used) can be a suitable policy. The most recently viewed tweet gets discarded first under this strategy. How can we make a cache that is more intelligent? According to the 80-20 rule, 20% of tweets generate 80% of read traffic, implying that some tweets are so popular that the majority of people read them. This means that each shard should be able to cache 20% of the daily read volume. How about caching the most recent data? This method could be beneficial to our service. If 80% of our users only view tweets from the previous three days, we can try to cache all of the tweets from the previous three days. Assume we have dedicated cache servers that store all tweets from all users for the previous three days. Every day, we receive 100 million new tweets or 30GB of new data, as estimated above (without photos and videos). We'll need less than 100GB of memory to keep all of the tweets from the last three days. Although this data can readily fit on a single server, we should replicate it across numerous servers to spread out the read traffic and lessen the stress on cache servers. So, whenever we're creating a user's timeline, we can check to see if the cache servers have all of that user's recent tweets. If so, we may just return the entire cached data. If there aren't enough tweets in the cache, we'll have to query the backend server for more. We can try caching photographs and videos from the previous three days using a similar strategy. Our cache would be similar to a hash table, with 'OwnerID' as the key and a doubly linked list holding all of that user's tweets from the previous three days as the value. We may always enter new tweets at the head of the linked list, which means all previous tweets will be near the tail of the linked list, since we want to retrieve the most current data first. As a result, tweets in the tail can be removed to make room for fresh tweets.","title":"Cache"},{"location":"DesigningTwitter/#timeline-generation","text":"For a detailed discussion about timeline generation, take a look at Designing Facebook\u2019s Newsfeed.","title":"Timeline Generation"},{"location":"DesigningTwitter/#replication-and-fault-tolerance","text":"We can have numerous secondary database servers for each DB partition because our system is read-heavy. Only read traffic will be sent to secondary servers. All writes will go to the primary server first, and then to the subsidiary servers. This system also provides fault tolerance, as we may failover to a secondary server if the original server fails.","title":"Replication and Fault Tolerance"},{"location":"DesigningTwitter/#load-balancing","text":"We may add a load balancing layer to our system in three places: 1) between clients and application servers, 2) between clients and application servers, and 3) between clients and application servers. 4) Between Aggregation servers and Cache servers. A simple Round Robin technique can be used at first, which evenly distributes incoming requests among servers. This Load Balancer is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, Load Balancer removes it from the rotation and stops transmitting traffic to it. Round Robin LB has the disadvantage of not taking server load into account. The Load Balancer will not cease delivering new requests to a server that is overloaded or slow. To address this, a more intelligent Load Balancer solution can be implemented, which queries the backend server about their load on a regular basis and adjusts traffic accordingly.","title":"Load Balancing"},{"location":"DesigningTwitter/#monitoring","text":"It is critical to be able to monitor our systems. We should collect data on a regular basis to get a quick picture of how our system is performing. To acquire a better knowledge of our service's performance, we can collect the following metrics/counters: What is the daily high of new tweets per day/second? Stats on timeline delivery, such as how many tweets are delivered each day/second by our service. The user's average latency when refreshing the timeline. We can determine whether we need more replication, load balancing, or caching by monitoring these counters.","title":"Monitoring"},{"location":"DesigningTwitter/#extended-requirements","text":"How do we serve feeds? Get all of the most recent tweets from the people you follow and arrange them by time. To fetch/show tweets, use pagination. Only get the top N tweets from everyone you're following. This N will be determined by the client's Viewport, as we show fewer tweets on mobile than on a Web client. To save time, we can also cache the following top tweets. Alternatively, we can pre-generate the feed to save time; see 'Ranking and timeline production' under Designing Instagram for more information. Retweet: We can store the ID of the original Tweet on each Tweet object in the database, but no content on this retweet object. Trending Topics: In the last N seconds, we can store the most frequently occurring hashtags or search queries and update them every M seconds. The frequency of tweets, search searches, retweets, and likes can be used to rank hot topics. We can attach more weight to things that are seen by a larger number of individuals. Who to follow? How to give suggestions? This feature will increase user interaction. Friends of persons someone follows can be suggested. To locate prominent persons for the suggestions, we can go two or three tiers down. People with more followers can be given priority. Because only a few ideas may be made at a time, shuffle and re-prioritize using Machine Learning (ML). People with recently increased follow-ship, common followers if the other person is following this user, common location or interests, and so on are examples of ML signals. Moments: Using ML \u2013 supervised learning or Clustering, obtain top news for various websites over the last 1 or 2 hours, identify related tweets, prioritize them, and categorize them (news, support, financial, entertainment, etc.). These articles can then be displayed as trending topics in Moments. Search: Search involves Indexing, Ranking, and Retrieval of tweets. A similar solution is discussed in our next problem Design Twitter Search.","title":"Extended Requirements"},{"location":"DesigningTwitterSearch/","text":"Designing Twitter Search Problem Statement Twitter is a popular social media platform that allows users to exchange photographs, news, and text-based communications. We'll create a service that can store and search user tweets in this chapter. Similar Problems: Tweet search. Difficulty Level: Medium What is Twitter Search? Twitter users can change their status at any time. Each status (also known as a tweet) is made up of plain text, and our goal is to create a system that allows users to search through all of their tweets. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Assume Twitter has 1.5 billion overall users, with 800 million of them engaged on a daily basis. Every day, Twitter receives 400 million tweets on average. A tweet's average size is 300 bytes. Let's say there are 500 million queries every day. Multiple terms will be concatenated with AND/OR in the search query. We need to create a system that can store and query tweets quickly. Capacity Estimation and Constraints Storage Capacity: With 400 million new tweets each day and an average of 300 bytes per tweet, the total storage we'll require is: 400M * 300 => 120GB/day Total storage per second: 120GB / 24hours / 3600sec ~= 1.38MB/second System APIs To expose the functionality of our service, we can use SOAP or REST APIs; for example, the search API could be defined as follows: search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. maximum_results_to_return (number): Number of tweets to return. sort (number): Optional sort mode: Latest first (0 - default), Best matched (1), Most liked (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of tweets matching the search query. Each result entry can have the user ID & name, tweet text, tweet ID, creation time, number of likes, etc. High Level Design Which tweet contains which word? This index will aid us in rapidly locating tweets that users are looking for. Detailed Component Design 1. Storage: Every day, we must store 120GB of new data. We need to devise a data partitioning technique that will efficiently distribute the data across numerous servers, given the massive amount of data. We will require the following storage in the next five years if we plan ahead: 120GB * 365days * 5years ~= 200TB We'll need about 250TB of total storage if we never wish to be more than 80% full at any given time. Assume we wish to preserve a backup copy of all tweets for disaster recovery; our total storage need will be 500TB. If we assume that a current server can store up to 4TB of data, we'll need 125 of them to store all of the data needed during the next five years. Let's begin with a simple design that stores tweets in a MySQL database. We'll suppose that the tweets are stored in a table with two columns: TweetID and TweetText. Assume we want to organize our data by TweetID. If our TweetIDs are system-wide unique, we can build a hash function to map a TweetID to a storage server where that tweet object can be stored. How can we create unique TweetIDs for the entire system? How many tweet objects can we expect in five years if we get 400 million new tweets every day? 400M * 365 days * 5 years => 730 billion This means we'd need a five-byte integer to uniquely identify TweetIDs. Assume we have a service that can produce a unique TweetID anytime an object needs to be stored (The TweetID discussed here will be similar to TweetID discussed in Designing Twitter). We can use the TweetID to discover the storage server and put our tweet object there using our hash function. 2. Index: How should we design our index? Because our tweet queries will be made up of words, we'll need an index to tell us which word appears in which tweet object. Let's start by estimating the size of our index. If we wish to create an index for all English words as well as certain well-known nouns such as people's names, city names, and so on, and we suppose that we have roughly 300K English words and 200K nouns, we will end up with 500K total words in our index. Let's say a word has an average length of five characters. We'll need 2.5MB of RAM to hold all the words if we maintain our index in memory: 500K * 5 => 2.5 MB Let's pretend we just want to maintain the index in memory for the last two years of tweets. Since there will be 730 billion tweets in five years, we will have 292 billion tweets in two years. How much RAM will we need to store all the TweetIDs if each one is 5 bytes long? 292B * 5 => 1460 GB As a result, our index will be similar to a large distributed hash table, with the 'key' being the term and the 'value' being a list of TweetIDs for all tweets that contain that phrase. Let's assume each tweet has 40 words on average, and since we won't be indexing prepositions and other tiny words like 'the,' 'an,' and 'and,' we'll have roughly 15 words in each tweet that need to be indexed. This means that each TweetID will be saved in our index 15 times. As a result, the total memory required to store our index is: (1460 * 15) + 2.5MB ~= 21 TB We'd need 152 high-end servers to house our index, assuming each has 144GB of memory. We can divide our data into two groups based on two criteria: Word-based sharding: We will iterate through all of the terms in a tweet and calculate the hash of each word to locate the server where it will be indexed while constructing our index. To find all tweets that contain a given word, we must only query the server that has that word. We have a few difficulties with this strategy: What if a word gets popular? The server will then get a flood of queries containing that word. This heavy load will have an impact on our service's performance. Maintaining a uniform distribution of terms while tweets expand is fairly difficult because some words can end up holding a lot more TweetIDs relative to others. We can either repartition our data or utilize Consistent Hashing to recover from these circumstances. Sharding based on the tweet object: While storing, we'll give the TweetID to our hash function, which will locate the server and index all of the tweet's words. When searching for a specific term, we must query all servers, with each server returning a collection of TweetIDs. These results will be compiled and returned to the user by a centralized server. Fault Tolerance What happens if an index server goes down? We can have a secondary replica of each server that can assume control after the parent server fails. The index will be duplicated on both primary and backup servers. What if both the primary and secondary servers fail simultaneously? We'll need to set up a new server and rebuild the index there. What are our options? We have no idea what words or tweets were saved on this server. If we used 'Sharding based on the tweet object,' the brute-force method would be to iterate through the entire database, filtering TweetIDs with our hash function to find all of the required tweets that would be kept on this server. This would be inefficient, and we wouldn't be able to service any queries from the server while it was being rebuilt, resulting in the user missing some tweets that should have been seen. How can we efficiently retrieve a mapping between tweets and the index server? We need to create a reverse index that maps all TweetIDs to their respective index servers. This information can be stored on our Index-Builder server. We'll need to create a Hashtable with the index server number as the key and a HashSet containing all of the TweetIDs stored at that index server as the value. Notice how we've stored all of the TweetIDs in a HashSet; this allows us to rapidly add and remove tweets from our index. So, whenever an index server has to rebuild itself, it can simply ask the Index-Builder server for all of the tweets it requires, and then fetch those tweets to create the index. This approach will surely be fast. We should also have a replica of the Index-Builder server for fault tolerance. Cache We can put a cache in front of our database to deal with hot tweets. Memcached can be used to store all of these hot tweets in memory. Before contacting the backend database, application servers can rapidly check if the tweet is in the cache. We can modify the number of cache servers required based on client usage trends. Least Recently Used (LRU) appears to be a good cache eviction policy for our system. Load Balancing Load balancing can be added to our system in two places: 1) between clients and application servers, and 2) between application servers and backend servers. A simple Round Robin technique can be used at first, which evenly distributes incoming requests among backend servers. This Load balancer is simple to set up and has no additional overhead. Another advantage of this strategy is that Load balancer will remove dead servers from the rotation and stop transmitting traffic to them. Round Robin Load balancer has the drawback of not taking server load into account. The Load balancer will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent load balancer system can be implemented, which will periodically query the backend server about their load and modify traffic accordingly. Ranking How about ranking the search results based on social graph distance, popularity, relevance, and so on? Assume we want to rank tweets based on their popularity, such as how many likes or comments they receive, and so on. Our ranking algorithm can calculate a 'popularity number' (depending on the amount of likes, etc.) and store it with the index in this scenario. Before submitting results to the aggregator server, each partition might sort the results based on this popularity value. The aggregator server compiles all of these results, arranges them by popularity, and provides the user the top results.","title":"Designing Twitter Search"},{"location":"DesigningTwitterSearch/#designing-twitter-search","text":"","title":"Designing Twitter Search"},{"location":"DesigningTwitterSearch/#problem-statement","text":"Twitter is a popular social media platform that allows users to exchange photographs, news, and text-based communications. We'll create a service that can store and search user tweets in this chapter. Similar Problems: Tweet search. Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningTwitterSearch/#what-is-twitter-search","text":"Twitter users can change their status at any time. Each status (also known as a tweet) is made up of plain text, and our goal is to create a system that allows users to search through all of their tweets.","title":"What is Twitter Search?"},{"location":"DesigningTwitterSearch/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningTwitterSearch/#solution","text":"","title":"Solution"},{"location":"DesigningTwitterSearch/#requirements-and-goals-of-the-system","text":"Assume Twitter has 1.5 billion overall users, with 800 million of them engaged on a daily basis. Every day, Twitter receives 400 million tweets on average. A tweet's average size is 300 bytes. Let's say there are 500 million queries every day. Multiple terms will be concatenated with AND/OR in the search query. We need to create a system that can store and query tweets quickly.","title":"Requirements and Goals of the System"},{"location":"DesigningTwitterSearch/#capacity-estimation-and-constraints","text":"Storage Capacity: With 400 million new tweets each day and an average of 300 bytes per tweet, the total storage we'll require is: 400M * 300 => 120GB/day Total storage per second: 120GB / 24hours / 3600sec ~= 1.38MB/second","title":"Capacity Estimation and Constraints"},{"location":"DesigningTwitterSearch/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs; for example, the search API could be defined as follows: search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. maximum_results_to_return (number): Number of tweets to return. sort (number): Optional sort mode: Latest first (0 - default), Best matched (1), Most liked (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of tweets matching the search query. Each result entry can have the user ID & name, tweet text, tweet ID, creation time, number of likes, etc.","title":"System APIs"},{"location":"DesigningTwitterSearch/#high-level-design","text":"Which tweet contains which word? This index will aid us in rapidly locating tweets that users are looking for.","title":"High Level Design"},{"location":"DesigningTwitterSearch/#detailed-component-design","text":"1. Storage: Every day, we must store 120GB of new data. We need to devise a data partitioning technique that will efficiently distribute the data across numerous servers, given the massive amount of data. We will require the following storage in the next five years if we plan ahead: 120GB * 365days * 5years ~= 200TB We'll need about 250TB of total storage if we never wish to be more than 80% full at any given time. Assume we wish to preserve a backup copy of all tweets for disaster recovery; our total storage need will be 500TB. If we assume that a current server can store up to 4TB of data, we'll need 125 of them to store all of the data needed during the next five years. Let's begin with a simple design that stores tweets in a MySQL database. We'll suppose that the tweets are stored in a table with two columns: TweetID and TweetText. Assume we want to organize our data by TweetID. If our TweetIDs are system-wide unique, we can build a hash function to map a TweetID to a storage server where that tweet object can be stored. How can we create unique TweetIDs for the entire system? How many tweet objects can we expect in five years if we get 400 million new tweets every day? 400M * 365 days * 5 years => 730 billion This means we'd need a five-byte integer to uniquely identify TweetIDs. Assume we have a service that can produce a unique TweetID anytime an object needs to be stored (The TweetID discussed here will be similar to TweetID discussed in Designing Twitter). We can use the TweetID to discover the storage server and put our tweet object there using our hash function. 2. Index: How should we design our index? Because our tweet queries will be made up of words, we'll need an index to tell us which word appears in which tweet object. Let's start by estimating the size of our index. If we wish to create an index for all English words as well as certain well-known nouns such as people's names, city names, and so on, and we suppose that we have roughly 300K English words and 200K nouns, we will end up with 500K total words in our index. Let's say a word has an average length of five characters. We'll need 2.5MB of RAM to hold all the words if we maintain our index in memory: 500K * 5 => 2.5 MB Let's pretend we just want to maintain the index in memory for the last two years of tweets. Since there will be 730 billion tweets in five years, we will have 292 billion tweets in two years. How much RAM will we need to store all the TweetIDs if each one is 5 bytes long? 292B * 5 => 1460 GB As a result, our index will be similar to a large distributed hash table, with the 'key' being the term and the 'value' being a list of TweetIDs for all tweets that contain that phrase. Let's assume each tweet has 40 words on average, and since we won't be indexing prepositions and other tiny words like 'the,' 'an,' and 'and,' we'll have roughly 15 words in each tweet that need to be indexed. This means that each TweetID will be saved in our index 15 times. As a result, the total memory required to store our index is: (1460 * 15) + 2.5MB ~= 21 TB We'd need 152 high-end servers to house our index, assuming each has 144GB of memory. We can divide our data into two groups based on two criteria: Word-based sharding: We will iterate through all of the terms in a tweet and calculate the hash of each word to locate the server where it will be indexed while constructing our index. To find all tweets that contain a given word, we must only query the server that has that word. We have a few difficulties with this strategy: What if a word gets popular? The server will then get a flood of queries containing that word. This heavy load will have an impact on our service's performance. Maintaining a uniform distribution of terms while tweets expand is fairly difficult because some words can end up holding a lot more TweetIDs relative to others. We can either repartition our data or utilize Consistent Hashing to recover from these circumstances. Sharding based on the tweet object: While storing, we'll give the TweetID to our hash function, which will locate the server and index all of the tweet's words. When searching for a specific term, we must query all servers, with each server returning a collection of TweetIDs. These results will be compiled and returned to the user by a centralized server.","title":"Detailed Component Design"},{"location":"DesigningTwitterSearch/#fault-tolerance","text":"What happens if an index server goes down? We can have a secondary replica of each server that can assume control after the parent server fails. The index will be duplicated on both primary and backup servers. What if both the primary and secondary servers fail simultaneously? We'll need to set up a new server and rebuild the index there. What are our options? We have no idea what words or tweets were saved on this server. If we used 'Sharding based on the tweet object,' the brute-force method would be to iterate through the entire database, filtering TweetIDs with our hash function to find all of the required tweets that would be kept on this server. This would be inefficient, and we wouldn't be able to service any queries from the server while it was being rebuilt, resulting in the user missing some tweets that should have been seen. How can we efficiently retrieve a mapping between tweets and the index server? We need to create a reverse index that maps all TweetIDs to their respective index servers. This information can be stored on our Index-Builder server. We'll need to create a Hashtable with the index server number as the key and a HashSet containing all of the TweetIDs stored at that index server as the value. Notice how we've stored all of the TweetIDs in a HashSet; this allows us to rapidly add and remove tweets from our index. So, whenever an index server has to rebuild itself, it can simply ask the Index-Builder server for all of the tweets it requires, and then fetch those tweets to create the index. This approach will surely be fast. We should also have a replica of the Index-Builder server for fault tolerance.","title":"Fault Tolerance"},{"location":"DesigningTwitterSearch/#cache","text":"We can put a cache in front of our database to deal with hot tweets. Memcached can be used to store all of these hot tweets in memory. Before contacting the backend database, application servers can rapidly check if the tweet is in the cache. We can modify the number of cache servers required based on client usage trends. Least Recently Used (LRU) appears to be a good cache eviction policy for our system.","title":"Cache"},{"location":"DesigningTwitterSearch/#load-balancing","text":"Load balancing can be added to our system in two places: 1) between clients and application servers, and 2) between application servers and backend servers. A simple Round Robin technique can be used at first, which evenly distributes incoming requests among backend servers. This Load balancer is simple to set up and has no additional overhead. Another advantage of this strategy is that Load balancer will remove dead servers from the rotation and stop transmitting traffic to them. Round Robin Load balancer has the drawback of not taking server load into account. The Load balancer will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent load balancer system can be implemented, which will periodically query the backend server about their load and modify traffic accordingly.","title":"Load Balancing"},{"location":"DesigningTwitterSearch/#ranking","text":"How about ranking the search results based on social graph distance, popularity, relevance, and so on? Assume we want to rank tweets based on their popularity, such as how many likes or comments they receive, and so on. Our ranking algorithm can calculate a 'popularity number' (depending on the amount of likes, etc.) and store it with the index in this scenario. Before submitting results to the aggregator server, each partition might sort the results based on this popularity value. The aggregator server compiles all of these results, arranges them by popularity, and provides the user the top results.","title":"Ranking"},{"location":"DesigningTypeaheadSuggestion/","text":"Designing Typeahead Suggestion Problem Statement Let's create a real-time suggestion service that suggests terms to consumers as they type text into a search box. Similar Services: Auto-suggestions, Typeahead search Difficulty: Medium What is Typeahead Suggestion? Users can use typeahead suggestions to find well-known and commonly searched terms. As the user puts into the search box, it attempts to guess the query based on the characters entered and provides a list of possible answers. Typeahead suggestions assist users in better articulating their search queries. It's not about speeding up the search process; instead, it's about guiding people and assisting them in formulating their search query. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Functional Requirements: Our service should provide the top 10 terms starting with whatever the user has written as they type their query. Non-function Requirements: Suggestions should be displayed in real time. Within 200 milliseconds, the user should be able to see the suggestions. Basic System Design and Algorithm The issue we're addressing is that we have a large number of'strings' that we need to store in a form that allows customers to search using any prefix. Next terms that match the supplied prefix will be suggested by our service. For example, if the user types in 'cap' and our database has the terms cap, cat, captain, or capital, our system should propose 'cap, captain, and capital.' We need a scheme that can efficiently store our data so that it can be queried rapidly because we need to serve a lot of queries with minimal latency. We can't rely on a database for this; we'll have to store our index in memory using a fast data structure. The Trie (pronounced \"try\") is one of the most suited data structures for our needs. A trie is a tree-like data structure for storing phrases, with each node storing a character in the phrase in order. If we need to store 'cap, cat, caption, captain, capital' in the trie, for example, it would look like this: Our service can now traverse the trie to the node 'P' to locate all the phrases that begin with this prefix if the user types 'cap' (e.g., cap-tion, cap-ital etc). To conserve storage capacity, we can merge nodes with only one branch. This is how the above trie can be saved: Should case-insensitive trie be used? Let's pretend our data is case-insensitive for the sake of simplicity and searchability. How do I discover the best recommendation? How can we determine the top 10 terms for a particular prefix now that we've found all of the terms for that prefix? One simple option is to keep track of the number of searches that ended at each node; for example, if users searched for 'CAPTAIN' 100 times and 'CAPTION' 500 times, we can keep track of this number by the phrase's last character. We now know that the top most searched word under the prefix 'CAP' is 'CAPTION' if the user inputs 'CAP'. We may then explore the sub-tree beneath it to find the top choices for a given prefix. How long will it take to traverse the sub-tree of a given prefix? We should expect a massive tree given the amount of data we need to index. Even traversing a sub-tree might take a long time; for example, the term \"system design interview questions\" has 30 levels. We need to improve the efficiency of our solution because we have very severe latency constraints. Can each node store the top suggestions? This will undoubtedly speed up our searches, but it will necessitate a significant amount of more storage. At each node, we can save the top 10 suggestions and return them to the user. To reach the desired efficiency, we must bear a significant increase in our storage capacity. Instead of saving the complete phrase, we can save space by storing only references to the terminal nodes. We must go back using the parent reference from the terminal node to find the proposed phrases. To keep track of top ideas, we'll need to save the frequency with each reference. How would we construct this trie? We can efficiently construct our trie from the bottom up. To compute their top suggestions and counts, each parent node will recursively call all of the child nodes. To select their top choices, parent nodes will combine the top proposals from all of their children. How should the trie be updated? Assuming five billion searches each day, this equates to about 60K queries per second. If we try to update our trie after every query, it will consume a lot of resources and may slow down our read requests. One way to deal with this might be to update our trie offline after a set period of time. We can log new requests as they come in and track their frequency. We can either log every query or sample and log the 1000th query. It's okay to log every 1000th searched term if we don't want to show a term that has been searched less than 1000 times. We can put up a Map-Reduce (MR) system to handle all of the logging data on a regular basis, say once an hour. These MR tasks will calculate the frequency of all terms searched in the previous hour. We may then add this additional information to our trie. We can update the trie's current snapshot with all of the new terms and their frequency. We should perform this offline because we don't want update trie requests to obstruct our read queries. There are two possibilities: To update the trie offline, we can make a copy of it on each server. After that, we can start using it and throw away the old one. Another alternative is to set up each trie server as a master-slave arrangement. While the master is serving traffic, we can update the slave. We can make the slave our new master after the update is complete. We can then upgrade our old master, which will then be able to serve traffic as well. How can the frequency of typeahead suggestions be updated? Because the frequencies of our typeahead suggestions are stored with each node, we must also update them! Rather than recreating all search phrases from beginning, we can merely update changes in frequency. If we want to maintain track of all the phrases searched in the last 10 days, we'll have to deduct the counts from the time period that is no longer included and add the counts for the new time period. Based on the Exponential Moving Average (EMA) of each term, we can add and subtract frequencies. The most recent data is given more weight in EMA. The exponentially weighted moving average is another name for it. We'll proceed to the phrase's terminal node and boost its frequency after we've added a new term to the trie. Because each node saves the top 10 searches, it's likely that this search term leaped into the top 10 queries of a few additional nodes. The top 10 queries of those nodes must then be updated. We must return from the node all the way to the root. We check if the current query is in the top 10 for each parent. If so, we update the corresponding frequency. If not, we check if the current query\u2019s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency. How can we get a term out of the trie? Let's imagine we need to eliminate a term from the trial due to a legal concern, hatred, or piracy, for example. When the normal update occurs, we can totally delete such phrases from the trie; in the meantime, we can add a filtering layer to each server that will remove any such terms before transmitting them to users. What different ranking criteria may there be for suggestions? In addition to a simple count, other criteria such as freshness, user location, language, demographics, personal history, and so on must be considered when ranking phrases. Permanent Storage of the Trie How can we keep trie in a file so that we can easily reconstruct our trie when the system restarts? Periodically, we can take a snapshot of our trie and save it to a file. If the server goes down, we'll be able to reconstruct the trie. Starting with the root node, we can save the trie level by level. We can keep track of the characters each node includes and how many children it has. We should put all of the children of each node right after it. Let's pretend we've got the following trie: This trie will be stored in a file using the above-mentioned scheme: \"C2,A2,R1,T,P,O1,D.\" We can easily reconstruct our trie with this information. We don't store top ideas and their counts with each node, as you may have seen. It's difficult to preserve this information since, because our trie is organized top down, we don't have any child nodes generated before the parent, therefore there's no convenient method to keep track of their references. This requires recalculating all of the top terms with counts. This can be done as the trie is being constructed. Each node will calculate and provide its top recommendations to its parent. To determine its top suggestions, each parent node will combine the findings of all of its offspring. Scale Estimation If we establish a service on the same scale as Google, we can expect 5 billion searches each day, or about 60K requests per second. We can predict that just 20% of the 5 billion inquiries will be unique due to the large number of duplicates. We can exclude a lot of less often searched queries if we only want to index the top 50% of search phrases. Assume there are 100 million distinct phrases for which we wish to create an index. Storage Estimation: If each query has an average length of 3 words and a word has an average length of 5 characters, the average query size will be 15 characters. If a character takes two bytes to store, an average query will take 30 bytes to store. So, in total, we'll require: 100 million * 30 bytes => 3 GB Every day, we can expect some growth in this data, but we should also remove some terms that are no longer searched. If we assume that we receive 2% new requests per day and that we keep our index for the last year, total storage should be: 3GB + (0.02 * 3 GB * 365 days) => 25 GB Data Partition Even though our index can easily fit on a single server, we can still split it to fulfill our efficiency and latency requirements. How can we split our data efficiently so that it may be distributed across numerous servers? a. Range Based Partitioning: What if we divided our phrases into groups based on their first letter? As a result, we save all phrases beginning with the letter 'A' in one partition, those beginning with the letter 'B' in another, and so on. We can even merge a few letters that aren't used very often into one division. This partitioning scheme should be created statically so that we can always store and search phrases in a predictable manner. The biggest issue with this strategy is that it can result in unbalanced servers, such as if we decide to place all terms beginning with the letter 'E' into one partition, only to discover later that we have too many terms beginning with the letter 'E' to fit into one partition. We can see that the problem described above will occur with any statically stated scheme. It is impossible to determine whether each of our partitions will fit on a single server in a static manner. b. Partition based on the maximum capacity of the server: Let's imagine we divide our trie according to the servers' maximum memory capacity. We can continue to store data on a server as long as it has memory. We break our partition there to allocate that range to this server and go on to the next server to continue this process whenever a sub-tree cannot fit into a server. Assume that our first trie server can store all terms from 'A' to 'AABC,' implying that our next server will store phrases from 'AABD' onwards. If our second server has enough space to store 'BXA,' the third server will begin with 'BXB,' and so on. To conveniently retrieve this partitioning strategy, we can keep a hash table: Server 1, A-AABC Server 2, AABD-BXA Server 3, BXB-CDA If the user types 'A,' we must query both server 1 and server 2 to discover the top choices. We still have to query servers 1 and 2 when the user types 'AA,' but when the user types 'AAA,' we only have to query server 1. In front of our trie servers, we can put a load balancer that can store this mapping and divert traffic. Also, if we're querying many servers, we'll either have to merge the results on the server side to get the overall top results, or we'll have to rely on our clients to do it. If we wish to achieve this on the server side, we'll need to add another layer of servers (let's call them aggregators) between load balancers and trie servers. The top results from several trie servers will be returned to the client by these servers. Partitioning based on maximum capacity might still lead to hotspots; for example, if there are a lot of queries for terms that start with 'cap,' the server that holds it will be under a lot of strain relative to others. c. Partition based on the hash of the term: Each phrase will be passed via a hash function, which will generate a server number, which will be used to store the word. As a result, our word distribution will be random, reducing hotspots. The downside of this technique is that we have to ask all of the servers for typeahead suggestions for a term and then aggregate the responses. Cache We must understand that caching the most often searched terms will be incredibly beneficial to our business. A small fraction of inquiries will be responsible for the majority of the traffic. Separate cache servers can be placed in front of the trie servers to store the most commonly searched phrases and typeahead suggestions. Before hitting the trie servers, application servers should check these cache servers to see if they have the desired searched terms. This will allow us to travel the tri faster. We can also create a simple Machine Learning (ML) model that uses simple counting, personalisation, or trending data to forecast interaction on each proposal and cache these terms in advance. Load Balancer and Replication Our trie servers should have replicas for load balancing as well as fault tolerance. A load balancer that maintains track of our data partitioning method and redirects traffic depending on prefixes is also required. Fault Tolerance What happens if one of the trie servers goes down? We can have a master-slave arrangement, as mentioned previously; if the master dies, the slave can take over after failover. Any server that restarts can recreate the trie using the most recent snapshot. Typeahead Client On the client side, we can make the following changes to improve the user experience: The client should only attempt to contact the server if the user has been idle for 50 milliseconds. The client can cancel in-progress requests if the user is constantly typing. The client can initially wait till the user types a few characters. Clients can save time by pre-fetching some data from the server. Clients can keep track of recent suggestions locally. The rate of reuse in recent history is extremely high. One of the most crucial things is establishing an early connection with the server. The client can establish a connection with the server as soon as the user visits the search engine page. As a result, the client does not waste time establishing the connection when the user types the first character. For efficiency, the server can push some of their cache to CDNs and Internet Service Providers (ISPs). Personalization Users will receive typeahead suggestions based on previous searches, location, language, and other factors. We can save each user's personal history on the server separately and cache it on the client. Before transmitting the final set to the user, the server can include these customised phrases. Personal searches should always take precedence over others.","title":"Designing Typeahead Suggestion"},{"location":"DesigningTypeaheadSuggestion/#designing-typeahead-suggestion","text":"","title":"Designing Typeahead Suggestion"},{"location":"DesigningTypeaheadSuggestion/#problem-statement","text":"Let's create a real-time suggestion service that suggests terms to consumers as they type text into a search box. Similar Services: Auto-suggestions, Typeahead search Difficulty: Medium","title":"Problem Statement"},{"location":"DesigningTypeaheadSuggestion/#what-is-typeahead-suggestion","text":"Users can use typeahead suggestions to find well-known and commonly searched terms. As the user puts into the search box, it attempts to guess the query based on the characters entered and provides a list of possible answers. Typeahead suggestions assist users in better articulating their search queries. It's not about speeding up the search process; instead, it's about guiding people and assisting them in formulating their search query.","title":"What is Typeahead Suggestion?"},{"location":"DesigningTypeaheadSuggestion/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningTypeaheadSuggestion/#solution","text":"","title":"Solution"},{"location":"DesigningTypeaheadSuggestion/#requirements-and-goals-of-the-system","text":"Functional Requirements: Our service should provide the top 10 terms starting with whatever the user has written as they type their query. Non-function Requirements: Suggestions should be displayed in real time. Within 200 milliseconds, the user should be able to see the suggestions.","title":"Requirements and Goals of the System"},{"location":"DesigningTypeaheadSuggestion/#basic-system-design-and-algorithm","text":"The issue we're addressing is that we have a large number of'strings' that we need to store in a form that allows customers to search using any prefix. Next terms that match the supplied prefix will be suggested by our service. For example, if the user types in 'cap' and our database has the terms cap, cat, captain, or capital, our system should propose 'cap, captain, and capital.' We need a scheme that can efficiently store our data so that it can be queried rapidly because we need to serve a lot of queries with minimal latency. We can't rely on a database for this; we'll have to store our index in memory using a fast data structure. The Trie (pronounced \"try\") is one of the most suited data structures for our needs. A trie is a tree-like data structure for storing phrases, with each node storing a character in the phrase in order. If we need to store 'cap, cat, caption, captain, capital' in the trie, for example, it would look like this: Our service can now traverse the trie to the node 'P' to locate all the phrases that begin with this prefix if the user types 'cap' (e.g., cap-tion, cap-ital etc). To conserve storage capacity, we can merge nodes with only one branch. This is how the above trie can be saved: Should case-insensitive trie be used? Let's pretend our data is case-insensitive for the sake of simplicity and searchability. How do I discover the best recommendation? How can we determine the top 10 terms for a particular prefix now that we've found all of the terms for that prefix? One simple option is to keep track of the number of searches that ended at each node; for example, if users searched for 'CAPTAIN' 100 times and 'CAPTION' 500 times, we can keep track of this number by the phrase's last character. We now know that the top most searched word under the prefix 'CAP' is 'CAPTION' if the user inputs 'CAP'. We may then explore the sub-tree beneath it to find the top choices for a given prefix. How long will it take to traverse the sub-tree of a given prefix? We should expect a massive tree given the amount of data we need to index. Even traversing a sub-tree might take a long time; for example, the term \"system design interview questions\" has 30 levels. We need to improve the efficiency of our solution because we have very severe latency constraints. Can each node store the top suggestions? This will undoubtedly speed up our searches, but it will necessitate a significant amount of more storage. At each node, we can save the top 10 suggestions and return them to the user. To reach the desired efficiency, we must bear a significant increase in our storage capacity. Instead of saving the complete phrase, we can save space by storing only references to the terminal nodes. We must go back using the parent reference from the terminal node to find the proposed phrases. To keep track of top ideas, we'll need to save the frequency with each reference. How would we construct this trie? We can efficiently construct our trie from the bottom up. To compute their top suggestions and counts, each parent node will recursively call all of the child nodes. To select their top choices, parent nodes will combine the top proposals from all of their children. How should the trie be updated? Assuming five billion searches each day, this equates to about 60K queries per second. If we try to update our trie after every query, it will consume a lot of resources and may slow down our read requests. One way to deal with this might be to update our trie offline after a set period of time. We can log new requests as they come in and track their frequency. We can either log every query or sample and log the 1000th query. It's okay to log every 1000th searched term if we don't want to show a term that has been searched less than 1000 times. We can put up a Map-Reduce (MR) system to handle all of the logging data on a regular basis, say once an hour. These MR tasks will calculate the frequency of all terms searched in the previous hour. We may then add this additional information to our trie. We can update the trie's current snapshot with all of the new terms and their frequency. We should perform this offline because we don't want update trie requests to obstruct our read queries. There are two possibilities: To update the trie offline, we can make a copy of it on each server. After that, we can start using it and throw away the old one. Another alternative is to set up each trie server as a master-slave arrangement. While the master is serving traffic, we can update the slave. We can make the slave our new master after the update is complete. We can then upgrade our old master, which will then be able to serve traffic as well. How can the frequency of typeahead suggestions be updated? Because the frequencies of our typeahead suggestions are stored with each node, we must also update them! Rather than recreating all search phrases from beginning, we can merely update changes in frequency. If we want to maintain track of all the phrases searched in the last 10 days, we'll have to deduct the counts from the time period that is no longer included and add the counts for the new time period. Based on the Exponential Moving Average (EMA) of each term, we can add and subtract frequencies. The most recent data is given more weight in EMA. The exponentially weighted moving average is another name for it. We'll proceed to the phrase's terminal node and boost its frequency after we've added a new term to the trie. Because each node saves the top 10 searches, it's likely that this search term leaped into the top 10 queries of a few additional nodes. The top 10 queries of those nodes must then be updated. We must return from the node all the way to the root. We check if the current query is in the top 10 for each parent. If so, we update the corresponding frequency. If not, we check if the current query\u2019s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency. How can we get a term out of the trie? Let's imagine we need to eliminate a term from the trial due to a legal concern, hatred, or piracy, for example. When the normal update occurs, we can totally delete such phrases from the trie; in the meantime, we can add a filtering layer to each server that will remove any such terms before transmitting them to users. What different ranking criteria may there be for suggestions? In addition to a simple count, other criteria such as freshness, user location, language, demographics, personal history, and so on must be considered when ranking phrases.","title":"Basic System Design and Algorithm"},{"location":"DesigningTypeaheadSuggestion/#permanent-storage-of-the-trie","text":"How can we keep trie in a file so that we can easily reconstruct our trie when the system restarts? Periodically, we can take a snapshot of our trie and save it to a file. If the server goes down, we'll be able to reconstruct the trie. Starting with the root node, we can save the trie level by level. We can keep track of the characters each node includes and how many children it has. We should put all of the children of each node right after it. Let's pretend we've got the following trie: This trie will be stored in a file using the above-mentioned scheme: \"C2,A2,R1,T,P,O1,D.\" We can easily reconstruct our trie with this information. We don't store top ideas and their counts with each node, as you may have seen. It's difficult to preserve this information since, because our trie is organized top down, we don't have any child nodes generated before the parent, therefore there's no convenient method to keep track of their references. This requires recalculating all of the top terms with counts. This can be done as the trie is being constructed. Each node will calculate and provide its top recommendations to its parent. To determine its top suggestions, each parent node will combine the findings of all of its offspring.","title":"Permanent Storage of the Trie"},{"location":"DesigningTypeaheadSuggestion/#scale-estimation","text":"If we establish a service on the same scale as Google, we can expect 5 billion searches each day, or about 60K requests per second. We can predict that just 20% of the 5 billion inquiries will be unique due to the large number of duplicates. We can exclude a lot of less often searched queries if we only want to index the top 50% of search phrases. Assume there are 100 million distinct phrases for which we wish to create an index. Storage Estimation: If each query has an average length of 3 words and a word has an average length of 5 characters, the average query size will be 15 characters. If a character takes two bytes to store, an average query will take 30 bytes to store. So, in total, we'll require: 100 million * 30 bytes => 3 GB Every day, we can expect some growth in this data, but we should also remove some terms that are no longer searched. If we assume that we receive 2% new requests per day and that we keep our index for the last year, total storage should be: 3GB + (0.02 * 3 GB * 365 days) => 25 GB","title":"Scale Estimation"},{"location":"DesigningTypeaheadSuggestion/#data-partition","text":"Even though our index can easily fit on a single server, we can still split it to fulfill our efficiency and latency requirements. How can we split our data efficiently so that it may be distributed across numerous servers? a. Range Based Partitioning: What if we divided our phrases into groups based on their first letter? As a result, we save all phrases beginning with the letter 'A' in one partition, those beginning with the letter 'B' in another, and so on. We can even merge a few letters that aren't used very often into one division. This partitioning scheme should be created statically so that we can always store and search phrases in a predictable manner. The biggest issue with this strategy is that it can result in unbalanced servers, such as if we decide to place all terms beginning with the letter 'E' into one partition, only to discover later that we have too many terms beginning with the letter 'E' to fit into one partition. We can see that the problem described above will occur with any statically stated scheme. It is impossible to determine whether each of our partitions will fit on a single server in a static manner. b. Partition based on the maximum capacity of the server: Let's imagine we divide our trie according to the servers' maximum memory capacity. We can continue to store data on a server as long as it has memory. We break our partition there to allocate that range to this server and go on to the next server to continue this process whenever a sub-tree cannot fit into a server. Assume that our first trie server can store all terms from 'A' to 'AABC,' implying that our next server will store phrases from 'AABD' onwards. If our second server has enough space to store 'BXA,' the third server will begin with 'BXB,' and so on. To conveniently retrieve this partitioning strategy, we can keep a hash table: Server 1, A-AABC Server 2, AABD-BXA Server 3, BXB-CDA If the user types 'A,' we must query both server 1 and server 2 to discover the top choices. We still have to query servers 1 and 2 when the user types 'AA,' but when the user types 'AAA,' we only have to query server 1. In front of our trie servers, we can put a load balancer that can store this mapping and divert traffic. Also, if we're querying many servers, we'll either have to merge the results on the server side to get the overall top results, or we'll have to rely on our clients to do it. If we wish to achieve this on the server side, we'll need to add another layer of servers (let's call them aggregators) between load balancers and trie servers. The top results from several trie servers will be returned to the client by these servers. Partitioning based on maximum capacity might still lead to hotspots; for example, if there are a lot of queries for terms that start with 'cap,' the server that holds it will be under a lot of strain relative to others. c. Partition based on the hash of the term: Each phrase will be passed via a hash function, which will generate a server number, which will be used to store the word. As a result, our word distribution will be random, reducing hotspots. The downside of this technique is that we have to ask all of the servers for typeahead suggestions for a term and then aggregate the responses.","title":"Data Partition"},{"location":"DesigningTypeaheadSuggestion/#cache","text":"We must understand that caching the most often searched terms will be incredibly beneficial to our business. A small fraction of inquiries will be responsible for the majority of the traffic. Separate cache servers can be placed in front of the trie servers to store the most commonly searched phrases and typeahead suggestions. Before hitting the trie servers, application servers should check these cache servers to see if they have the desired searched terms. This will allow us to travel the tri faster. We can also create a simple Machine Learning (ML) model that uses simple counting, personalisation, or trending data to forecast interaction on each proposal and cache these terms in advance.","title":"Cache"},{"location":"DesigningTypeaheadSuggestion/#load-balancer-and-replication","text":"Our trie servers should have replicas for load balancing as well as fault tolerance. A load balancer that maintains track of our data partitioning method and redirects traffic depending on prefixes is also required.","title":"Load Balancer and Replication"},{"location":"DesigningTypeaheadSuggestion/#fault-tolerance","text":"What happens if one of the trie servers goes down? We can have a master-slave arrangement, as mentioned previously; if the master dies, the slave can take over after failover. Any server that restarts can recreate the trie using the most recent snapshot.","title":"Fault Tolerance"},{"location":"DesigningTypeaheadSuggestion/#typeahead-client","text":"On the client side, we can make the following changes to improve the user experience: The client should only attempt to contact the server if the user has been idle for 50 milliseconds. The client can cancel in-progress requests if the user is constantly typing. The client can initially wait till the user types a few characters. Clients can save time by pre-fetching some data from the server. Clients can keep track of recent suggestions locally. The rate of reuse in recent history is extremely high. One of the most crucial things is establishing an early connection with the server. The client can establish a connection with the server as soon as the user visits the search engine page. As a result, the client does not waste time establishing the connection when the user types the first character. For efficiency, the server can push some of their cache to CDNs and Internet Service Providers (ISPs).","title":"Typeahead Client"},{"location":"DesigningTypeaheadSuggestion/#personalization","text":"Users will receive typeahead suggestions based on previous searches, location, language, and other factors. We can save each user's personal history on the server separately and cache it on the client. Before transmitting the final set to the user, the server can include these customised phrases. Personal searches should always take precedence over others.","title":"Personalization"},{"location":"DesigningURLShorteningService/","text":"Creating a TinyURL-style URL shortening service Problem Statement Let's make a TinyURL-style URL shortening service. Short aliases for long URLs will be provided by this service. Bit.ly, goo.gl, qlink.me, and other similar services Level of Difficulty: Easy What is the purpose of URL shortening? For long URLs, URL shortening is utilized to create shorter aliases. These shortened aliases are referred to as \"short links.\" When users click on these short links, they are forwarded to the original URL. When displayed, printed, messaged, or tweeted, short links save a lot of space. Shorter URLs are also less likely to be mistyped by users. For example, if we use TinyURL to shorten this page: https://www.jayaaemekar.io/collection/page/5668639101419520/ We would get: http://tinyurl.com/jlg8zpc The abbreviated URL is almost one-third the length of the original. The abbreviated URL is almost one-third the length of the original. URL shortening is used for a variety of purposes, including optimizing links across devices, tracking individual links to gauge audience and campaign performance, and concealing connected original URLs. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System \ud83d\udca1 At the start of the interview, you should always outline criteria. Ask questions to figure out the exact extent of the system that the interviewer is thinking of. The following requirements should be met by our URL shortening system: Functional Requirements: Given a URL, our service should create a unique and shorter alias for it. This is referred to as a short link. This URL should be short enough to be copied and pasted into programs without difficulty. Our service should redirect visitors to the original site when they click on a short link. Users should be allowed to choose a custom short link for their URL as an option. Links will expire after a set amount of time. The expiration time should be configurable by the user. Non-Functional Requirements: The system should have a high level of availability. This is necessary because if our service is unavailable, all URL redirections would fail. URL redirection should take place in real time with the least amount of latency possible. It should not be possible to guess the length of shortened connections (not predictable). Extended Requirements: Analytical data; for example, how many times has a redirection occurred? Our service should also be accessible through REST APIs by other services. Capacity Estimation and Constraints Our system will rely heavily on reading. In comparison to new URL shortenings, there will be a lot of redirection requests. Assume that read and write have a 100:1 ratio. Traffic estimates: If we assume 500 million new URL shortenings every month and a 100:1 read/write ratio, we can expect 50 billion redirections in the same time period: 100 * 500M => 50B What would our system's Queries Per Second (QPS) be? Per second, new URL shortenings: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let's say we keep track of every URL shortening request (and the abbreviated link that goes with it) for five years. With 500 million new URLs expected per month, the total number of objects we expect to store is 30 billion: 500 million * 5 years * 12 months = 30 billion Let's assume that each stored object is 500 bytes in size (this is just a guess\u2013we'll look at it later). We'll need a total of 15TB of storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: How much RAM will we need to keep some of the most often visited URLs if we wish to cache them? We'd like to cache this 20% of hot URLs if we follow the 80-20 rule, which states that 20% of URLs produce 80% of traffic. We'll get 1.7 billion requests each day if we have 20K requests per second: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to keep in mind is that because there will be a lot of duplicate requests (for the same URL), our actual memory consumption will be less than 170GB. Estimates at a high level: The following is a summary of our high-level estimations for our service, assuming 500 million new URLs each month and a 100:1 read:write ratio: System APIs \ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The following are possible API specifications for creating and removing URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? By consuming all URL keys in the existing design, a hostile person can put us out of business. We can restrict users based on their api_dev_key to prevent abuse. A specific amount of URL creations and redirections each time period can be configured for each api_dev_key (which may be set to a different duration per developer key). Database Design \ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. A few points to consider regarding the data we'll be storing: We'll need billions of records to store. Each item we keep is little (less than 1K). Except for storing which user created a URL, there are no linkages between records. Our service requires a lot of reading. Database Schema: We'd need two tables: one to store information about URL mappings, and another to store data about the user who created the short link. What kind of database should we use? A NoSQL store like DynamoDB, Cassandra, or Riak is a preferable choice because we expect to store billions of rows and don't need to employ associations between items. It would also be easy to scale a NoSQL database. Algorithms and Basic System Design We're trying to figure out how to make a short and unique key for a given URL. The abbreviated URL in the TinyURL example in Section 1 is \"http://tinyurl.com/jlg8zpc.\" The short key we want to produce is the final seven characters of this URL. Here, we'll look at two options: Encoding actual URL We can generate a unique hash of the supplied URL (e.g., MD5 or SHA256, etc.). After that, the hash can be decoded for display. This encoding might be base36 ([a-z,0-9]) or base62 ([A-Z, a-z, 0-9]), and we can use Base64 encoding by adding '+' and '/'. What should the length of the short key be, is a legitimate question. Is it better to have six, eight, or ten characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. The MD5 algorithm produces a 128-bit hash value when used as a hash function. We'll get a string with more than 21 characters after base64 encoding (since each base64 character encodes 6 bits of the hash value). How will we choose our key now that we only have space for 8 characters per short key? For the key, we can use the first six (or eight) letters. This could lead to key duplication; to avoid this, we can exchange certain characters or choose other characters from the encoding string. What are the different issues with our solution? The following are a couple of issues with our encoding scheme: If numerous users enter the same URL, the abbreviated URL will be the same, which is unacceptable. What if parts of the URL are URL-encoded? e.g., http://www.jayaaemekar.io/distributed.php?id=design, and http://www.jayaaemekar.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Alternative to the problems: To make each input URL unique, we can append an ascending sequence number to it and then construct a hash of it. However, we do not need to save this sequence number in the databases. An ever-increasing sequence number could be a concern with this method. Is it possible for it to overflow? Increasing the sequence number will have an effect on the service's performance. Another option is to include a user id to the input URL (which should be unique). If the user hasn't signed in yet, we'll have to prompt them to select a uniqueness key. If there is still a disagreement, we must keep creating keys until we find one that is unique. Generating keys offline We could create a separate Key Generation Service (KGS) that produces random six-letter strings and saves them in a database (let's call it key-DB). We'll just utilize one of the already-generated keys to abbreviate a URL whenever we need to. This method simplifies and expedites the process. We won't have to worry about duplications or collisions because the URL won't be encoded. KGS will ensure that all keys placed into key-DB are one-of-a-kind. Can concurrency lead to issues? Once a key has been used, it should be marked in the database to prevent it from being reused. If many servers are reading keys at the same time, we may see a situation where two or more servers attempt to read the same key from the database. What are our options for dealing with this concurrent issue? KGS allows servers to read and mark database keys. To store keys, KGS can employ two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys into the used keys table as soon as they are given to one of the servers. KGS can maintain some keys in memory at all times so that they can be rapidly provided to a server when it is needed. For simplicity, KGS can move keys to the used keys table as soon as they are loaded into memory. This guarantees that each server has its own set of keys. We will be squandering those keys if KGS dies before allocating all of the loaded keys to some server\u2013which may be acceptable given the large amount of keys we have. KGS must also ensure that the same key is not used by several servers. Before removing keys from the data structure and delivering them to a server, it must synchronize (or gain a lock on) the data structure holding the keys. How big should the key-DB be? We can construct 68.7 billion unique six-letter keys using base64 encoding. If each alpha-numeric character requires one byte, we can store all of these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Doesn't KGS represent a single point of failure? Yes, it is correct. We can solve this by having a backup copy of KGS. When the primary server fails, the standby server can generate and distribute keys in its place. Is it possible for each app server to cache some keys from the key-DB? Yes, this will undoubtedly expedite things. However, if the application server dies before all of the keys have been consumed, we will lose those keys. Because we have 68B distinct six-letter keys, this may be okay. How would we go about doing a key lookup? To acquire the whole URL, we may look up the key in our database. If it's in the database, send a \"HTTP 302 Redirect\" status to the browser, including the stored URL in the \"Location\" field. If the key isn't in our system, return the user to the homepage or deliver a \"HTTP 404 Not Found\" status. Should custom aliases be limited in size? Custom aliases are supported by our service. Users can choose any 'key' they like, but a custom alias is not required. However, imposing a size restriction on a custom alias is understandable (and frequently desirable) in order to maintain a consistent URL database. Assume that each customer key can have a maximum of 16 characters (as reflected in the above database schema). Data Partitioning and Replication We need to split our database such that it can hold information about billions of URLs in order to scale it out. We need to devise a partitioning strategy that will divide and store our data across multiple DB servers. Range Based Partitioning: Based on the initial letter of the hash key, we can store URLs in different partitions. As a result, we save all URLs that begin with the letter 'A' (and 'a') in one partition, those that begin with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. We can even merge a few characters that aren't used very often into a single database segment. We should devise a static partitioning scheme to ensure that we can always store and retrieve URLs in a consistent manner. The biggest issue with this method is that it can result in unbalanced database servers. For example, suppose we decide to put all URLs beginning with the letter 'E' into a database partition, only to discover later that we have far too many URLs beginning with the letter 'E.' Partitioning based on hashes: We take a hash of the object we're storing in this scheme. The hash is then used to determine which partition to use. In our situation, the hash of the 'key' or the short link can be used to determine the partition in which the data object is stored. Our hashing function will distribute URLs into different divisions at random (e.g., any 'key' can be mapped to a number between [1...256]), and this number will represent the partition in which we will put our object. This method can still result in overloaded partitions, which can be remedied by employing Consistent Hashing. Cache URLs that are often visited can be cached. We can utilize a commercially available solution like Memcached, which can store complete URLs along with their hashes. Before contacting backend storage, application servers can rapidly check if the needed URL is in the cache. Should we have a lot of cache memory? We can start with 20% of daily traffic and change the number of cache servers needed based on client usage patterns. To cache 20% of daily traffic, we'll require 170GB of memory, as previously calculated. We can easily fit all of the cache into one machine because a modern-day server can have 256GB of memory. Alternatively, we can store all of these popular URLs on a couple of smaller servers. Which cache eviction policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to change a link with a newer/hotter URL? For our system, LRU (Least Recently Used) can be a suitable policy. We start with the URL that has been used the least lately. To store our URLs and Hashes, we can use a Linked Hash Map or a similar data structure, which will also keep track of the URLs that have been accessed recently. We may replicate our cache servers to divide the load between them to boost efficiency even more. How do I refresh each cache replica? Our servers would hit a backend database whenever a cache miss occurred. We may update the cache and pass the new entry to all cache replicas once this happens. By adding the new entry, each copy can update its cache. If the entry already exists in a replica, it can be ignored. Load Balancer (LB) We may add a load balancing layer to our system in three places: Between the application servers and the clients The Relationship Between Application and Database Servers The Relationship Between Application and Cache Servers We might start with a simple Round Robin strategy, which evenly distributes incoming requests among backend servers. This LB is easy to set up and doesn't add any more overhead. Another advantage of this method is that if a server goes down, LB removes it from the rotation and stops transmitting traffic to it. We don't take the server load into account with Round Robin LB, which is a concern. The LB will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent LB solution can be implemented, which queries the backend server about its load on a regular basis and adjusts traffic accordingly. Database cleansing or purging Should entries be saved indefinitely or should they be deleted? What should happen to the link if it reaches the user-specified expiration time? It would put a lot of strain on our database if we decided to actively look for outdated links and remove them. Instead, we can execute a lazy cleanup and gently remove expired links. Only expired links will be erased by our service, while some expired links may exist longer but will never be returned to users. If a user attempts to access an expired link, we can erase the link and provide the user an error message. A separate Cleanup service can run on a regular basis to clear out expired links from our cache and storage. This service should be extremely light, and it should only be used when user traffic is predicted to be low. Each link can have a default expiration time (e.g.,two years). After removing an expired link, we may re-use the key by putting it back in the key-DB. Should links that haven't been seen in a certain amount of time, say six months, be removed? This could be challenging. Because storage is becoming more affordable, we can chose to store links indefinitely. Telemetry What were the user locations, how many times a short URL was used, etc.? How would we keep track of these figures? What happens when a popular URL is bombarded with a huge number of concurrent requests if it's part of a DB row that gets updated on each view? The visitor's nation, date and time of access, web page that relates to the click, browser, or platform from which the page was visited are all statistics worth keeping track of. Security and Permissions Can users build private URLs or restrict access to a URL to a specific group of users? In the database, we can store the permission level (public/private) for each URL. We can also construct a separate table to keep track of UserIDs with access to a given URL. We can return an error (HTTP 401) if a user does not have permission and attempts to access a URL. Given that we're using a NoSQL wide-column database like Cassandra to store our data, the 'Hash' (or the KGS produced 'key') would be the key for the table containing permissions. The UserIDs of those users who have authorization to see the URL will be stored in the columns.","title":"Designing a URL Shortening service"},{"location":"DesigningURLShorteningService/#creating-a-tinyurl-style-url-shortening-service","text":"","title":"Creating a TinyURL-style URL shortening service"},{"location":"DesigningURLShorteningService/#problem-statement","text":"Let's make a TinyURL-style URL shortening service. Short aliases for long URLs will be provided by this service. Bit.ly, goo.gl, qlink.me, and other similar services Level of Difficulty: Easy","title":"Problem Statement"},{"location":"DesigningURLShorteningService/#what-is-the-purpose-of-url-shortening","text":"For long URLs, URL shortening is utilized to create shorter aliases. These shortened aliases are referred to as \"short links.\" When users click on these short links, they are forwarded to the original URL. When displayed, printed, messaged, or tweeted, short links save a lot of space. Shorter URLs are also less likely to be mistyped by users. For example, if we use TinyURL to shorten this page: https://www.jayaaemekar.io/collection/page/5668639101419520/ We would get: http://tinyurl.com/jlg8zpc The abbreviated URL is almost one-third the length of the original. The abbreviated URL is almost one-third the length of the original. URL shortening is used for a variety of purposes, including optimizing links across devices, tracking individual links to gauge audience and campaign performance, and concealing connected original URLs.","title":"What is the purpose of URL shortening?"},{"location":"DesigningURLShorteningService/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningURLShorteningService/#solution","text":"","title":"Solution"},{"location":"DesigningURLShorteningService/#requirements-and-goals-of-the-system","text":"\ud83d\udca1 At the start of the interview, you should always outline criteria. Ask questions to figure out the exact extent of the system that the interviewer is thinking of. The following requirements should be met by our URL shortening system: Functional Requirements: Given a URL, our service should create a unique and shorter alias for it. This is referred to as a short link. This URL should be short enough to be copied and pasted into programs without difficulty. Our service should redirect visitors to the original site when they click on a short link. Users should be allowed to choose a custom short link for their URL as an option. Links will expire after a set amount of time. The expiration time should be configurable by the user. Non-Functional Requirements: The system should have a high level of availability. This is necessary because if our service is unavailable, all URL redirections would fail. URL redirection should take place in real time with the least amount of latency possible. It should not be possible to guess the length of shortened connections (not predictable). Extended Requirements: Analytical data; for example, how many times has a redirection occurred? Our service should also be accessible through REST APIs by other services.","title":"Requirements and Goals of the System"},{"location":"DesigningURLShorteningService/#capacity-estimation-and-constraints","text":"Our system will rely heavily on reading. In comparison to new URL shortenings, there will be a lot of redirection requests. Assume that read and write have a 100:1 ratio. Traffic estimates: If we assume 500 million new URL shortenings every month and a 100:1 read/write ratio, we can expect 50 billion redirections in the same time period: 100 * 500M => 50B What would our system's Queries Per Second (QPS) be? Per second, new URL shortenings: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let's say we keep track of every URL shortening request (and the abbreviated link that goes with it) for five years. With 500 million new URLs expected per month, the total number of objects we expect to store is 30 billion: 500 million * 5 years * 12 months = 30 billion Let's assume that each stored object is 500 bytes in size (this is just a guess\u2013we'll look at it later). We'll need a total of 15TB of storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: How much RAM will we need to keep some of the most often visited URLs if we wish to cache them? We'd like to cache this 20% of hot URLs if we follow the 80-20 rule, which states that 20% of URLs produce 80% of traffic. We'll get 1.7 billion requests each day if we have 20K requests per second: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to keep in mind is that because there will be a lot of duplicate requests (for the same URL), our actual memory consumption will be less than 170GB. Estimates at a high level: The following is a summary of our high-level estimations for our service, assuming 500 million new URLs each month and a 100:1 read:write ratio:","title":"Capacity Estimation and Constraints"},{"location":"DesigningURLShorteningService/#system-apis","text":"\ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The following are possible API specifications for creating and removing URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? By consuming all URL keys in the existing design, a hostile person can put us out of business. We can restrict users based on their api_dev_key to prevent abuse. A specific amount of URL creations and redirections each time period can be configured for each api_dev_key (which may be set to a different duration per developer key).","title":"System APIs"},{"location":"DesigningURLShorteningService/#database-design","text":"\ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. A few points to consider regarding the data we'll be storing: We'll need billions of records to store. Each item we keep is little (less than 1K). Except for storing which user created a URL, there are no linkages between records. Our service requires a lot of reading. Database Schema: We'd need two tables: one to store information about URL mappings, and another to store data about the user who created the short link. What kind of database should we use? A NoSQL store like DynamoDB, Cassandra, or Riak is a preferable choice because we expect to store billions of rows and don't need to employ associations between items. It would also be easy to scale a NoSQL database.","title":"Database Design"},{"location":"DesigningURLShorteningService/#algorithms-and-basic-system-design","text":"We're trying to figure out how to make a short and unique key for a given URL. The abbreviated URL in the TinyURL example in Section 1 is \"http://tinyurl.com/jlg8zpc.\" The short key we want to produce is the final seven characters of this URL. Here, we'll look at two options:","title":"Algorithms and Basic System Design"},{"location":"DesigningURLShorteningService/#encoding-actual-url","text":"We can generate a unique hash of the supplied URL (e.g., MD5 or SHA256, etc.). After that, the hash can be decoded for display. This encoding might be base36 ([a-z,0-9]) or base62 ([A-Z, a-z, 0-9]), and we can use Base64 encoding by adding '+' and '/'. What should the length of the short key be, is a legitimate question. Is it better to have six, eight, or ten characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. The MD5 algorithm produces a 128-bit hash value when used as a hash function. We'll get a string with more than 21 characters after base64 encoding (since each base64 character encodes 6 bits of the hash value). How will we choose our key now that we only have space for 8 characters per short key? For the key, we can use the first six (or eight) letters. This could lead to key duplication; to avoid this, we can exchange certain characters or choose other characters from the encoding string. What are the different issues with our solution? The following are a couple of issues with our encoding scheme: If numerous users enter the same URL, the abbreviated URL will be the same, which is unacceptable. What if parts of the URL are URL-encoded? e.g., http://www.jayaaemekar.io/distributed.php?id=design, and http://www.jayaaemekar.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Alternative to the problems: To make each input URL unique, we can append an ascending sequence number to it and then construct a hash of it. However, we do not need to save this sequence number in the databases. An ever-increasing sequence number could be a concern with this method. Is it possible for it to overflow? Increasing the sequence number will have an effect on the service's performance. Another option is to include a user id to the input URL (which should be unique). If the user hasn't signed in yet, we'll have to prompt them to select a uniqueness key. If there is still a disagreement, we must keep creating keys until we find one that is unique.","title":"Encoding actual URL"},{"location":"DesigningURLShorteningService/#generating-keys-offline","text":"We could create a separate Key Generation Service (KGS) that produces random six-letter strings and saves them in a database (let's call it key-DB). We'll just utilize one of the already-generated keys to abbreviate a URL whenever we need to. This method simplifies and expedites the process. We won't have to worry about duplications or collisions because the URL won't be encoded. KGS will ensure that all keys placed into key-DB are one-of-a-kind. Can concurrency lead to issues? Once a key has been used, it should be marked in the database to prevent it from being reused. If many servers are reading keys at the same time, we may see a situation where two or more servers attempt to read the same key from the database. What are our options for dealing with this concurrent issue? KGS allows servers to read and mark database keys. To store keys, KGS can employ two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys into the used keys table as soon as they are given to one of the servers. KGS can maintain some keys in memory at all times so that they can be rapidly provided to a server when it is needed. For simplicity, KGS can move keys to the used keys table as soon as they are loaded into memory. This guarantees that each server has its own set of keys. We will be squandering those keys if KGS dies before allocating all of the loaded keys to some server\u2013which may be acceptable given the large amount of keys we have. KGS must also ensure that the same key is not used by several servers. Before removing keys from the data structure and delivering them to a server, it must synchronize (or gain a lock on) the data structure holding the keys. How big should the key-DB be? We can construct 68.7 billion unique six-letter keys using base64 encoding. If each alpha-numeric character requires one byte, we can store all of these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Doesn't KGS represent a single point of failure? Yes, it is correct. We can solve this by having a backup copy of KGS. When the primary server fails, the standby server can generate and distribute keys in its place. Is it possible for each app server to cache some keys from the key-DB? Yes, this will undoubtedly expedite things. However, if the application server dies before all of the keys have been consumed, we will lose those keys. Because we have 68B distinct six-letter keys, this may be okay. How would we go about doing a key lookup? To acquire the whole URL, we may look up the key in our database. If it's in the database, send a \"HTTP 302 Redirect\" status to the browser, including the stored URL in the \"Location\" field. If the key isn't in our system, return the user to the homepage or deliver a \"HTTP 404 Not Found\" status. Should custom aliases be limited in size? Custom aliases are supported by our service. Users can choose any 'key' they like, but a custom alias is not required. However, imposing a size restriction on a custom alias is understandable (and frequently desirable) in order to maintain a consistent URL database. Assume that each customer key can have a maximum of 16 characters (as reflected in the above database schema).","title":"Generating keys offline"},{"location":"DesigningURLShorteningService/#data-partitioning-and-replication","text":"We need to split our database such that it can hold information about billions of URLs in order to scale it out. We need to devise a partitioning strategy that will divide and store our data across multiple DB servers. Range Based Partitioning: Based on the initial letter of the hash key, we can store URLs in different partitions. As a result, we save all URLs that begin with the letter 'A' (and 'a') in one partition, those that begin with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. We can even merge a few characters that aren't used very often into a single database segment. We should devise a static partitioning scheme to ensure that we can always store and retrieve URLs in a consistent manner. The biggest issue with this method is that it can result in unbalanced database servers. For example, suppose we decide to put all URLs beginning with the letter 'E' into a database partition, only to discover later that we have far too many URLs beginning with the letter 'E.' Partitioning based on hashes: We take a hash of the object we're storing in this scheme. The hash is then used to determine which partition to use. In our situation, the hash of the 'key' or the short link can be used to determine the partition in which the data object is stored. Our hashing function will distribute URLs into different divisions at random (e.g., any 'key' can be mapped to a number between [1...256]), and this number will represent the partition in which we will put our object. This method can still result in overloaded partitions, which can be remedied by employing Consistent Hashing.","title":"Data Partitioning and Replication"},{"location":"DesigningURLShorteningService/#cache","text":"URLs that are often visited can be cached. We can utilize a commercially available solution like Memcached, which can store complete URLs along with their hashes. Before contacting backend storage, application servers can rapidly check if the needed URL is in the cache. Should we have a lot of cache memory? We can start with 20% of daily traffic and change the number of cache servers needed based on client usage patterns. To cache 20% of daily traffic, we'll require 170GB of memory, as previously calculated. We can easily fit all of the cache into one machine because a modern-day server can have 256GB of memory. Alternatively, we can store all of these popular URLs on a couple of smaller servers. Which cache eviction policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to change a link with a newer/hotter URL? For our system, LRU (Least Recently Used) can be a suitable policy. We start with the URL that has been used the least lately. To store our URLs and Hashes, we can use a Linked Hash Map or a similar data structure, which will also keep track of the URLs that have been accessed recently. We may replicate our cache servers to divide the load between them to boost efficiency even more. How do I refresh each cache replica? Our servers would hit a backend database whenever a cache miss occurred. We may update the cache and pass the new entry to all cache replicas once this happens. By adding the new entry, each copy can update its cache. If the entry already exists in a replica, it can be ignored.","title":"Cache"},{"location":"DesigningURLShorteningService/#load-balancer-lb","text":"We may add a load balancing layer to our system in three places: Between the application servers and the clients The Relationship Between Application and Database Servers The Relationship Between Application and Cache Servers We might start with a simple Round Robin strategy, which evenly distributes incoming requests among backend servers. This LB is easy to set up and doesn't add any more overhead. Another advantage of this method is that if a server goes down, LB removes it from the rotation and stops transmitting traffic to it. We don't take the server load into account with Round Robin LB, which is a concern. The LB will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent LB solution can be implemented, which queries the backend server about its load on a regular basis and adjusts traffic accordingly.","title":"Load Balancer (LB)"},{"location":"DesigningURLShorteningService/#database-cleansing-or-purging","text":"Should entries be saved indefinitely or should they be deleted? What should happen to the link if it reaches the user-specified expiration time? It would put a lot of strain on our database if we decided to actively look for outdated links and remove them. Instead, we can execute a lazy cleanup and gently remove expired links. Only expired links will be erased by our service, while some expired links may exist longer but will never be returned to users. If a user attempts to access an expired link, we can erase the link and provide the user an error message. A separate Cleanup service can run on a regular basis to clear out expired links from our cache and storage. This service should be extremely light, and it should only be used when user traffic is predicted to be low. Each link can have a default expiration time (e.g.,two years). After removing an expired link, we may re-use the key by putting it back in the key-DB. Should links that haven't been seen in a certain amount of time, say six months, be removed? This could be challenging. Because storage is becoming more affordable, we can chose to store links indefinitely.","title":"Database cleansing or purging"},{"location":"DesigningURLShorteningService/#telemetry","text":"What were the user locations, how many times a short URL was used, etc.? How would we keep track of these figures? What happens when a popular URL is bombarded with a huge number of concurrent requests if it's part of a DB row that gets updated on each view? The visitor's nation, date and time of access, web page that relates to the click, browser, or platform from which the page was visited are all statistics worth keeping track of.","title":"Telemetry"},{"location":"DesigningURLShorteningService/#security-and-permissions","text":"Can users build private URLs or restrict access to a URL to a specific group of users? In the database, we can store the permission level (public/private) for each URL. We can also construct a separate table to keep track of UserIDs with access to a given URL. We can return an error (HTTP 401) if a user does not have permission and attempts to access a URL. Given that we're using a NoSQL wide-column database like Cassandra to store our data, the 'Hash' (or the KGS produced 'key') would be the key for the table containing permissions. The UserIDs of those users who have authorization to see the URL will be stored in the columns.","title":"Security and Permissions"},{"location":"DesigningUberBackend/","text":"Designing Uber backend Problem Statement Let's design a ride-sharing service like Uber, which connects passengers who need a ride with drivers who have a car. Similar Services: Lyft, Didi, Via, Sidecar, etc. Difficulty level: Hard Prerequisite: Designing Yelp What is Uber? Customers may book drivers for taxi rides using Uber. Uber drivers drive consumers around in their own automobiles. The Uber app allows consumers and drivers to communicate with one another via their smartphones. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Let\u2019s start with building a simpler version of Uber. In our system, there are two sorts of users: 1) Motorists 2) Clientele. Drivers must inform the service of their current position and availability to pick up customers on a frequent basis. Passengers can see all available drivers in their immediate vicinity. Customers can request a ride, and neighboring drivers will be notified. Once a consumer accepts a ride, the driver and customer can see each other's current location until the trip is completed. When the driver arrives at his destination, he marks the ride as completed and becomes ready for the next ride. Capacity Estimation and Constraints Assume we have 300 million consumers and one million drivers, with one million daily active customers and 500 thousand daily active drivers. Assume 1 million daily rides. Assume that every three seconds, all active drivers broadcast their present location. The system should be able to contact drivers in real time if a customer requests a ride. Basic System Design and Algorithm We'll alter the solution outlined in Designing Yelp to make it work for the \"Uber\" use cases mentioned above. The most significant difference we have is that our QuadTree was not designed with frequent updates in mind. So, with our Dynamic Grid solution, we have two issues: We need to change our data structures to reflect the fact that all active drivers are reporting their whereabouts every three seconds. It will take a lot of time and resources to update the QuadTree for every change in the driver's location. We must locate the correct grid based on the driver's previous location to update a driver to its new location. We must delete the driver from the current grid and move/reinsert the user to the right grid if the new position does not correspond to the current grid. If the new grid exceeds the maximum number of drivers after this relocation, we must repartition it. We need a fast way to communicate the current location of all surrounding drivers to every active customer in the vicinity. Also, when a ride is in progress, our system must inform both the driver and the passenger of the car's present location. Although our QuadTree assists us in swiftly locating nearby drivers, a speedy update in the tree is not guaranteed. Do we need to modify our QuadTree every time a driver reports their location? QuadTree will have some old data with each driver update and may not accurately reflect the current position of drivers. If you recall, the QuadTree was designed to quickly locate nearby drivers (or locations). There will be a lot more updates to our tree than asking for nearby drivers because all current drivers submit their location every three seconds. So, what if we save all drivers' most recent positions in a hash table and update our QuadTree less frequently? Assume that the current location of a driver will be reflected in the QuadTree within 15 seconds. Meanwhile, we'll keep a hash table to keep track of the present location drivers have reported; This will be known as DriverLocationHT. How much memory/RAM does DriverLocationHT require? In the hash table, we must store DriveID, as well as their current and previous locations. To store one record, we'll need a total of 35 bytes: DriverID (3 bytes - 1 million drivers) Old latitude (8 bytes) Old longitude (8 bytes) New latitude (8 bytes) New longitude (8 bytes) Total = 35 bytes If we have 1 million total drivers, we need the following memory (ignoring hash table overhead): 1 million * 35 bytes => 35 MB How much bandwidth will our service consume to receive location updates from all drivers? If we get It will be (3+16 => 19 bytes) for DriverID and their location. We will receive 9.5MB per three seconds if we receive this information every three seconds from 500K daily active drivers. Do we need to distribute DriverLocationHT onto multiple servers? Although our memory and bandwidth requirements don't necessitate it because all of this data can be stored on a single server, we should divide DriverLocationHT among numerous servers for scalability, performance, and fault tolerance. To make the distribution entirely random, we can distribute depending on the DriverID. The machines that hold DriverLocationHT are referred to as the Driver Location server. Each of these servers will do two things in addition to storing the driver's location: As soon as the server receives an update on a driver's position, it will broadcast it to all clients who are interested. The server must notify the appropriate QuadTree server in order to update the driver's location. As previously stated, this can occur every 10 seconds. How can we efficiently broadcast the driver\u2019s location to customers? We can utilize a Push Model, in which the server sends the positions to all users who are interested. We could create a dedicated Notification Service that broadcasts the current location of drivers to all clients that are interested. We can create a publisher/subscriber model for our Notification service. When a customer launches the Uber app on their smartphone, the server searches for nearby drivers. Before providing the list of drivers to the client, we shall subscribe the customer to all updates from those drivers on the server side. We can keep a list of clients (subscribers) who want to know where a driver is and, if there is an update in DriverLocationHT for that driver, we can broadcast the driver's current location to all subscribers. Our system ensures that the customer is always aware of the driver's current location. How much memory will we need to store all these subscriptions? As previously said, we expect 1 million daily active consumers and 500 thousand daily active drivers. Assume that five consumers subscribe to one driver on average. Let's pretend we keep all of this data in a hash table so we can update it quickly. To keep the subscriptions running, we need to keep track of the driver and customer IDs. We'll require 21MB of memory if we need 3 bytes for DriverID and 8 bytes for CustomerID. (500K * 3) + (500K * 5 * 8 ) ~= 21 MB How much bandwidth will we need to broadcast the driver\u2019s location to customers? For every active driver, we have five subscribers, so the total subscribers we have: 5 * 500K => 2.5M To all these customers we need to send DriverID (3 bytes) and their location (16 bytes) every second, so, we need the following bandwidth: 2.5M * 19 bytes => 47.5 MB/s How can we efficiently implement Notification service? We can either use HTTP long polling or push notifications. How will the new publishers/drivers get added for a current customer? Customers will be subscribed to nearby drivers when they open the Uber app for the first time, as we proposed above; however, what will happen if a new driver enters the area the client is looking at? We need to keep track of the region the client is watching in order to dynamically add a new customer/driver subscription. This will complicate our solution; how about clients pulling this information from the server instead of pushing it? How about if clients pull information about nearby drivers from the server? Clients can send their current position to the server, which will locate any nearby QuadTree drivers and return them to the client. When the client receives this information, they can change their screen to reflect the drivers' current positions. To reduce the amount of round trips to the server, clients might query every five seconds. In comparison to the push paradigm discussed above, this solution appears to be easier. Do we need to repartition a grid as soon as it reaches the maximum limit? We can have a buffer to allow each grid to grow slightly beyond the limit before partitioning it. Let's say our grids can expand or contract by 10% more before we separate or merge them. On high-traffic grids, this should reduce the demand for a grid split or merge. How would \u201cRequest A Ride\u201d use case work? The customer will make a ride request. One of the Aggregator servers will accept the request and send it to the QuadTree servers, who will then return nearby drivers. The Aggregator server gathers all of the results and organizes them according to their ratings. The Aggregator server will send a notification to the top (say three) drivers at the same time, and the ride will be assigned to the driver who accepts the request first. A cancellation request will be sent to the other drivers. The Aggregator will request a ride from the next three drivers on the list if none of the three drivers answer. The customer is notified when a driver accepts a request. Fault Tolerance and Replication What happens if a Driver Location or Notification server goes down? We'd need duplicates of these servers so that if the primary goes down, the backup can take over. We can also store this data in some persistent storage, such as SSDs with quick IOs, so that we can recover the data from the persistent storage if both the primary and secondary servers fail. Ranking How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we find the best drivers within a certain radius? Assume we maintain track of each driver's overall ratings in our database and QuadTree. In our system, an aggregated number can represent this popularity, such as how many stars a driver receives out of 10. We can ask each partition of the QuadTree to return the top 10 drivers with the highest rating while looking for the top 10 drivers within a defined radius. The aggregator server can then select the top ten drivers from all of the drivers returned by the various partitions. Advanced Issues How will we deal with clients on sluggish or unstable networks? What happens if a client is disconnected during a ride? In this case, how will we handle billing? What if clients pull all of the data rather than servers always pushing it?","title":"Designing Uber backend"},{"location":"DesigningUberBackend/#designing-uber-backend","text":"","title":"Designing Uber backend"},{"location":"DesigningUberBackend/#problem-statement","text":"Let's design a ride-sharing service like Uber, which connects passengers who need a ride with drivers who have a car. Similar Services: Lyft, Didi, Via, Sidecar, etc. Difficulty level: Hard Prerequisite: Designing Yelp","title":"Problem Statement"},{"location":"DesigningUberBackend/#what-is-uber","text":"Customers may book drivers for taxi rides using Uber. Uber drivers drive consumers around in their own automobiles. The Uber app allows consumers and drivers to communicate with one another via their smartphones.","title":"What is Uber?"},{"location":"DesigningUberBackend/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningUberBackend/#solution","text":"","title":"Solution"},{"location":"DesigningUberBackend/#requirements-and-goals-of-the-system","text":"Let\u2019s start with building a simpler version of Uber. In our system, there are two sorts of users: 1) Motorists 2) Clientele. Drivers must inform the service of their current position and availability to pick up customers on a frequent basis. Passengers can see all available drivers in their immediate vicinity. Customers can request a ride, and neighboring drivers will be notified. Once a consumer accepts a ride, the driver and customer can see each other's current location until the trip is completed. When the driver arrives at his destination, he marks the ride as completed and becomes ready for the next ride.","title":"Requirements and Goals of the System"},{"location":"DesigningUberBackend/#capacity-estimation-and-constraints","text":"Assume we have 300 million consumers and one million drivers, with one million daily active customers and 500 thousand daily active drivers. Assume 1 million daily rides. Assume that every three seconds, all active drivers broadcast their present location. The system should be able to contact drivers in real time if a customer requests a ride.","title":"Capacity Estimation and Constraints"},{"location":"DesigningUberBackend/#basic-system-design-and-algorithm","text":"We'll alter the solution outlined in Designing Yelp to make it work for the \"Uber\" use cases mentioned above. The most significant difference we have is that our QuadTree was not designed with frequent updates in mind. So, with our Dynamic Grid solution, we have two issues: We need to change our data structures to reflect the fact that all active drivers are reporting their whereabouts every three seconds. It will take a lot of time and resources to update the QuadTree for every change in the driver's location. We must locate the correct grid based on the driver's previous location to update a driver to its new location. We must delete the driver from the current grid and move/reinsert the user to the right grid if the new position does not correspond to the current grid. If the new grid exceeds the maximum number of drivers after this relocation, we must repartition it. We need a fast way to communicate the current location of all surrounding drivers to every active customer in the vicinity. Also, when a ride is in progress, our system must inform both the driver and the passenger of the car's present location. Although our QuadTree assists us in swiftly locating nearby drivers, a speedy update in the tree is not guaranteed. Do we need to modify our QuadTree every time a driver reports their location? QuadTree will have some old data with each driver update and may not accurately reflect the current position of drivers. If you recall, the QuadTree was designed to quickly locate nearby drivers (or locations). There will be a lot more updates to our tree than asking for nearby drivers because all current drivers submit their location every three seconds. So, what if we save all drivers' most recent positions in a hash table and update our QuadTree less frequently? Assume that the current location of a driver will be reflected in the QuadTree within 15 seconds. Meanwhile, we'll keep a hash table to keep track of the present location drivers have reported; This will be known as DriverLocationHT. How much memory/RAM does DriverLocationHT require? In the hash table, we must store DriveID, as well as their current and previous locations. To store one record, we'll need a total of 35 bytes: DriverID (3 bytes - 1 million drivers) Old latitude (8 bytes) Old longitude (8 bytes) New latitude (8 bytes) New longitude (8 bytes) Total = 35 bytes If we have 1 million total drivers, we need the following memory (ignoring hash table overhead): 1 million * 35 bytes => 35 MB How much bandwidth will our service consume to receive location updates from all drivers? If we get It will be (3+16 => 19 bytes) for DriverID and their location. We will receive 9.5MB per three seconds if we receive this information every three seconds from 500K daily active drivers. Do we need to distribute DriverLocationHT onto multiple servers? Although our memory and bandwidth requirements don't necessitate it because all of this data can be stored on a single server, we should divide DriverLocationHT among numerous servers for scalability, performance, and fault tolerance. To make the distribution entirely random, we can distribute depending on the DriverID. The machines that hold DriverLocationHT are referred to as the Driver Location server. Each of these servers will do two things in addition to storing the driver's location: As soon as the server receives an update on a driver's position, it will broadcast it to all clients who are interested. The server must notify the appropriate QuadTree server in order to update the driver's location. As previously stated, this can occur every 10 seconds. How can we efficiently broadcast the driver\u2019s location to customers? We can utilize a Push Model, in which the server sends the positions to all users who are interested. We could create a dedicated Notification Service that broadcasts the current location of drivers to all clients that are interested. We can create a publisher/subscriber model for our Notification service. When a customer launches the Uber app on their smartphone, the server searches for nearby drivers. Before providing the list of drivers to the client, we shall subscribe the customer to all updates from those drivers on the server side. We can keep a list of clients (subscribers) who want to know where a driver is and, if there is an update in DriverLocationHT for that driver, we can broadcast the driver's current location to all subscribers. Our system ensures that the customer is always aware of the driver's current location. How much memory will we need to store all these subscriptions? As previously said, we expect 1 million daily active consumers and 500 thousand daily active drivers. Assume that five consumers subscribe to one driver on average. Let's pretend we keep all of this data in a hash table so we can update it quickly. To keep the subscriptions running, we need to keep track of the driver and customer IDs. We'll require 21MB of memory if we need 3 bytes for DriverID and 8 bytes for CustomerID. (500K * 3) + (500K * 5 * 8 ) ~= 21 MB How much bandwidth will we need to broadcast the driver\u2019s location to customers? For every active driver, we have five subscribers, so the total subscribers we have: 5 * 500K => 2.5M To all these customers we need to send DriverID (3 bytes) and their location (16 bytes) every second, so, we need the following bandwidth: 2.5M * 19 bytes => 47.5 MB/s How can we efficiently implement Notification service? We can either use HTTP long polling or push notifications. How will the new publishers/drivers get added for a current customer? Customers will be subscribed to nearby drivers when they open the Uber app for the first time, as we proposed above; however, what will happen if a new driver enters the area the client is looking at? We need to keep track of the region the client is watching in order to dynamically add a new customer/driver subscription. This will complicate our solution; how about clients pulling this information from the server instead of pushing it? How about if clients pull information about nearby drivers from the server? Clients can send their current position to the server, which will locate any nearby QuadTree drivers and return them to the client. When the client receives this information, they can change their screen to reflect the drivers' current positions. To reduce the amount of round trips to the server, clients might query every five seconds. In comparison to the push paradigm discussed above, this solution appears to be easier. Do we need to repartition a grid as soon as it reaches the maximum limit? We can have a buffer to allow each grid to grow slightly beyond the limit before partitioning it. Let's say our grids can expand or contract by 10% more before we separate or merge them. On high-traffic grids, this should reduce the demand for a grid split or merge. How would \u201cRequest A Ride\u201d use case work? The customer will make a ride request. One of the Aggregator servers will accept the request and send it to the QuadTree servers, who will then return nearby drivers. The Aggregator server gathers all of the results and organizes them according to their ratings. The Aggregator server will send a notification to the top (say three) drivers at the same time, and the ride will be assigned to the driver who accepts the request first. A cancellation request will be sent to the other drivers. The Aggregator will request a ride from the next three drivers on the list if none of the three drivers answer. The customer is notified when a driver accepts a request.","title":"Basic System Design and Algorithm"},{"location":"DesigningUberBackend/#fault-tolerance-and-replication","text":"What happens if a Driver Location or Notification server goes down? We'd need duplicates of these servers so that if the primary goes down, the backup can take over. We can also store this data in some persistent storage, such as SSDs with quick IOs, so that we can recover the data from the persistent storage if both the primary and secondary servers fail.","title":"Fault Tolerance and Replication"},{"location":"DesigningUberBackend/#ranking","text":"How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we find the best drivers within a certain radius? Assume we maintain track of each driver's overall ratings in our database and QuadTree. In our system, an aggregated number can represent this popularity, such as how many stars a driver receives out of 10. We can ask each partition of the QuadTree to return the top 10 drivers with the highest rating while looking for the top 10 drivers within a defined radius. The aggregator server can then select the top ten drivers from all of the drivers returned by the various partitions.","title":"Ranking"},{"location":"DesigningUberBackend/#advanced-issues","text":"How will we deal with clients on sluggish or unstable networks? What happens if a client is disconnected during a ride? In this case, how will we handle billing? What if clients pull all of the data rather than servers always pushing it?","title":"Advanced Issues"},{"location":"DesigningWebCrawler/","text":"Designing a Web Crawler Problem Statement Let's design a Web Crawler that will systematically browse and download the World Wide Web. Web crawlers are also known as web spiders, robots, worms, walkers, and bots. Difficulty Level: Hard What is a Web Crawler? A web crawler is a computer program that crawls the Internet in a systematic and automated manner. It gathers documents by fetching links from a set of starting pages in a recursive manner. Web crawling is used by many sites, particularly search engines, to provide up-to-date information. To do speedier searches, search engines download all of the pages and index them. Other applications for web crawlers include: Validate the syntax and structure of online pages and links. To keep an eye on websites to check if the structure or content changes. Keeping major Web sites' mirror pages up to date. To look for copyright violations. Create a special-purpose index, such as one that understands the material stored in Web-based multimedia files. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System Let\u2019s assume we need to crawl all pages across the web. Scalability: Our service should be scalable enough to crawl the entire Internet and retrieve hundreds of millions of pages. Extensibility: Our service should be built in a modular fashion, with the anticipation of future capabilities being added. In the future, there may be newer document kinds that need to be downloaded and processed. Some Design Considerations Crawling the web is a complex task, and there are many ways to go about it. We should be asking a few questions before going any further: Is it a crawler for HTML pages only? Or should we fetch and store other types of media, such as sound files, images, videos, etc.? This is significant because the response has the potential to alter the design. If we're developing a general-purpose crawler that downloads a variety of media kinds, we might wish to divide the parsing module into several sets of modules: one for HTML, another for photos, and yet another for videos, with each module extracting what's useful for that media type. Let's suppose our crawler will only deal with HTML for the time being, but it should be extendable and easy to add support for additional media types. What protocols are we looking at? HTTP? What about FTP links? What different protocols should our crawler handle? We'll assume HTTP for the sake of this exercise. Again, extending the idea to include FTP and other protocols later should not be difficult. What is the expected number of pages we will crawl? How big will the URL database become? Assume we have one billion websites to crawl. Because a website can have a lot of URLs, we'll set an upper limit of 15 billion unique web pages that our crawler can access. What is \u2018RobotsExclusion\u2019 and how should we deal with it? The Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers, is implemented by courteous Web crawlers. Before downloading any real material from a website, the Robots Exclusion Protocol needs a Web crawler to fetch a special document called robot.txt that contains these declarations. Capacity Estimation and Constraints If we want to crawl 15 billion pages within four weeks, how many pages do we need to fetch per second? 15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec What about storage? Page sizes vary greatly, but as we will only be working with HTML content, we will assume an average page size of 100KB. If each page contains 500 bytes of metadata, the total storage required is: 15B * (100KB + 500) ~= 1.5 petabytes Assuming a 70% capacity model (we don\u2019t want to go above 70% of the total capacity of our storage system), total storage we will need: 1.5 petabytes / 0.7 ~= 2.14 petabytes High Level design Any Web crawler's core method is to accept a list of seed URLs as input and perform the following stages repeatedly. Choose a URL from the list of unvisited URLs. Determine the host-IP name's address. Connect to the host in order to download the related document. Search the contents of the page for new URLs. Add the new URLs to the list of URLs that have not been visited. Store or index the contents of the downloaded document, for example. Return to step 1 How to crawl? Breadth first or depth first? BFS (breadth-first search) is commonly utilized. Depth First Search (DFS) is also used in some scenarios, such as when your crawler has already established a connection with a website and wants to save time by DFSing all of the URLs on that page. Path-ascending crawling: Path-ascending crawling can aid in the discovery of a large number of isolated resources or resources for which no inbound link would have been discovered in a conventional crawl of a Web site. A crawler would rise to every path in each URL it intended to crawl in this scheme. When given the seed URL http://foo.com/a/b/page.html, for example, it will try to crawl /a/b/, /a/, and /. Difficulties in implementing efficient web crawler There are two important characteristics of the Web that makes Web crawling a very difficult task: 1. Large volume of Web pages: Because a web crawler can only download a percentage of the online pages at any given moment due to the vast number of web pages, it is vital that the web crawler be intelligent enough to prioritize download. 2. Rate of change on web pages. Another issue in today's dynamic world is that online web pages are always changing. As a result, by the time the crawler downloads a site's last page, the page may have changed or a new page may have been added. These are the bare minimal requirements for a crawler: URL frontier: To keep track of the URLs to download and prioritize which ones should be crawled first. HTTP Fetcher: This program retrieves a web page from a server. Extractor: This program extracts hyperlinks from HTML texts. Duplicate Eliminator: To ensure that the identical content is not accidentally extracted twice. Datastore: This is where you'll keep retrieved pages, URLs, and other metadata. Detailed Component Design Assume that our crawler is operating on a single server and that all of the crawling is handled by many worker threads, each of which performs all of the processes required to download and process a document in a loop. The initial step in this loop is to remove an absolute URL from the download shared URL frontier. A scheme (for example, \"HTTP\") indicates the network protocol that should be used to download an absolute URL. We can build these protocols in a modular fashion for flexibility, so that our crawler can easily support additional protocols in the future. The worker contacts the relevant protocol module to download the document based on the URL scheme. The document is downloaded and stored in a Document Input Stream (DIS). By storing documents in DIS, other modules will be able to read them many times. The worker thread runs the dedupe test after writing the document to the DIS to see if this document (associated with a different URL) has been seen before. If this is the case, the document is not further processed, and the worker thread deletes the next URL from the frontier. The downloaded document must then be processed by our crawler. Each document can have its own MIME type, such as HTML page, Image, Video, and so on. We can build these MIME schemes in a modular fashion so that if our crawler needs to handle other kinds in the future, we can easily add them. The worker calls the process method of each processing module associated with the MIME type of the downloaded document based on that MIME type. In addition, our HTML processing module will extract all of the page's links. To evaluate if a link should be downloaded, it is transformed to an absolute URL and tested against a user-supplied URL filter. If the URL passes the filter, the worker does the URL-seen test, which determines whether the URL has been seen before, that is, whether it is in the URL frontier or has been downloaded. The URL is added to the border if it is new. Let\u2019s discuss these components in detail, and see how they can be fit onto multiple machines: 1. The URL frontier: The data structure that stores all of the URLs that need to be downloaded is known as the URL frontier. We can crawl by traversing the Web in order of breadth, beginning with the pages in the seed set. Using a FIFO queue, such traversals are simple to implement. We can distribute our URL frontier across multiple servers because we'll have a large list of URLs to crawl. Assume there are several worker threads conducting the crawling operations on each server. Let's also assume that our hash function associates each URL with a server that will crawl it. When constructing a distributed URL border, keep the following politeness requirements in mind: Our crawler should avoid overburdening a server by downloading a large number of pages. A web server should not be connected to numerous machines. On each server, our crawler can have a set of distinct FIFO sub-queues to implement this politeness restriction. Each worker thread will have its own sub-queue from which URLs for crawling will be removed. When a new URL needs to be added, the URL's canonical hostname determines which FIFO sub-queue it goes into. Each hostname can be mapped to a thread number using our hash function. These two factors together suggest that just one worker thread will download content from a given Web server, and that the Web server will not be overloaded because of the FIFO queue. What will the size of our URL border be? The number of URLs would be in the hundreds of millions. As a result, we'll need to save our URLs to disk. We can design our queues so that enqueuing and dequeuing buffers are separate. The enqueue buffer will be flushed to disk once it is full, whereas the dequeue buffer will preserve a cache of URLs that need to be visited and can read from disk to fill the buffer frequently. 2. The fetcher module: A fetcher module's job is to download the document associated with a specified URL using the proper network protocol, such as HTTP. As previously stated, webmasters use robot.txt to block the crawler from accessing certain areas of their websites. Our crawler's HTTP protocol module can retain a fixed-sized cache mapping host-names to their robot's exclusion criteria to avoid downloading this file on every request. 3. Document input stream: The design of our crawler allows many processing modules to process the same document. To avoid downloading a document several times, we use a Document Input Stream abstraction to cache the document locally (DIS). A DIS is a type of input stream that stores the whole content of a document downloaded over the internet. It also has options for rereading the document. Small documents (64 KB or less) can be cached totally in memory by the DIS, whereas bigger documents can be temporarily written to a backup file. Each worker thread has its own DIS, which it reuses from one document to the next. The worker delivers the URL extracted from the border to the appropriate protocol module, which initializes the DIS from a network connection to include the document's contents. The DIS is then passed to all appropriate processing modules by the worker. 4. Document Dedupe test: Many documents on the Internet are accessible via many URLs. There are also many instances where documents are replicated over many servers. Any Web crawler will download the same document several times as a result of both of these effects. We run a dedupe test on each document to eliminate duplication and prevent it from being processed twice. To execute this test, we can generate a 64-bit checksum for each processed document and store it in a database. Each new document's checksum can be compared to all previously calculated checksums to see if it's been seen before. MD5 or SHA can be used to calculate these checksums. What size would the checksum storage be? If the entire point of our checksum store is to perform dedupe, we only need to hold a single set of checksums for all previously processed documents. With 15 billion unique web pages, we would require: 15B * 8 bytes => 120 GB Although this can fit within the memory of a modern server, if we don't have enough, we can keep a smaller LRU-based cache on each server, with everything backed up by persistent storage. The checksum must be present in the cache for the dedupe test to succeed. If not, it must determine whether the checksum is stored in the back storage. If the checksum is found, the document will be ignored. It will be added to the cache and back storage if not. 5. URL filters: The URL filtering technique allows you to customize the list of URLs that are downloaded. This is used to create a blacklist of websites that our crawler will ignore. The worker thread evaluates the user-supplied URL filter before adding each URL to the frontier. Filters can be used to limit URLs by domain, prefix, or protocol type. 6. Domain name resolution: A Web crawler must use the Domain Name Service (DNS) to map the Web server's hostname into an IP address before accessing it. Given the number of URLs we'll be working with, DNS name resolution will be a major bottleneck for our crawlers. By constructing a local DNS server, we may begin caching DNS results to avoid repeated calls. 7. URL dedupe test: Any Web crawler will come across several links to the same document while extracting links. To prevent downloading and processing a document numerous times, each extracted link must pass a URL dedupe test before being included to the URL frontier. We can store all of the URLs observed by our crawler in canonical form in a database to execute the URL dedupe test. We save a fixed-sized checksum instead of the textual representation of each URL in the URL collection to conserve space. We can keep an in-memory cache of popular URLs on each host shared by all threads to reduce the amount of operations on the database store. The purpose for this cache is that some URLs include a lot of links, therefore caching the most popular ones in memory will result in a high hit rate. How much space do we need to store URLs? If the entire point of our checksum is to perform URL deduplication, we only need to retain a single set of checksums for all previously viewed URLs. With 15 billion unique URLs and a checksum of four bytes, we'd need: 15B * 4 bytes => 60 GB Can we use bloom filters for deduping? Bloom filters are a probabilistic data structure that can produce false positives when used for set membership checking. The set is represented by a big bit vector. By computing the element's 'n' hash functions and setting the relevant bits, an element is added to the set. If the bits at all 'n' of an element's hash locations are set, the element is considered to be in the set. As a result, a document may be mistakenly identified as part of the collection, but false negatives are not feasible. The drawback of applying a bloom filter for the URL seen test is that each false positive causes the URL to be removed from the frontier, preventing the document from being downloaded. Also, with the help of larger bit vector chance of a false positive can be reduced. 8. Checkpointing: It takes weeks to crawl the entire Internet. Our crawler can write regular snapshots of its state to the disk to protect against failures. A crawl that has been paused or aborted can easily be resumed from the most recent checkpoint. Fault tolerance For distribution among crawling servers, we should utilize consistent hashing. Consistent hashing will aid in the replacement of a dead host as well as the distribution of load among crawling servers. All of our crawling servers will checkpoint and save their FIFO queues to disks on a regular basis. We can replace a server if it goes down. Meanwhile, reliable hashing should distribute the burden to other servers. Data Partitioning Our crawler will deal with three different types of data: 1) Visitor URLs 2) Dedupe URL Checksums 3) Dedupe checksums should be documented. We can store these data on the same host because we are distributing URLs based on hostnames. As a result, each host will keep track of the URLs that must be visited, as well as the checksums of all previously visited URLs and the checksums of all downloaded contents. We may presume that URLs will be redistributed from overloaded hosts because we'll be utilizing consistent hashing. Periodically, each host will execute checkpointing and dump a snapshot of all the data it has on a distant server. This ensures that if a server fails, another server can take its data from the previous snapshot and replace it. Crawler Traps Crawler traps, spam sites, and disguised material abound. Crawler traps are URLs or groups of URLs that cause a crawler to crawl forever. Unintentional crawler traps exist. A cycle can be created via a symbolic link within a file system, for example. Other crawler traps are placed on purpose. People have created traps that dynamically generate an infinite Web of documents, for example. The reasons for such traps differ. Anti-spam traps are designed to catch spammers hunting for email addresses, while search engine crawlers get caught by traps on other sites to improve their search rankings.","title":"Designing a Web Crawler"},{"location":"DesigningWebCrawler/#designing-a-web-crawler","text":"","title":"Designing a Web Crawler"},{"location":"DesigningWebCrawler/#problem-statement","text":"Let's design a Web Crawler that will systematically browse and download the World Wide Web. Web crawlers are also known as web spiders, robots, worms, walkers, and bots. Difficulty Level: Hard","title":"Problem Statement"},{"location":"DesigningWebCrawler/#what-is-a-web-crawler","text":"A web crawler is a computer program that crawls the Internet in a systematic and automated manner. It gathers documents by fetching links from a set of starting pages in a recursive manner. Web crawling is used by many sites, particularly search engines, to provide up-to-date information. To do speedier searches, search engines download all of the pages and index them. Other applications for web crawlers include: Validate the syntax and structure of online pages and links. To keep an eye on websites to check if the structure or content changes. Keeping major Web sites' mirror pages up to date. To look for copyright violations. Create a special-purpose index, such as one that understands the material stored in Web-based multimedia files.","title":"What is a Web Crawler?"},{"location":"DesigningWebCrawler/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningWebCrawler/#solution","text":"","title":"Solution"},{"location":"DesigningWebCrawler/#requirements-and-goals-of-the-system","text":"Let\u2019s assume we need to crawl all pages across the web. Scalability: Our service should be scalable enough to crawl the entire Internet and retrieve hundreds of millions of pages. Extensibility: Our service should be built in a modular fashion, with the anticipation of future capabilities being added. In the future, there may be newer document kinds that need to be downloaded and processed.","title":"Requirements and Goals of the System"},{"location":"DesigningWebCrawler/#some-design-considerations","text":"Crawling the web is a complex task, and there are many ways to go about it. We should be asking a few questions before going any further: Is it a crawler for HTML pages only? Or should we fetch and store other types of media, such as sound files, images, videos, etc.? This is significant because the response has the potential to alter the design. If we're developing a general-purpose crawler that downloads a variety of media kinds, we might wish to divide the parsing module into several sets of modules: one for HTML, another for photos, and yet another for videos, with each module extracting what's useful for that media type. Let's suppose our crawler will only deal with HTML for the time being, but it should be extendable and easy to add support for additional media types. What protocols are we looking at? HTTP? What about FTP links? What different protocols should our crawler handle? We'll assume HTTP for the sake of this exercise. Again, extending the idea to include FTP and other protocols later should not be difficult. What is the expected number of pages we will crawl? How big will the URL database become? Assume we have one billion websites to crawl. Because a website can have a lot of URLs, we'll set an upper limit of 15 billion unique web pages that our crawler can access. What is \u2018RobotsExclusion\u2019 and how should we deal with it? The Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers, is implemented by courteous Web crawlers. Before downloading any real material from a website, the Robots Exclusion Protocol needs a Web crawler to fetch a special document called robot.txt that contains these declarations.","title":"Some Design Considerations"},{"location":"DesigningWebCrawler/#capacity-estimation-and-constraints","text":"If we want to crawl 15 billion pages within four weeks, how many pages do we need to fetch per second? 15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec What about storage? Page sizes vary greatly, but as we will only be working with HTML content, we will assume an average page size of 100KB. If each page contains 500 bytes of metadata, the total storage required is: 15B * (100KB + 500) ~= 1.5 petabytes Assuming a 70% capacity model (we don\u2019t want to go above 70% of the total capacity of our storage system), total storage we will need: 1.5 petabytes / 0.7 ~= 2.14 petabytes","title":"Capacity Estimation and Constraints"},{"location":"DesigningWebCrawler/#high-level-design","text":"Any Web crawler's core method is to accept a list of seed URLs as input and perform the following stages repeatedly. Choose a URL from the list of unvisited URLs. Determine the host-IP name's address. Connect to the host in order to download the related document. Search the contents of the page for new URLs. Add the new URLs to the list of URLs that have not been visited. Store or index the contents of the downloaded document, for example. Return to step 1 How to crawl? Breadth first or depth first? BFS (breadth-first search) is commonly utilized. Depth First Search (DFS) is also used in some scenarios, such as when your crawler has already established a connection with a website and wants to save time by DFSing all of the URLs on that page. Path-ascending crawling: Path-ascending crawling can aid in the discovery of a large number of isolated resources or resources for which no inbound link would have been discovered in a conventional crawl of a Web site. A crawler would rise to every path in each URL it intended to crawl in this scheme. When given the seed URL http://foo.com/a/b/page.html, for example, it will try to crawl /a/b/, /a/, and /. Difficulties in implementing efficient web crawler There are two important characteristics of the Web that makes Web crawling a very difficult task: 1. Large volume of Web pages: Because a web crawler can only download a percentage of the online pages at any given moment due to the vast number of web pages, it is vital that the web crawler be intelligent enough to prioritize download. 2. Rate of change on web pages. Another issue in today's dynamic world is that online web pages are always changing. As a result, by the time the crawler downloads a site's last page, the page may have changed or a new page may have been added. These are the bare minimal requirements for a crawler: URL frontier: To keep track of the URLs to download and prioritize which ones should be crawled first. HTTP Fetcher: This program retrieves a web page from a server. Extractor: This program extracts hyperlinks from HTML texts. Duplicate Eliminator: To ensure that the identical content is not accidentally extracted twice. Datastore: This is where you'll keep retrieved pages, URLs, and other metadata.","title":"High Level design"},{"location":"DesigningWebCrawler/#detailed-component-design","text":"Assume that our crawler is operating on a single server and that all of the crawling is handled by many worker threads, each of which performs all of the processes required to download and process a document in a loop. The initial step in this loop is to remove an absolute URL from the download shared URL frontier. A scheme (for example, \"HTTP\") indicates the network protocol that should be used to download an absolute URL. We can build these protocols in a modular fashion for flexibility, so that our crawler can easily support additional protocols in the future. The worker contacts the relevant protocol module to download the document based on the URL scheme. The document is downloaded and stored in a Document Input Stream (DIS). By storing documents in DIS, other modules will be able to read them many times. The worker thread runs the dedupe test after writing the document to the DIS to see if this document (associated with a different URL) has been seen before. If this is the case, the document is not further processed, and the worker thread deletes the next URL from the frontier. The downloaded document must then be processed by our crawler. Each document can have its own MIME type, such as HTML page, Image, Video, and so on. We can build these MIME schemes in a modular fashion so that if our crawler needs to handle other kinds in the future, we can easily add them. The worker calls the process method of each processing module associated with the MIME type of the downloaded document based on that MIME type. In addition, our HTML processing module will extract all of the page's links. To evaluate if a link should be downloaded, it is transformed to an absolute URL and tested against a user-supplied URL filter. If the URL passes the filter, the worker does the URL-seen test, which determines whether the URL has been seen before, that is, whether it is in the URL frontier or has been downloaded. The URL is added to the border if it is new. Let\u2019s discuss these components in detail, and see how they can be fit onto multiple machines: 1. The URL frontier: The data structure that stores all of the URLs that need to be downloaded is known as the URL frontier. We can crawl by traversing the Web in order of breadth, beginning with the pages in the seed set. Using a FIFO queue, such traversals are simple to implement. We can distribute our URL frontier across multiple servers because we'll have a large list of URLs to crawl. Assume there are several worker threads conducting the crawling operations on each server. Let's also assume that our hash function associates each URL with a server that will crawl it. When constructing a distributed URL border, keep the following politeness requirements in mind: Our crawler should avoid overburdening a server by downloading a large number of pages. A web server should not be connected to numerous machines. On each server, our crawler can have a set of distinct FIFO sub-queues to implement this politeness restriction. Each worker thread will have its own sub-queue from which URLs for crawling will be removed. When a new URL needs to be added, the URL's canonical hostname determines which FIFO sub-queue it goes into. Each hostname can be mapped to a thread number using our hash function. These two factors together suggest that just one worker thread will download content from a given Web server, and that the Web server will not be overloaded because of the FIFO queue. What will the size of our URL border be? The number of URLs would be in the hundreds of millions. As a result, we'll need to save our URLs to disk. We can design our queues so that enqueuing and dequeuing buffers are separate. The enqueue buffer will be flushed to disk once it is full, whereas the dequeue buffer will preserve a cache of URLs that need to be visited and can read from disk to fill the buffer frequently. 2. The fetcher module: A fetcher module's job is to download the document associated with a specified URL using the proper network protocol, such as HTTP. As previously stated, webmasters use robot.txt to block the crawler from accessing certain areas of their websites. Our crawler's HTTP protocol module can retain a fixed-sized cache mapping host-names to their robot's exclusion criteria to avoid downloading this file on every request. 3. Document input stream: The design of our crawler allows many processing modules to process the same document. To avoid downloading a document several times, we use a Document Input Stream abstraction to cache the document locally (DIS). A DIS is a type of input stream that stores the whole content of a document downloaded over the internet. It also has options for rereading the document. Small documents (64 KB or less) can be cached totally in memory by the DIS, whereas bigger documents can be temporarily written to a backup file. Each worker thread has its own DIS, which it reuses from one document to the next. The worker delivers the URL extracted from the border to the appropriate protocol module, which initializes the DIS from a network connection to include the document's contents. The DIS is then passed to all appropriate processing modules by the worker. 4. Document Dedupe test: Many documents on the Internet are accessible via many URLs. There are also many instances where documents are replicated over many servers. Any Web crawler will download the same document several times as a result of both of these effects. We run a dedupe test on each document to eliminate duplication and prevent it from being processed twice. To execute this test, we can generate a 64-bit checksum for each processed document and store it in a database. Each new document's checksum can be compared to all previously calculated checksums to see if it's been seen before. MD5 or SHA can be used to calculate these checksums. What size would the checksum storage be? If the entire point of our checksum store is to perform dedupe, we only need to hold a single set of checksums for all previously processed documents. With 15 billion unique web pages, we would require: 15B * 8 bytes => 120 GB Although this can fit within the memory of a modern server, if we don't have enough, we can keep a smaller LRU-based cache on each server, with everything backed up by persistent storage. The checksum must be present in the cache for the dedupe test to succeed. If not, it must determine whether the checksum is stored in the back storage. If the checksum is found, the document will be ignored. It will be added to the cache and back storage if not. 5. URL filters: The URL filtering technique allows you to customize the list of URLs that are downloaded. This is used to create a blacklist of websites that our crawler will ignore. The worker thread evaluates the user-supplied URL filter before adding each URL to the frontier. Filters can be used to limit URLs by domain, prefix, or protocol type. 6. Domain name resolution: A Web crawler must use the Domain Name Service (DNS) to map the Web server's hostname into an IP address before accessing it. Given the number of URLs we'll be working with, DNS name resolution will be a major bottleneck for our crawlers. By constructing a local DNS server, we may begin caching DNS results to avoid repeated calls. 7. URL dedupe test: Any Web crawler will come across several links to the same document while extracting links. To prevent downloading and processing a document numerous times, each extracted link must pass a URL dedupe test before being included to the URL frontier. We can store all of the URLs observed by our crawler in canonical form in a database to execute the URL dedupe test. We save a fixed-sized checksum instead of the textual representation of each URL in the URL collection to conserve space. We can keep an in-memory cache of popular URLs on each host shared by all threads to reduce the amount of operations on the database store. The purpose for this cache is that some URLs include a lot of links, therefore caching the most popular ones in memory will result in a high hit rate. How much space do we need to store URLs? If the entire point of our checksum is to perform URL deduplication, we only need to retain a single set of checksums for all previously viewed URLs. With 15 billion unique URLs and a checksum of four bytes, we'd need: 15B * 4 bytes => 60 GB Can we use bloom filters for deduping? Bloom filters are a probabilistic data structure that can produce false positives when used for set membership checking. The set is represented by a big bit vector. By computing the element's 'n' hash functions and setting the relevant bits, an element is added to the set. If the bits at all 'n' of an element's hash locations are set, the element is considered to be in the set. As a result, a document may be mistakenly identified as part of the collection, but false negatives are not feasible. The drawback of applying a bloom filter for the URL seen test is that each false positive causes the URL to be removed from the frontier, preventing the document from being downloaded. Also, with the help of larger bit vector chance of a false positive can be reduced. 8. Checkpointing: It takes weeks to crawl the entire Internet. Our crawler can write regular snapshots of its state to the disk to protect against failures. A crawl that has been paused or aborted can easily be resumed from the most recent checkpoint.","title":"Detailed Component Design"},{"location":"DesigningWebCrawler/#fault-tolerance","text":"For distribution among crawling servers, we should utilize consistent hashing. Consistent hashing will aid in the replacement of a dead host as well as the distribution of load among crawling servers. All of our crawling servers will checkpoint and save their FIFO queues to disks on a regular basis. We can replace a server if it goes down. Meanwhile, reliable hashing should distribute the burden to other servers.","title":"Fault tolerance"},{"location":"DesigningWebCrawler/#data-partitioning","text":"Our crawler will deal with three different types of data: 1) Visitor URLs 2) Dedupe URL Checksums 3) Dedupe checksums should be documented. We can store these data on the same host because we are distributing URLs based on hostnames. As a result, each host will keep track of the URLs that must be visited, as well as the checksums of all previously visited URLs and the checksums of all downloaded contents. We may presume that URLs will be redistributed from overloaded hosts because we'll be utilizing consistent hashing. Periodically, each host will execute checkpointing and dump a snapshot of all the data it has on a distant server. This ensures that if a server fails, another server can take its data from the previous snapshot and replace it.","title":"Data Partitioning"},{"location":"DesigningWebCrawler/#crawler-traps","text":"Crawler traps, spam sites, and disguised material abound. Crawler traps are URLs or groups of URLs that cause a crawler to crawl forever. Unintentional crawler traps exist. A cycle can be created via a symbolic link within a file system, for example. Other crawler traps are placed on purpose. People have created traps that dynamically generate an infinite Web of documents, for example. The reasons for such traps differ. Anti-spam traps are designed to catch spammers hunting for email addresses, while search engine crawlers get caught by traps on other sites to improve their search rankings.","title":"Crawler Traps"},{"location":"DesigningYelporNearbyFriends/","text":"Designing Yelp or Nearby Friends Problem Statement Let's create a Yelp-style service where users can look for nearby restaurants, theaters, and shopping malls, among other things, and add/view reviews. Similar Services: Proximity server. Difficulty Level: Hard Why Yelp or Proximity Server? Proximity servers are used to find nearby attractions such as venues, events, and other activities. If you haven't utilized yelp.com before, do so first (you can search for nearby restaurants, theaters, and so on) and spend some time learning about the various possibilities available. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System What do we hope to accomplish with a Yelp-like service? Our service will store information about various locations so that users can search for them. Following a query, our service will offer a list of nearby locations. The following requirements should be met by our Yelp-like service: Functional Requirements: Places should be added, deleted, and updated by users. Users should be able to find all local places within a certain radius based on their location (longitude/latitude). Users should be allowed to leave feedback or reviews on a location. Pictures, text, and a rating can all be included in the feedback. Non-functional Requirements: Users should be able to search in real time with minimal delay. Our service should be able to handle a large number of searches. When compared to adding a new location, there will be a lot of search inquiries. Scale Estimation Assume our system has 500 million places and 100 thousand queries per second (QPS). Let's additionally assume a 20% annual increase in the number of locations and QPS. Database Schema Each Place can have the following fields: LocationID (8 bytes): Uniquely identifies a location. Name (256 bytes) Latitude (8 bytes) Longitude (8 bytes) Description (512 bytes) Category (1 byte): E.g., coffee shop, restaurant, theater, etc. Although a four-byte integer can uniquely identify 500 million places, we will use eight bytes for LocationID to accommodate future expansion. Total size: 8 + 256 + 8 + 8 + 512 + 1 => 793 bytes We also need to store reviews, photos, and ratings of a Place. We can have a different table to store reviews for Places: LocationID (8 bytes) ReviewID (4 bytes): Uniquely identifies a review, assuming any location will not have more than 2^32 reviews. ReviewText (512 bytes) Rating (1 byte): how many stars a place gets out of ten. Similarly, we can have a separate table to store photos for Places and Reviews. System APIs To expose the functionality of our service, we can use SOAP or REST APIs. The following could be the API for searching's definition: search(api_dev_key, search_terms, user_location, radius_filter, maximum_results_to_return, category_filter, sort, page_token) Parameters: api_dev_key (string): A registered account's API developer key. This will be used to throttle users based on their quota allocation, among other things. search_terms (string): A string containing the search terms. user_location (string): Location of the user. radius_filter (number): Optional search option to define radius in meters. maximum_results_to_return (number): Number of business results to return. category_filter (string): Optional category to filter search results, e.g., Restaurants, Shopping Centers, etc. sort (number): Optional sort mode: Best matched (0 - default), Minimum distance (1), Highest rated (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON file containing data about a list of businesses that match the search query. The business name, address, category, rating, and thumbnail will all be included in each search entry. Basic System Design and Algorithm At a high level, each dataset given above must be stored and indexed (places, reviews, etc.). Users expect to see results in real time while searching for local areas, therefore the indexing must be read efficiently for them to query this enormous database. We don't need to worry about frequent data updates because the position of a place doesn't change too often. In contrast, if we want to construct a service where items, such as people or cabs, change their location regularly, we might come up with a radically different design. Let's look at the various options for storing this information and determine which technique is appropriate for our needs: a. SQL solution One simple option is to put all of the information in a database such as MySQL. Each location will be recorded in its own row, with its own LocationID. Each location's longitude and latitude will be stored separately in two columns, so we'll need indexes on both of these values to perform a quick search. We can use the following query to find all the nearby places of a given location (X, Y) within a radius of 'D': Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D The preceding question is not quite accurate, as we know that the distance formula (Pythagorean theorem) must be used to find the distance between two points, but let's go with it for now. How efficient would this query be? We expect that our service will store 500 million items. Because we have two different indexes, each one can produce a large list of locations, making an intersection of the two lists inefficient. Another perspective on this issue is that there may be too many locations between 'X-D' and 'X+D,' and similarly between 'Y-D' and 'Y+D.' We can enhance the performance of our query if we can shorten these lists. b. Grids To arrange sites into smaller sets, we can divide the entire map into smaller grids. Each grid will contain all of the Places that fall within a specified longitude and latitude range. This technique would allow us to discover neighboring locations by querying only a few grids. We can identify all the neighboring grids based on a given location and radius, and then query these grids to find close places. It would be safe to assume that GridID (a four bytes number) would uniquely identify grids in our system. What could be a reasonable grid size? Because we also want to limit the number of grids, the grid size might be equal to the distance we want to query. If the grid size is the same as the distance we want to query, we just need to look in the grid that contains the supplied location and the eight grids around it. We can simply discover the grid number of any place (lat, long) and its nearby grids because our grids are statically specified (due to the set grid size). We may save the GridID with each location in the database and create an index on it for faster searching. Our query will now look like this: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D and GridID in (GridID, GridID1, GridID2, ..., GridID8) This will surely reduce the query's execution time. Is it necessary to store our index in memory? Our service's performance will be improved by keeping the index in memory. We can keep our index in a hash table with the grid number as the key and the list of places in the grid as the value. How much memory will we need to store the index? Let's say our search radius is 10 miles; we'll have 20 million grids because the earth's total area is roughly 200 million square miles. We'd need a four-byte integer to uniquely identify each grid, and since LocationID is eight bytes, we'd require 4GB of memory to hold the index (ignoring hash table overhead). (4 * 20M) + (8 * 500M) ~= 4 GB Because our locations are not evenly distributed among grids, this method may still be slow for grids with a large number of places. On the one hand, we can have densely crowded areas with many places, and on the other hand, we can have sparsely populated areas. This issue can be overcome if we can dynamically alter our grid size such that whenever we have a large grid, we can divide it down into smaller grids. The following are some potential drawbacks of this strategy: 1) how to map these grids to locations, and 2) how to locate all of a grid's neighboring grids. c. Dynamic size grids Let's pretend we don't want more than 500 spots in a grid since we want to search faster. When a grid hits this size limit, we divide it into four equal-sized grids and distribute locations among them. This implies densely crowded locations, such as downtown San Francisco, will have several grids, while sparsely populated areas, such as the Pacific Ocean, will have enormous grids with places just along the shore. What data-structure can hold this information? Our needs can be met by a tree with four children at each node. Each node will represent a grid and will store information about all of the locations within that grid. If a node surpasses our 500-place limit, it will be broken down into four child nodes, with spots distributed among them. All of the leaf nodes will now depict grids that can't be broken down any more. As a result, leaf nodes will maintain a list of locations. A QuadTree is a tree structure in which each node can have four offspring. How will we build a QuadTree? We'll start with a single node that represents the entire world in a single grid. We'll divide it into four nodes and distribute places among them because it'll contain more than 500 sites. This step will be repeated for each child node until no nodes with more than 500 locations remain. How will we find the grid for a given location? We'll start with the root node and work our way down to the node/grid we need. We'll check if the current node we're visiting has any children at each step. If it has, we will repeat the operation on the child node that has our desired location. If the node doesn't have any children, it's the one we want. How will we find neighboring grids of a given grid? We can connect all leaf nodes using a doubly linked list because only leaf nodes include a list of locations. We can iterate forward and backward among the surrounding leaf nodes to find our target positions in this fashion. Another method for locating adjacent grids is to use parent nodes. We can store a pointer in each node to access its parent, and we can quickly discover siblings of a node because each parent node holds pointers to all of its offspring. By continuing up through the parent pointers, we can keep broadening our search for neighboring grids. We can query the backend database to discover details about adjacent LocationIDs once we have them. What will be the search workflow? We'll start by locating the node that includes the user's current location. We can return them to the user if that node contains enough desired spots. If not, we'll keep expanding to surrounding nodes (either through parent pointers or a doubly linked list) until we either locate the needed number of places or our maximum radius search is exhausted. To store the QuadTree, how much memory will be required? If we merely cache LocationID and Lat/Long for each Place, we'd require 12GB to keep all of them. 24 * 500M => 12 GB Since each grid can have a maximum of 500 places, and we have 500M locations, how many total grids we will have? 500M / 500 => 1M grids That means we'll have 1 million leaf nodes, each with 12GB of location data. Internal nodes make up around 1/3 of a QuadTree with 1M leaf nodes, and each internal node has four pointers (for its children). If each pointer is 8 bytes long, the total amount of memory required to hold all internal nodes is: 1M * 1/3 * 4 * 8 = 10 MB As a result, total memory for the QuadTree would be 12.01GB. This is small enough to fit into a modern server. How would we insert a new Place into our system? We need to update the databases as well as the QuadTree whenever a new Place is added by a user. If our tree is on a single server, adding a new Place is simple; but, if the QuadTree is split across multiple servers, we must first locate the new Place's grid/server and then add it there (discussed in the next section). Data Partitioning What if we have so many places that our index will not fit in the memory of a single machine? With a 20% annual growth rate, we will eventually surpass the server's memory limit. What if one server is unable to handle all of the read traffic? We must partition our QuadTree to tackle these problems! We'll look at two options (both of these partitioning strategies can be used with databases): a. Sharding based on regions: We can divide our locations into regions (similar to zip codes), with each region's locations kept on a single node. To save a location, we will use the server's region, and to query for nearby places, we will use the region server that contains the user's position. This strategy has couple of flaws: What if a region becomes hot? On the server holding that region, there would be a lot of inquiries, making it slow. This will have an impact on the quality of our service. When compared to other regions, some can wind up storing a lot of spots over time. As a result, keeping a consistent allocation of sites while regions grow is difficult. We must either repartition our data or apply consistent hashing to recover from these instances. b. Sharding based on LocationID: Each LocationID will be mapped to a server where it will be stored by our hash function. We'll go through all of the places and calculate the hash of each LocationID to select a server where it may be stored while creating our QuadTree. To locate places near a location, we must query all servers, with each server returning a list of neighboring locations. These results will be compiled and returned to the user by a centralized server. Will we have different QuadTree structure on different partitions? Yes, this is possible because an equal number of locations in any particular grid on all partitions is not guaranteed. We do, however, ensure that each server has roughly the same number of Places. However, because we will be scanning all nearby grids within the stated radius on all partitions, the differing tree structure on different servers will not be an issue. The remaining part of this chapter is based on an assumtion that we have partitioned our data based on LocationID. Replication and Fault Tolerance Data partitioning can be replaced by having replicas of QuadTree servers. We can have clones of each QuadTree server to spread read traffic. We can set up a master-slave setup in which replicas (slaves) solely provide read traffic and all write traffic goes to the master before being applied to slaves. Slaves may not have certain newly inserted spots (there will be a few milliseconds delay), but this may be okay. What will happen when a QuadTree server dies? We can have a secondary replica of each server that will assume control after the primary dies. The QuadTree structure will be the same on both primary and backup servers. What if both primary and secondary servers die at the same time? We'll need to set aside a new server and rebuild the QuadTree on it. We don't know what locations were saved on this server, so how can we do that? The brute-force technique would be to cycle over the entire database, filtering LocationIDs with our hash function to find all of the required locations that would be kept on this server. This would be wasteful and slow; also, while the server is being rebuilt, we will be unable to service any queries from it, resulting in the users missing certain areas that they should have viewed. How can we efficiently retrieve a mapping between Places and QuadTree server? We must create a reverse index that maps all Places to their QuadTree server. This information can be stored on a separate QuadTree Index server. We'll need to create a HashMap with the QuadTree server number as the key and a HashSet containing all the Places kept on that QuadTree server as the value. Because information servers can generate QuadTrees using LocationID and Lat/Long, we need to keep them with each place. We've kept Places' data in a HashSet, which allows us to swiftly add and remove Places from our index. So now, anytime a QuadTree server has to rebuild itself, it can simply request all of the Places it requires from the QuadTree Index server. This method will undoubtedly be quick. For fault tolerance, we need additionally have a duplicate of the QuadTree Index server. If a QuadTree Index server fails, the index can be rebuilt by iterating through the database. Cache We can put a cache in front of our database to deal with busy places. We may use an off-the-shelf solution like Memcache to store all of the information about hot spots. Before reaching the backend database, application servers can rapidly verify if the cache has that Place. We can modify the number of cache servers required based on client usage patterns. Least Recently Used (LRU) appears to be a good cache eviction policy for our system. Load Balancing (LB) We can use the LB layer in our system in two places: 1) between clients and application servers, and 2) between application servers and backend servers. A basic Round Robin technique can be used to distribute all incoming requests evenly among backend servers at first. This LB is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, the load balancer will remove it from the rotation and stop distributing traffic to it. Round Robin LB has the drawback of not taking server load into account. The load balancer will not stop sending new requests to a server that is overcrowded or slow. To tackle this, a more intelligent LB solution would be required, one that polls the backend server about their load on a regular basis and adjusts traffic accordingly. Ranking How about if we want to rank the search results not only by proximity but also by popularity or relevance? How can we return most popular places within a given radius? Assume we keep track of each location's overall popularity. In our system, an aggregated figure can indicate this popularity, such as how many stars a location receives out of 10 (this would be an average of multiple user rankings)? This number will be saved in both the database and the QuadTree. We can ask each partition of the QuadTree to return the top 100 places with the most popularity while looking for the top 100 places within a defined radius. The aggregator server may then decide the top 100 locations from all of the locations returned by different partitions. Remember that our system was not designed to update location data often. How can we change the popularity of a location in our QuadTree using this design? Although we can search for a location and update its popularity in the QuadTree, this would consume a significant amount of resources and could slow down search queries and system throughput. We can decide to update it once or twice a day if the popularity of a location is not expected to show in the system within a few hours, especially when the load on the system is low. The QuadTree's dynamic updates are discussed in depth in our following challenge, Designing Uber backend.","title":"Designing Yelp or Nearby Friends"},{"location":"DesigningYelporNearbyFriends/#designing-yelp-or-nearby-friends","text":"","title":"Designing Yelp or Nearby Friends"},{"location":"DesigningYelporNearbyFriends/#problem-statement","text":"Let's create a Yelp-style service where users can look for nearby restaurants, theaters, and shopping malls, among other things, and add/view reviews. Similar Services: Proximity server. Difficulty Level: Hard","title":"Problem Statement"},{"location":"DesigningYelporNearbyFriends/#why-yelp-or-proximity-server","text":"Proximity servers are used to find nearby attractions such as venues, events, and other activities. If you haven't utilized yelp.com before, do so first (you can search for nearby restaurants, theaters, and so on) and spend some time learning about the various possibilities available.","title":"Why Yelp or Proximity Server?"},{"location":"DesigningYelporNearbyFriends/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningYelporNearbyFriends/#solution","text":"","title":"Solution"},{"location":"DesigningYelporNearbyFriends/#requirements-and-goals-of-the-system","text":"What do we hope to accomplish with a Yelp-like service? Our service will store information about various locations so that users can search for them. Following a query, our service will offer a list of nearby locations. The following requirements should be met by our Yelp-like service: Functional Requirements: Places should be added, deleted, and updated by users. Users should be able to find all local places within a certain radius based on their location (longitude/latitude). Users should be allowed to leave feedback or reviews on a location. Pictures, text, and a rating can all be included in the feedback. Non-functional Requirements: Users should be able to search in real time with minimal delay. Our service should be able to handle a large number of searches. When compared to adding a new location, there will be a lot of search inquiries.","title":"Requirements and Goals of the System"},{"location":"DesigningYelporNearbyFriends/#scale-estimation","text":"Assume our system has 500 million places and 100 thousand queries per second (QPS). Let's additionally assume a 20% annual increase in the number of locations and QPS.","title":"Scale Estimation"},{"location":"DesigningYelporNearbyFriends/#database-schema","text":"Each Place can have the following fields: LocationID (8 bytes): Uniquely identifies a location. Name (256 bytes) Latitude (8 bytes) Longitude (8 bytes) Description (512 bytes) Category (1 byte): E.g., coffee shop, restaurant, theater, etc. Although a four-byte integer can uniquely identify 500 million places, we will use eight bytes for LocationID to accommodate future expansion. Total size: 8 + 256 + 8 + 8 + 512 + 1 => 793 bytes We also need to store reviews, photos, and ratings of a Place. We can have a different table to store reviews for Places: LocationID (8 bytes) ReviewID (4 bytes): Uniquely identifies a review, assuming any location will not have more than 2^32 reviews. ReviewText (512 bytes) Rating (1 byte): how many stars a place gets out of ten. Similarly, we can have a separate table to store photos for Places and Reviews.","title":"Database Schema"},{"location":"DesigningYelporNearbyFriends/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs. The following could be the API for searching's definition: search(api_dev_key, search_terms, user_location, radius_filter, maximum_results_to_return, category_filter, sort, page_token) Parameters: api_dev_key (string): A registered account's API developer key. This will be used to throttle users based on their quota allocation, among other things. search_terms (string): A string containing the search terms. user_location (string): Location of the user. radius_filter (number): Optional search option to define radius in meters. maximum_results_to_return (number): Number of business results to return. category_filter (string): Optional category to filter search results, e.g., Restaurants, Shopping Centers, etc. sort (number): Optional sort mode: Best matched (0 - default), Minimum distance (1), Highest rated (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON file containing data about a list of businesses that match the search query. The business name, address, category, rating, and thumbnail will all be included in each search entry.","title":"System APIs"},{"location":"DesigningYelporNearbyFriends/#basic-system-design-and-algorithm","text":"At a high level, each dataset given above must be stored and indexed (places, reviews, etc.). Users expect to see results in real time while searching for local areas, therefore the indexing must be read efficiently for them to query this enormous database. We don't need to worry about frequent data updates because the position of a place doesn't change too often. In contrast, if we want to construct a service where items, such as people or cabs, change their location regularly, we might come up with a radically different design. Let's look at the various options for storing this information and determine which technique is appropriate for our needs: a. SQL solution One simple option is to put all of the information in a database such as MySQL. Each location will be recorded in its own row, with its own LocationID. Each location's longitude and latitude will be stored separately in two columns, so we'll need indexes on both of these values to perform a quick search. We can use the following query to find all the nearby places of a given location (X, Y) within a radius of 'D': Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D The preceding question is not quite accurate, as we know that the distance formula (Pythagorean theorem) must be used to find the distance between two points, but let's go with it for now. How efficient would this query be? We expect that our service will store 500 million items. Because we have two different indexes, each one can produce a large list of locations, making an intersection of the two lists inefficient. Another perspective on this issue is that there may be too many locations between 'X-D' and 'X+D,' and similarly between 'Y-D' and 'Y+D.' We can enhance the performance of our query if we can shorten these lists. b. Grids To arrange sites into smaller sets, we can divide the entire map into smaller grids. Each grid will contain all of the Places that fall within a specified longitude and latitude range. This technique would allow us to discover neighboring locations by querying only a few grids. We can identify all the neighboring grids based on a given location and radius, and then query these grids to find close places. It would be safe to assume that GridID (a four bytes number) would uniquely identify grids in our system. What could be a reasonable grid size? Because we also want to limit the number of grids, the grid size might be equal to the distance we want to query. If the grid size is the same as the distance we want to query, we just need to look in the grid that contains the supplied location and the eight grids around it. We can simply discover the grid number of any place (lat, long) and its nearby grids because our grids are statically specified (due to the set grid size). We may save the GridID with each location in the database and create an index on it for faster searching. Our query will now look like this: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D and GridID in (GridID, GridID1, GridID2, ..., GridID8) This will surely reduce the query's execution time. Is it necessary to store our index in memory? Our service's performance will be improved by keeping the index in memory. We can keep our index in a hash table with the grid number as the key and the list of places in the grid as the value. How much memory will we need to store the index? Let's say our search radius is 10 miles; we'll have 20 million grids because the earth's total area is roughly 200 million square miles. We'd need a four-byte integer to uniquely identify each grid, and since LocationID is eight bytes, we'd require 4GB of memory to hold the index (ignoring hash table overhead). (4 * 20M) + (8 * 500M) ~= 4 GB Because our locations are not evenly distributed among grids, this method may still be slow for grids with a large number of places. On the one hand, we can have densely crowded areas with many places, and on the other hand, we can have sparsely populated areas. This issue can be overcome if we can dynamically alter our grid size such that whenever we have a large grid, we can divide it down into smaller grids. The following are some potential drawbacks of this strategy: 1) how to map these grids to locations, and 2) how to locate all of a grid's neighboring grids. c. Dynamic size grids Let's pretend we don't want more than 500 spots in a grid since we want to search faster. When a grid hits this size limit, we divide it into four equal-sized grids and distribute locations among them. This implies densely crowded locations, such as downtown San Francisco, will have several grids, while sparsely populated areas, such as the Pacific Ocean, will have enormous grids with places just along the shore. What data-structure can hold this information? Our needs can be met by a tree with four children at each node. Each node will represent a grid and will store information about all of the locations within that grid. If a node surpasses our 500-place limit, it will be broken down into four child nodes, with spots distributed among them. All of the leaf nodes will now depict grids that can't be broken down any more. As a result, leaf nodes will maintain a list of locations. A QuadTree is a tree structure in which each node can have four offspring. How will we build a QuadTree? We'll start with a single node that represents the entire world in a single grid. We'll divide it into four nodes and distribute places among them because it'll contain more than 500 sites. This step will be repeated for each child node until no nodes with more than 500 locations remain. How will we find the grid for a given location? We'll start with the root node and work our way down to the node/grid we need. We'll check if the current node we're visiting has any children at each step. If it has, we will repeat the operation on the child node that has our desired location. If the node doesn't have any children, it's the one we want. How will we find neighboring grids of a given grid? We can connect all leaf nodes using a doubly linked list because only leaf nodes include a list of locations. We can iterate forward and backward among the surrounding leaf nodes to find our target positions in this fashion. Another method for locating adjacent grids is to use parent nodes. We can store a pointer in each node to access its parent, and we can quickly discover siblings of a node because each parent node holds pointers to all of its offspring. By continuing up through the parent pointers, we can keep broadening our search for neighboring grids. We can query the backend database to discover details about adjacent LocationIDs once we have them. What will be the search workflow? We'll start by locating the node that includes the user's current location. We can return them to the user if that node contains enough desired spots. If not, we'll keep expanding to surrounding nodes (either through parent pointers or a doubly linked list) until we either locate the needed number of places or our maximum radius search is exhausted. To store the QuadTree, how much memory will be required? If we merely cache LocationID and Lat/Long for each Place, we'd require 12GB to keep all of them. 24 * 500M => 12 GB Since each grid can have a maximum of 500 places, and we have 500M locations, how many total grids we will have? 500M / 500 => 1M grids That means we'll have 1 million leaf nodes, each with 12GB of location data. Internal nodes make up around 1/3 of a QuadTree with 1M leaf nodes, and each internal node has four pointers (for its children). If each pointer is 8 bytes long, the total amount of memory required to hold all internal nodes is: 1M * 1/3 * 4 * 8 = 10 MB As a result, total memory for the QuadTree would be 12.01GB. This is small enough to fit into a modern server. How would we insert a new Place into our system? We need to update the databases as well as the QuadTree whenever a new Place is added by a user. If our tree is on a single server, adding a new Place is simple; but, if the QuadTree is split across multiple servers, we must first locate the new Place's grid/server and then add it there (discussed in the next section).","title":"Basic System Design and Algorithm"},{"location":"DesigningYelporNearbyFriends/#data-partitioning","text":"What if we have so many places that our index will not fit in the memory of a single machine? With a 20% annual growth rate, we will eventually surpass the server's memory limit. What if one server is unable to handle all of the read traffic? We must partition our QuadTree to tackle these problems! We'll look at two options (both of these partitioning strategies can be used with databases): a. Sharding based on regions: We can divide our locations into regions (similar to zip codes), with each region's locations kept on a single node. To save a location, we will use the server's region, and to query for nearby places, we will use the region server that contains the user's position. This strategy has couple of flaws: What if a region becomes hot? On the server holding that region, there would be a lot of inquiries, making it slow. This will have an impact on the quality of our service. When compared to other regions, some can wind up storing a lot of spots over time. As a result, keeping a consistent allocation of sites while regions grow is difficult. We must either repartition our data or apply consistent hashing to recover from these instances. b. Sharding based on LocationID: Each LocationID will be mapped to a server where it will be stored by our hash function. We'll go through all of the places and calculate the hash of each LocationID to select a server where it may be stored while creating our QuadTree. To locate places near a location, we must query all servers, with each server returning a list of neighboring locations. These results will be compiled and returned to the user by a centralized server. Will we have different QuadTree structure on different partitions? Yes, this is possible because an equal number of locations in any particular grid on all partitions is not guaranteed. We do, however, ensure that each server has roughly the same number of Places. However, because we will be scanning all nearby grids within the stated radius on all partitions, the differing tree structure on different servers will not be an issue. The remaining part of this chapter is based on an assumtion that we have partitioned our data based on LocationID.","title":"Data Partitioning"},{"location":"DesigningYelporNearbyFriends/#replication-and-fault-tolerance","text":"Data partitioning can be replaced by having replicas of QuadTree servers. We can have clones of each QuadTree server to spread read traffic. We can set up a master-slave setup in which replicas (slaves) solely provide read traffic and all write traffic goes to the master before being applied to slaves. Slaves may not have certain newly inserted spots (there will be a few milliseconds delay), but this may be okay. What will happen when a QuadTree server dies? We can have a secondary replica of each server that will assume control after the primary dies. The QuadTree structure will be the same on both primary and backup servers. What if both primary and secondary servers die at the same time? We'll need to set aside a new server and rebuild the QuadTree on it. We don't know what locations were saved on this server, so how can we do that? The brute-force technique would be to cycle over the entire database, filtering LocationIDs with our hash function to find all of the required locations that would be kept on this server. This would be wasteful and slow; also, while the server is being rebuilt, we will be unable to service any queries from it, resulting in the users missing certain areas that they should have viewed. How can we efficiently retrieve a mapping between Places and QuadTree server? We must create a reverse index that maps all Places to their QuadTree server. This information can be stored on a separate QuadTree Index server. We'll need to create a HashMap with the QuadTree server number as the key and a HashSet containing all the Places kept on that QuadTree server as the value. Because information servers can generate QuadTrees using LocationID and Lat/Long, we need to keep them with each place. We've kept Places' data in a HashSet, which allows us to swiftly add and remove Places from our index. So now, anytime a QuadTree server has to rebuild itself, it can simply request all of the Places it requires from the QuadTree Index server. This method will undoubtedly be quick. For fault tolerance, we need additionally have a duplicate of the QuadTree Index server. If a QuadTree Index server fails, the index can be rebuilt by iterating through the database.","title":"Replication and Fault Tolerance"},{"location":"DesigningYelporNearbyFriends/#cache","text":"We can put a cache in front of our database to deal with busy places. We may use an off-the-shelf solution like Memcache to store all of the information about hot spots. Before reaching the backend database, application servers can rapidly verify if the cache has that Place. We can modify the number of cache servers required based on client usage patterns. Least Recently Used (LRU) appears to be a good cache eviction policy for our system.","title":"Cache"},{"location":"DesigningYelporNearbyFriends/#load-balancing-lb","text":"We can use the LB layer in our system in two places: 1) between clients and application servers, and 2) between application servers and backend servers. A basic Round Robin technique can be used to distribute all incoming requests evenly among backend servers at first. This LB is simple to set up and has no additional overhead. Another advantage of this method is that if a server goes down, the load balancer will remove it from the rotation and stop distributing traffic to it. Round Robin LB has the drawback of not taking server load into account. The load balancer will not stop sending new requests to a server that is overcrowded or slow. To tackle this, a more intelligent LB solution would be required, one that polls the backend server about their load on a regular basis and adjusts traffic accordingly.","title":"Load Balancing (LB)"},{"location":"DesigningYelporNearbyFriends/#ranking","text":"How about if we want to rank the search results not only by proximity but also by popularity or relevance? How can we return most popular places within a given radius? Assume we keep track of each location's overall popularity. In our system, an aggregated figure can indicate this popularity, such as how many stars a location receives out of 10 (this would be an average of multiple user rankings)? This number will be saved in both the database and the QuadTree. We can ask each partition of the QuadTree to return the top 100 places with the most popularity while looking for the top 100 places within a defined radius. The aggregator server may then decide the top 100 locations from all of the locations returned by different partitions. Remember that our system was not designed to update location data often. How can we change the popularity of a location in our QuadTree using this design? Although we can search for a location and update its popularity in the QuadTree, this would consume a significant amount of resources and could slow down search queries and system throughput. We can decide to update it once or twice a day if the popularity of a location is not expected to show in the system within a few hours, especially when the load on the system is low. The QuadTree's dynamic updates are discussed in depth in our following challenge, Designing Uber backend.","title":"Ranking"},{"location":"DesigningYoutubeorNetflix/","text":"Designing Youtube or Netflix Problem Statement Let's create a video sharing service similar to Youtube, where users may publish, view, and search videos. Similar Services: netflix.com, vimeo.com, dailymotion.com, veoh.com Difficulty Level: Medium What is Youtube? Youtube is one of the world's most popular video-sharing platforms. Users can submit, watch, share, rate, and report videos, as well as leave comments on them. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Hints to solve the problem 1. Consider functional and non-functional requirements. 2. Estimation of capacity and constraints, such as traffic, bandwidth, and storage. 3. Consider System APIs. 4. How do you create a database system? 5. What about data replication and partitioning? 6. Consider Cache and Load Balancing Solution Requirements and Goals of the System For the purposes of this exercise, we intend to create a simplified version of Youtube that meets the following criteria: Functional Requirements: Users should be able to upload videos. Users should be able to share and view videos. Users should be able to perform searches based on video titles. Our services should be able to record stats of videos, e.g., likes/dislikes, total number of views, etc. Users should be able to add and view comments on videos. Non-Functional Requirements: The system should be highly reliable, any video uploaded should not be lost. The system should be highly available. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a video for a while, it should be fine. Users should have a real time experience while watching videos and should not feel any lag. Not in scope: Video recommendations, most popular videos, channels, subscriptions, watch later, favorites, etc. Capacity Estimation and Constraints Assume there are 1.5 billion overall users, with 800 million being daily active users. If a consumer watches five videos each day on average, the total video views per second is: 800M * 5 / 86400 sec => 46K videos/sec Assume our upload:view ratio is 1:200, which is 200 videos are seen for every video posted, resulting in 230 videos uploaded each second. 46K / 200 => 230 videos/sec Storage Estimates: Assume that 500 hours of videos are added to Youtube every minute. If one minute of video requires an average of 50MB of storage (films must be kept in several codecs), the total storage required for videos uploaded in one minute is: 500 hours * 60 min * 50MB => 1500 GB/min (25 GB/sec) These numbers are calculated without taking into account video compression and replication, which might alter our results. Bandwidth estimates: With 500 hours of video uploads each minute and a bandwidth of 10MB/min per video upload, we'd be obtaining 300GB of data per minute. 500 hours * 60 mins * 10MB => 300GB/min (5GB/sec) Assuming an upload:view ratio of 1:200, we would need 1TB/s outgoing bandwidth. System APIs To expose the functionality of our service, we can use SOAP or REST APIs. The APIs for posting and searching videos could have the following definitions: uploadVideo(api_dev_key, video_title, vide_description, tags[], category_id, default_language, recording_details, video_contents) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. video_title (string): Title of the video. vide_description (string): Optional description of the video. tags (string[]): Optional tags for the video. category_id (string): Category of the video, e.g., Film, Song, People, etc. default_language (string): For example English, Mandarin, Hindi, etc. recording_details (string): Location where the video was recorded. video_contents (stream): Video to be uploaded. Returns: (string) A successful upload will return HTTP 202 (request accepted) and once the video encoding is completed the user is notified through email with a link to access the video. We can also expose a queryable API to let users know the current status of their uploaded video. searchVideo(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string): The API developer key of a registered account of our service. search_query (string): A string containing the search terms. user_location (string): Optional location of the user performing the search. maximum_videos_to_return (number): Maximum number of results returned in one request. page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about the list of video resources matching the search query. Each video resource will have a video title, a thumbnail, a video creation date, and a view count. streamVideo(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string): The API developer key of a registered account of our service. video_id (string): A string to identify the video. offset (number): We should be able to stream video from any offset, which is measured in seconds from the start of the video. We'll need to save the offset on the server if we allow playing/paused videos from many devices. Users will be able to resume watching a video on any device from the point where they left off. codec (string) & resolution(string): To support play/pause from different devices, the client should communicate the codec and resolution information to the API. Assume you were watching a video on Netflix on your TV when you paused it and switched to Netflix on your phone. Both these devices have different resolutions and use different codecs, therefore you'll need codec and resolution in this scenario. Returns: (STREAM) A media stream (a video chunk) from the given offset. High Level Design We'd require the following components at a high level: Processing Queue: Each submitted video will be placed in a processing queue, which will be de-queued for encoding, thumbnail generation, and storage later. Encoder: To encode each uploaded video into multiple formats. Thumbnails generator: To generate a few thumbnails for each video. Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage. User Database: To store user\u2019s information, e.g., name, email, address, etc. Video metadata storage: A metadata database that stores all of the information about videos, such as title, system file path, uploading user, total views, likes, dislikes, and so on. It will also serve as a repository for all video comments. Database Schema Video metadata storage - MySql A SQL database can be used to store video metadata. Each video should have the following information: VideoID Title Description Size Thumbnail Uploader/User Total number of likes Total number of dislikes Total number of views For each video comment, we need to store following information: CommentID VideoID UserID Comment TimeOfCreation User data storage - MySql UserID, Name, email, address, age, registration details etc. Detailed Component We'll focus on designing a system that can retrieve videos quickly because the service will be read-heavy. We may expect a read:write ratio of 200:1, which equals 200 video views for every video uploaded. Where would the videos be kept? A distributed file storage system such as HDFS or GlusterFS can be used to store videos. How should we manage read traffic efficiently? Read traffic and write traffic should be separated. We can split our read traffic over numerous servers because each video will have multiple copies. We can have master-slave arrangements for metadata, where writes go to the master first and subsequently to all slaves. Such configurations can result in data staleness; for example, when a new video is added, its metadata is placed in the master first, and our slaves will not be able to see it until it is applied to the slave; as a result, the user will receive stale results. This staleness might be acceptable in our system as it would be very short-lived and the user would be able to see the new videos after a few milliseconds. How would you store thumbnails? The number of thumbnails will far outnumber the number of videos. If we estimate that each movie will have five thumbnails, we'll need a storage system that can handle a large amount of read traffic. Before determining which thumbnail storage method to utilize, two factors must be considered: Thumbnails are little files with a maximum file size of 5KB. In comparison to videos, read traffic for thumbnails will be massive. Users will watch a single video at a time, although they may be viewing a page with 20 thumbnails of other films. Consider putting all of the thumbnails on a CD. Given the large number of files, we must do numerous seeks to various regions on the disk in order to read them. This is inefficient and will result in longer delays. Bigtable is a good option because it merges numerous files into a single block for storage on the disk and reads a modest quantity of data quickly. Our service's two most important needs are both of these. Keeping hot thumbnails in the cache will also assist to reduce latency, and because thumbnail files are short, we can simply cache a large number of them in memory. Video Uploads: Because videos can be large, we should support continuing from the same position if the connection stops while uploading. Video Encoding: Newly uploaded videos are saved on the server, and a new task to encode the video into several formats is added to the processing queue. The uploader will be alerted when all of the encoding is complete, and the video will be made available for viewing and sharing. Metadata Sharding We need to distribute our data across numerous machines to do read/write operations efficiently because we have a large number of new films every day and our read load is really high. We can shard our data in a variety of ways. Let's have a look at the various sharding strategies one by one: Sharding based on UserID: We can try storing all of a user's data on a single server. We can give the UserID to our hash function while storing, which will map the user to a database server where all of the metadata for that user's films will be stored. When searching for a user's videos, we can utilize our hash function to locate the server that holds the user's data and read it from there. We'll have to ask all servers to find videos by title, and each server will return a set of videos. Before sending the results to the user, a centralized server would aggregate and rank them. This strategy has several flaws: What if a user gets well-known? There could be a large number of queries on the server that hold that user, causing a performance bottleneck. This will have an impact on our service's overall performance. Some users may accumulate a large number of movies over time compared to others. It's difficult to keep a uniform distribution of rising user data. We must either repartition/redistribute our data or utilize consistent hashing to balance the load between servers to recover from these circumstances. Sharding based on VideoID: Each VideoID will be mapped to a random server where the metadata for that video will be stored. We will query all servers to find a user's videos, and each server will return a set of videos. Before sending these results to the user, a centralized server will aggregate and rank them. This method overcomes the problem of popular users, but also transfers the focus on popular videos. We can boost our performance even further by putting a cache in front of the database servers to store hot videos. Video Deduplication Our service will have to deal with widespread video duplication as a result of a large number of customers uploading a large volume of video data. Duplicate videos can have different aspect ratios or encodings, overlays or extra borders, or be fragments from a lengthier original video. The development of duplicate videos has a variety of consequences: Data Storage: Keeping numerous copies of the same video could be a waste of storage space. Caching: Duplicate movies reduce cache efficiency by consuming space that could be used for unique content. Network usage: Duplicate videos will increase the quantity of data that must be transferred over the network to caching services in-network. Energy consumption: Increased storage, an ineffective cache, and network utilization can all lead to energy waste. Duplicate search results, longer video startup times, and interrupted streaming for the end user will be the result of these inefficiencies. Deduplication makes the greatest sense for our service early on, while a user is uploading a video, rather than waiting until later to identify duplicate films. We will save a lot of resources by using inline deduplication instead of encoding, transferring, and storing the duplicate copy of the video. Our service can execute video matching algorithms (e.g., Block Matching, Phase Correlation, etc.) to discover duplications as soon as a user starts uploading a video. We can either halt the upload and use the old copy or continue the upload and use the freshly uploaded video if it is of higher quality. We can intelligently break the video into smaller chunks if the newly uploaded video is a subpart of an old video or vice versa, so that we only upload the missing parts. Load Balancing We should employ Consistent Hashing among our cache servers, as this will assist balance the demand between them. Due to the varying popularity of each video, we will be adopting a static hash-based approach to map videos to hostnames, which may result in an uneven load on the logical replicas. If a video gets popular, for example, the logical replica corresponding to that video will see more traffic than other servers. These unequal loads for logical replicas can then result in unequal load distribution on physical servers. - Any busy server in one area can send a client to a less busy server in the same cache location to remedy this issue. For this circumstance, we can employ dynamic HTTP redirections. However, there are several disadvantages to using redirections. First, because our service attempts to load balance locally, several redirections may occur if the server receiving the redirection is unable to serve the video. Furthermore, each redirection necessitates an additional HTTP request from the client, resulting in longer delays before the video begins to play. Furthermore, because higher tier caches are only available at a limited number of locations, inter-tier (or cross data-center) redirections send a client to a distant cache location. Cache Our service requires a large-scale video distribution infrastructure to serve globally distributed users. Using a huge number of geographically distributed video cache servers, our service should bring its material closer to the user. We need an approach that maximizes user performance while simultaneously distributing the load equitably across the cache servers. To cache hot database rows, we can add a cache to metadata servers. Application servers can quickly verify if the cache has the needed rows by using Memcache to cache the data before reaching the database. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. The least recently viewed row is discarded first under this policy. How can we make our cache more intelligent? If we follow the 80-20 rule, which states that 20% of daily video read volume generates 80% of traffic, implying that some videos are so popular that the majority of people watch them, we can try caching 20% of daily video read volume and metadata. Content Delivery Network (CDN) A content delivery network (CDN) is a network of distributed servers that deliver web content to users based on their geographic location, the origin of the web page, and the position of a content delivery server. In our Caching chapter, look at the 'CDN' section. Popular videos can be moved to CDNs using our service: Content is replicated in many locations using CDNs. Videos are more likely to be closer to the user, and videos will stream from a friendlier network with fewer hops. CDNs make extensive use of caching and can typically serve videos from memory. Our servers in multiple data centers can provide less popular videos (1-20 views per day) that are not cached by CDNs. Fault Tolerance For database server distribution, we should utilize Consistent Hashing. Consistent hashing will aid in the replacement of a dead server as well as the distribution of load among servers.","title":"Designing Youtube or Netflix"},{"location":"DesigningYoutubeorNetflix/#designing-youtube-or-netflix","text":"","title":"Designing Youtube or Netflix"},{"location":"DesigningYoutubeorNetflix/#problem-statement","text":"Let's create a video sharing service similar to Youtube, where users may publish, view, and search videos. Similar Services: netflix.com, vimeo.com, dailymotion.com, veoh.com Difficulty Level: Medium","title":"Problem Statement"},{"location":"DesigningYoutubeorNetflix/#what-is-youtube","text":"Youtube is one of the world's most popular video-sharing platforms. Users can submit, watch, share, rate, and report videos, as well as leave comments on them.","title":"What is Youtube?"},{"location":"DesigningYoutubeorNetflix/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningYoutubeorNetflix/#solution","text":"","title":"Solution"},{"location":"DesigningYoutubeorNetflix/#requirements-and-goals-of-the-system","text":"For the purposes of this exercise, we intend to create a simplified version of Youtube that meets the following criteria: Functional Requirements: Users should be able to upload videos. Users should be able to share and view videos. Users should be able to perform searches based on video titles. Our services should be able to record stats of videos, e.g., likes/dislikes, total number of views, etc. Users should be able to add and view comments on videos. Non-Functional Requirements: The system should be highly reliable, any video uploaded should not be lost. The system should be highly available. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a video for a while, it should be fine. Users should have a real time experience while watching videos and should not feel any lag. Not in scope: Video recommendations, most popular videos, channels, subscriptions, watch later, favorites, etc.","title":"Requirements and Goals of the System"},{"location":"DesigningYoutubeorNetflix/#capacity-estimation-and-constraints","text":"Assume there are 1.5 billion overall users, with 800 million being daily active users. If a consumer watches five videos each day on average, the total video views per second is: 800M * 5 / 86400 sec => 46K videos/sec Assume our upload:view ratio is 1:200, which is 200 videos are seen for every video posted, resulting in 230 videos uploaded each second. 46K / 200 => 230 videos/sec Storage Estimates: Assume that 500 hours of videos are added to Youtube every minute. If one minute of video requires an average of 50MB of storage (films must be kept in several codecs), the total storage required for videos uploaded in one minute is: 500 hours * 60 min * 50MB => 1500 GB/min (25 GB/sec) These numbers are calculated without taking into account video compression and replication, which might alter our results. Bandwidth estimates: With 500 hours of video uploads each minute and a bandwidth of 10MB/min per video upload, we'd be obtaining 300GB of data per minute. 500 hours * 60 mins * 10MB => 300GB/min (5GB/sec) Assuming an upload:view ratio of 1:200, we would need 1TB/s outgoing bandwidth.","title":"Capacity Estimation and Constraints"},{"location":"DesigningYoutubeorNetflix/#system-apis","text":"To expose the functionality of our service, we can use SOAP or REST APIs. The APIs for posting and searching videos could have the following definitions: uploadVideo(api_dev_key, video_title, vide_description, tags[], category_id, default_language, recording_details, video_contents) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. video_title (string): Title of the video. vide_description (string): Optional description of the video. tags (string[]): Optional tags for the video. category_id (string): Category of the video, e.g., Film, Song, People, etc. default_language (string): For example English, Mandarin, Hindi, etc. recording_details (string): Location where the video was recorded. video_contents (stream): Video to be uploaded. Returns: (string) A successful upload will return HTTP 202 (request accepted) and once the video encoding is completed the user is notified through email with a link to access the video. We can also expose a queryable API to let users know the current status of their uploaded video. searchVideo(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string): The API developer key of a registered account of our service. search_query (string): A string containing the search terms. user_location (string): Optional location of the user performing the search. maximum_videos_to_return (number): Maximum number of results returned in one request. page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about the list of video resources matching the search query. Each video resource will have a video title, a thumbnail, a video creation date, and a view count. streamVideo(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string): The API developer key of a registered account of our service. video_id (string): A string to identify the video. offset (number): We should be able to stream video from any offset, which is measured in seconds from the start of the video. We'll need to save the offset on the server if we allow playing/paused videos from many devices. Users will be able to resume watching a video on any device from the point where they left off. codec (string) & resolution(string): To support play/pause from different devices, the client should communicate the codec and resolution information to the API. Assume you were watching a video on Netflix on your TV when you paused it and switched to Netflix on your phone. Both these devices have different resolutions and use different codecs, therefore you'll need codec and resolution in this scenario. Returns: (STREAM) A media stream (a video chunk) from the given offset.","title":"System APIs"},{"location":"DesigningYoutubeorNetflix/#high-level-design","text":"We'd require the following components at a high level: Processing Queue: Each submitted video will be placed in a processing queue, which will be de-queued for encoding, thumbnail generation, and storage later. Encoder: To encode each uploaded video into multiple formats. Thumbnails generator: To generate a few thumbnails for each video. Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage. User Database: To store user\u2019s information, e.g., name, email, address, etc. Video metadata storage: A metadata database that stores all of the information about videos, such as title, system file path, uploading user, total views, likes, dislikes, and so on. It will also serve as a repository for all video comments.","title":"High Level Design"},{"location":"DesigningYoutubeorNetflix/#database-schema","text":"Video metadata storage - MySql A SQL database can be used to store video metadata. Each video should have the following information: VideoID Title Description Size Thumbnail Uploader/User Total number of likes Total number of dislikes Total number of views For each video comment, we need to store following information: CommentID VideoID UserID Comment TimeOfCreation User data storage - MySql UserID, Name, email, address, age, registration details etc.","title":"Database Schema"},{"location":"DesigningYoutubeorNetflix/#detailed-component","text":"We'll focus on designing a system that can retrieve videos quickly because the service will be read-heavy. We may expect a read:write ratio of 200:1, which equals 200 video views for every video uploaded. Where would the videos be kept? A distributed file storage system such as HDFS or GlusterFS can be used to store videos. How should we manage read traffic efficiently? Read traffic and write traffic should be separated. We can split our read traffic over numerous servers because each video will have multiple copies. We can have master-slave arrangements for metadata, where writes go to the master first and subsequently to all slaves. Such configurations can result in data staleness; for example, when a new video is added, its metadata is placed in the master first, and our slaves will not be able to see it until it is applied to the slave; as a result, the user will receive stale results. This staleness might be acceptable in our system as it would be very short-lived and the user would be able to see the new videos after a few milliseconds. How would you store thumbnails? The number of thumbnails will far outnumber the number of videos. If we estimate that each movie will have five thumbnails, we'll need a storage system that can handle a large amount of read traffic. Before determining which thumbnail storage method to utilize, two factors must be considered: Thumbnails are little files with a maximum file size of 5KB. In comparison to videos, read traffic for thumbnails will be massive. Users will watch a single video at a time, although they may be viewing a page with 20 thumbnails of other films. Consider putting all of the thumbnails on a CD. Given the large number of files, we must do numerous seeks to various regions on the disk in order to read them. This is inefficient and will result in longer delays. Bigtable is a good option because it merges numerous files into a single block for storage on the disk and reads a modest quantity of data quickly. Our service's two most important needs are both of these. Keeping hot thumbnails in the cache will also assist to reduce latency, and because thumbnail files are short, we can simply cache a large number of them in memory. Video Uploads: Because videos can be large, we should support continuing from the same position if the connection stops while uploading. Video Encoding: Newly uploaded videos are saved on the server, and a new task to encode the video into several formats is added to the processing queue. The uploader will be alerted when all of the encoding is complete, and the video will be made available for viewing and sharing.","title":"Detailed Component"},{"location":"DesigningYoutubeorNetflix/#metadata-sharding","text":"We need to distribute our data across numerous machines to do read/write operations efficiently because we have a large number of new films every day and our read load is really high. We can shard our data in a variety of ways. Let's have a look at the various sharding strategies one by one: Sharding based on UserID: We can try storing all of a user's data on a single server. We can give the UserID to our hash function while storing, which will map the user to a database server where all of the metadata for that user's films will be stored. When searching for a user's videos, we can utilize our hash function to locate the server that holds the user's data and read it from there. We'll have to ask all servers to find videos by title, and each server will return a set of videos. Before sending the results to the user, a centralized server would aggregate and rank them. This strategy has several flaws: What if a user gets well-known? There could be a large number of queries on the server that hold that user, causing a performance bottleneck. This will have an impact on our service's overall performance. Some users may accumulate a large number of movies over time compared to others. It's difficult to keep a uniform distribution of rising user data. We must either repartition/redistribute our data or utilize consistent hashing to balance the load between servers to recover from these circumstances. Sharding based on VideoID: Each VideoID will be mapped to a random server where the metadata for that video will be stored. We will query all servers to find a user's videos, and each server will return a set of videos. Before sending these results to the user, a centralized server will aggregate and rank them. This method overcomes the problem of popular users, but also transfers the focus on popular videos. We can boost our performance even further by putting a cache in front of the database servers to store hot videos.","title":"Metadata Sharding"},{"location":"DesigningYoutubeorNetflix/#video-deduplication","text":"Our service will have to deal with widespread video duplication as a result of a large number of customers uploading a large volume of video data. Duplicate videos can have different aspect ratios or encodings, overlays or extra borders, or be fragments from a lengthier original video. The development of duplicate videos has a variety of consequences: Data Storage: Keeping numerous copies of the same video could be a waste of storage space. Caching: Duplicate movies reduce cache efficiency by consuming space that could be used for unique content. Network usage: Duplicate videos will increase the quantity of data that must be transferred over the network to caching services in-network. Energy consumption: Increased storage, an ineffective cache, and network utilization can all lead to energy waste. Duplicate search results, longer video startup times, and interrupted streaming for the end user will be the result of these inefficiencies. Deduplication makes the greatest sense for our service early on, while a user is uploading a video, rather than waiting until later to identify duplicate films. We will save a lot of resources by using inline deduplication instead of encoding, transferring, and storing the duplicate copy of the video. Our service can execute video matching algorithms (e.g., Block Matching, Phase Correlation, etc.) to discover duplications as soon as a user starts uploading a video. We can either halt the upload and use the old copy or continue the upload and use the freshly uploaded video if it is of higher quality. We can intelligently break the video into smaller chunks if the newly uploaded video is a subpart of an old video or vice versa, so that we only upload the missing parts.","title":"Video Deduplication"},{"location":"DesigningYoutubeorNetflix/#load-balancing","text":"We should employ Consistent Hashing among our cache servers, as this will assist balance the demand between them. Due to the varying popularity of each video, we will be adopting a static hash-based approach to map videos to hostnames, which may result in an uneven load on the logical replicas. If a video gets popular, for example, the logical replica corresponding to that video will see more traffic than other servers. These unequal loads for logical replicas can then result in unequal load distribution on physical servers. - Any busy server in one area can send a client to a less busy server in the same cache location to remedy this issue. For this circumstance, we can employ dynamic HTTP redirections. However, there are several disadvantages to using redirections. First, because our service attempts to load balance locally, several redirections may occur if the server receiving the redirection is unable to serve the video. Furthermore, each redirection necessitates an additional HTTP request from the client, resulting in longer delays before the video begins to play. Furthermore, because higher tier caches are only available at a limited number of locations, inter-tier (or cross data-center) redirections send a client to a distant cache location.","title":"Load Balancing"},{"location":"DesigningYoutubeorNetflix/#cache","text":"Our service requires a large-scale video distribution infrastructure to serve globally distributed users. Using a huge number of geographically distributed video cache servers, our service should bring its material closer to the user. We need an approach that maximizes user performance while simultaneously distributing the load equitably across the cache servers. To cache hot database rows, we can add a cache to metadata servers. Application servers can quickly verify if the cache has the needed rows by using Memcache to cache the data before reaching the database. For our system, the Least Recently Used (LRU) policy may be an appropriate cache eviction policy. The least recently viewed row is discarded first under this policy. How can we make our cache more intelligent? If we follow the 80-20 rule, which states that 20% of daily video read volume generates 80% of traffic, implying that some videos are so popular that the majority of people watch them, we can try caching 20% of daily video read volume and metadata.","title":"Cache"},{"location":"DesigningYoutubeorNetflix/#content-delivery-network-cdn","text":"A content delivery network (CDN) is a network of distributed servers that deliver web content to users based on their geographic location, the origin of the web page, and the position of a content delivery server. In our Caching chapter, look at the 'CDN' section. Popular videos can be moved to CDNs using our service: Content is replicated in many locations using CDNs. Videos are more likely to be closer to the user, and videos will stream from a friendlier network with fewer hops. CDNs make extensive use of caching and can typically serve videos from memory. Our servers in multiple data centers can provide less popular videos (1-20 views per day) that are not cached by CDNs.","title":"Content Delivery Network (CDN)"},{"location":"DesigningYoutubeorNetflix/#fault-tolerance","text":"For database server distribution, we should utilize Consistent Hashing. Consistent hashing will aid in the replacement of a dead server as well as the distribution of load among servers.","title":"Fault Tolerance"},{"location":"Events/","text":"WebSockets vs. Server-Sent Events vs. Long-Polling **Can you explain the differences between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are some of the most popular communication protocols between a client and a web server. Let's start with a basic understanding of how a regular HTTP web request looks. The sequence of events for a standard HTTP request is as follows: The client establishes a connection with the server and requests data. The response is calculated by the server. On the opened request, the server provides the response back to the client. Polling with Ajax The great majority of AJAX apps employ polling as a common strategy. The basic concept is that the client polls (or requests) data from the server on a regular basis. The client submits a request and awaits a response from the server. An empty response is returned if no data is available. The client establishes a connection with the server and sends a standard HTTP request for data. The requested webpage sends periodic queries to the server (e.g., 0.5 seconds). Just like regular HTTP traffic, the server calculates the response and sends it back. To acquire updates from the server, the client performs the first three steps on a regular basis. Polling has the drawback of requiring the client to continually requesting the server for new data. As a result, many answers are blank, resulting in HTTP overhead. Long-Polling HTTP This is a version of traditional polling that allows the server to push data to a client whenever it is available. Long-Polling involves the client requesting information from the server in the same way that standard polling does, but with the caveat that the server may not respond right away. This is why this approach is also known as a \"Hanging GET.\" Instead of returning an empty answer if the server does not have any data for the client, the server holds the request and waits for data to become available. A full response is delivered to the client once the data is accessible. The client then instantly re-requests information from the server, ensuring that the server has a waiting request ready to give data in response to an event virtually all of the time. The following is the basic life cycle of an application that uses HTTP Long-Polling: The client issues a standard HTTP request and then waits for a response. The server waits until an update is available or a timeout occurs before responding. When an update is available, the server sends the client a complete response. After getting a response, the client normally sends a new long-poll request, either immediately or after a delay to provide for an acceptable latency duration. There is a timeout on each Long-Poll request. After a connection is lost owing to timeouts, the client must rejoin on a regular basis. WebSockets Over a single TCP connection, WebSocket delivers full duplex communication channels. It establishes a permanent link between a client and a server, allowing both parties to begin transferring data at any moment. The WebSocket handshake is the process by which the client establishes a WebSocket connection. If the operation is successful, the server and client can freely communicate data in both ways. The WebSocket protocol allows for low-latency communication between a client and a server, allowing for real-time data flow from and to the server. This is accomplished by providing a standardized method for the server to deliver content to the browser without the client's permission, as well as allowing messages to be passed back and forth while the connection is open. A two-way (bi-directional) continuing dialogue between a client and a server can be established in this manner. Server-Delivered Events (SSEs) SSEs allow the client to establish a long-term and persistent connection with the server. This connection is used by the server to send data to a client. If the client wants to communicate data to the server, it will have to do so using a different technology/protocol. A client uses HTTP to request data from a server. The requesting webpage establishes a server connection. When new information becomes available, the server provides it to the client. When real-time transmission from the server to the client is required, or if the server generates data in a loop and will be transmitting several events to the client, SSEs are the ideal choice.","title":"Long-Polling vs WebSockets vs Server-Sent Events"},{"location":"Events/#websockets-vs-server-sent-events-vs-long-polling","text":"**Can you explain the differences between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are some of the most popular communication protocols between a client and a web server. Let's start with a basic understanding of how a regular HTTP web request looks. The sequence of events for a standard HTTP request is as follows: The client establishes a connection with the server and requests data. The response is calculated by the server. On the opened request, the server provides the response back to the client.","title":"WebSockets vs. Server-Sent Events vs. Long-Polling"},{"location":"Events/#polling-with-ajax","text":"The great majority of AJAX apps employ polling as a common strategy. The basic concept is that the client polls (or requests) data from the server on a regular basis. The client submits a request and awaits a response from the server. An empty response is returned if no data is available. The client establishes a connection with the server and sends a standard HTTP request for data. The requested webpage sends periodic queries to the server (e.g., 0.5 seconds). Just like regular HTTP traffic, the server calculates the response and sends it back. To acquire updates from the server, the client performs the first three steps on a regular basis. Polling has the drawback of requiring the client to continually requesting the server for new data. As a result, many answers are blank, resulting in HTTP overhead.","title":"Polling with Ajax"},{"location":"Events/#long-polling-http","text":"This is a version of traditional polling that allows the server to push data to a client whenever it is available. Long-Polling involves the client requesting information from the server in the same way that standard polling does, but with the caveat that the server may not respond right away. This is why this approach is also known as a \"Hanging GET.\" Instead of returning an empty answer if the server does not have any data for the client, the server holds the request and waits for data to become available. A full response is delivered to the client once the data is accessible. The client then instantly re-requests information from the server, ensuring that the server has a waiting request ready to give data in response to an event virtually all of the time. The following is the basic life cycle of an application that uses HTTP Long-Polling: The client issues a standard HTTP request and then waits for a response. The server waits until an update is available or a timeout occurs before responding. When an update is available, the server sends the client a complete response. After getting a response, the client normally sends a new long-poll request, either immediately or after a delay to provide for an acceptable latency duration. There is a timeout on each Long-Poll request. After a connection is lost owing to timeouts, the client must rejoin on a regular basis.","title":"Long-Polling HTTP"},{"location":"Events/#websockets","text":"Over a single TCP connection, WebSocket delivers full duplex communication channels. It establishes a permanent link between a client and a server, allowing both parties to begin transferring data at any moment. The WebSocket handshake is the process by which the client establishes a WebSocket connection. If the operation is successful, the server and client can freely communicate data in both ways. The WebSocket protocol allows for low-latency communication between a client and a server, allowing for real-time data flow from and to the server. This is accomplished by providing a standardized method for the server to deliver content to the browser without the client's permission, as well as allowing messages to be passed back and forth while the connection is open. A two-way (bi-directional) continuing dialogue between a client and a server can be established in this manner.","title":"WebSockets"},{"location":"Events/#server-delivered-events-sses","text":"SSEs allow the client to establish a long-term and persistent connection with the server. This connection is used by the server to send data to a client. If the client wants to communicate data to the server, it will have to do so using a different technology/protocol. A client uses HTTP to request data from a server. The requesting webpage establishes a server connection. When new information becomes available, the server provides it to the client. When real-time transmission from the server to the client is required, or if the server generates data in a loop and will be transmitting several events to the client, SSEs are the ideal choice.","title":"Server-Delivered Events (SSEs)"},{"location":"Indexes/","text":"Indexes When it comes to databases, indexes are well-known. Database performance will eventually degrade to the point where it is no longer acceptable. When this happens, database indexing is one of the first things you should do. The purpose of setting an index on a table in a database is to make it easier to search through the table and discover the row or rows we're looking for. Indexes can be built utilizing one or more columns from a database table, allowing for quick random lookups as well as efficient access to ordered items. An example would be a library catalog. A library catalog is a list of books found in a library that is kept in a register. The catalog is laid out in the format of a database table, with four columns: book title, author, subject, and publication date. Typically, there are two such catalogs: one ordered by book title and the other sorted by author name. That way, you may either think of a writer you'd like to read and then browse their books, or hunt up a specific book title you'd like to read if you don't know the author's name. These catalogs function as indexes for the book database. They give you a sorted set of data that you can search for pertinent information in. Simply said, an index is a data structure that functions similarly to a table of contents in that it directs us to the real data. When we establish an index on a table column, we store the column as well as a pointer to the entire row in the index. Assuming you have a table with a list of books, the diagram below depicts how an index on the 'Title' column would look: This notion can be used to larger datasets in the same way that a standard relational data store can. When it comes to indexes, the trick is to think about how users will access the data. Indexes are essential for maximizing data access when data sets are many terabytes in size but have very little payloads (e.g., 1 KB). Finding a tiny payload in such a vast dataset can be difficult because we can't iterate over all of it in an acceptable amount of time. Furthermore, such a big data collection is extremely likely to be dispersed among numerous physical devices, necessitating the development of a method for locating the correct physical location of the needed data. The best way to do this is to use indexes. How do Indexes decrease write performance? An index can drastically speed up data retrieval, but because of the additional keys, it can be rather huge, slowing down data input and update. When adding rows to a table with an active index or updating existing rows, we must not only write the data but also update the index. The writing performance will suffer as a result of this. All insert, update, and delete actions for the table suffer from this speed decrease. As a result, adding unnecessary indexes to tables should be avoided, and obsolete indexes should be eliminated. To be clear, increasing indexes is all about enhancing the speed of search queries. If the purpose of the database is to offer a data store that is frequently written to but seldom read from, then slowing down the more common process, writing, is probably not worth the performance boost we obtain from reading. See this page for further information. Database Indexes","title":"Indexes"},{"location":"Indexes/#indexes","text":"When it comes to databases, indexes are well-known. Database performance will eventually degrade to the point where it is no longer acceptable. When this happens, database indexing is one of the first things you should do. The purpose of setting an index on a table in a database is to make it easier to search through the table and discover the row or rows we're looking for. Indexes can be built utilizing one or more columns from a database table, allowing for quick random lookups as well as efficient access to ordered items. An example would be a library catalog. A library catalog is a list of books found in a library that is kept in a register. The catalog is laid out in the format of a database table, with four columns: book title, author, subject, and publication date. Typically, there are two such catalogs: one ordered by book title and the other sorted by author name. That way, you may either think of a writer you'd like to read and then browse their books, or hunt up a specific book title you'd like to read if you don't know the author's name. These catalogs function as indexes for the book database. They give you a sorted set of data that you can search for pertinent information in. Simply said, an index is a data structure that functions similarly to a table of contents in that it directs us to the real data. When we establish an index on a table column, we store the column as well as a pointer to the entire row in the index. Assuming you have a table with a list of books, the diagram below depicts how an index on the 'Title' column would look: This notion can be used to larger datasets in the same way that a standard relational data store can. When it comes to indexes, the trick is to think about how users will access the data. Indexes are essential for maximizing data access when data sets are many terabytes in size but have very little payloads (e.g., 1 KB). Finding a tiny payload in such a vast dataset can be difficult because we can't iterate over all of it in an acceptable amount of time. Furthermore, such a big data collection is extremely likely to be dispersed among numerous physical devices, necessitating the development of a method for locating the correct physical location of the needed data. The best way to do this is to use indexes.","title":"Indexes"},{"location":"Indexes/#how-do-indexes-decrease-write-performance","text":"An index can drastically speed up data retrieval, but because of the additional keys, it can be rather huge, slowing down data input and update. When adding rows to a table with an active index or updating existing rows, we must not only write the data but also update the index. The writing performance will suffer as a result of this. All insert, update, and delete actions for the table suffer from this speed decrease. As a result, adding unnecessary indexes to tables should be avoided, and obsolete indexes should be eliminated. To be clear, increasing indexes is all about enhancing the speed of search queries. If the purpose of the database is to offer a data store that is frequently written to but seldom read from, then slowing down the more common process, writing, is probably not worth the performance boost we obtain from reading. See this page for further information. Database Indexes","title":"How do Indexes decrease write performance?"},{"location":"KeyCharDistributedSystem/","text":"Key Characteristics of Distributed Systems Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them: Scalability Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly. Horizontal vs. Vertical Scaling: Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#key-characteristics-of-distributed-systems","text":"Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them:","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#scalability","text":"Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.","title":"Scalability"},{"location":"KeyCharDistributedSystem/#horizontal-vs-vertical-scaling","text":"Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Horizontal vs. Vertical Scaling:"},{"location":"LoadBalancing/","text":"Load Balancing A load balancer sits in front of your server and acts as a \"traffic cop,\" directing client requests across all servers. It aids in the distribution of traffic among a cluster of servers in order to improve application, website, or database responsiveness and availability. While distributing requests, LB also maintains track of the status of all the resources. A load balancer can be a real device, a virtualized instance, or a software process that runs on specialized hardware. Load Balancer will stop transmitting traffic to a server if it is not available to accept new requests, is not responding, or has a high error rate. We can strive to balance the load at each tier of the system to achieve full scalability and redundancy. Load balancers can be added in three places: Between the web server and the user Between web servers and a platform layer within the company, such as application servers or cache servers Between the database and the internal platform layer. Load Balancing Algorithms There are several load balancing methods available, each of which employs a different algorithm to meet the needs of the user. 1. Least Connection Method : Traffic is sent to the server with the fewest active connections using this strategy. When there are a significant number of persistent client connections that are distributed unevenly throughout the servers, this strategy is quite useful. 2. Least Response Time Method : Traffic is routed to the server with the fewest active connections and the fastest average response time. 3. Least Bandwidth Method : This approach chooses the server with the least amount of traffic (measured in megabits per second) at the time (Mbps). 4. Round Robin Method : This approach sends each new request to the next server after cycling through a list of servers. It restarts from the beginning when it reaches the end of the list. It's most useful when the servers are of similar specs and there aren't a lot of permanent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is intended to accommodate servers with varying processing capacities more effectively. A weight is allocated to each server (an integer value that indicates the processing capacity). Higher-weighted servers receive new connections before lower-weighted servers, and higher-weighted servers receive more connections than lower-weighted servers. 6. IP Hash : To redirect the request to a server, this approach calculates a hash of the client's IP address. Redundant Load Balancers The load balancer can be a single point of failure; to avoid this, link a second load balancer to the first to form a cluster. Each load balancer keeps an eye on the other's health, and since they're both capable of providing traffic and detecting failures, if the main load balancer fails, the second load balancer takes over. Benefits of Load Balancing Users benefit from faster and more consistent service. Users will not have to wait for a single underperforming server to complete its earlier responsibilities. Instead, their queries are forwarded to a more readily available resource right away. There is reduced downtime and higher throughput for service providers. Even a complete server failure will have no effect on the end user experience because the load balancer will simply redirect traffic to a healthy server. Load balancing makes it easier for system administrators to deal with incoming requests while reducing the amount of time users have to wait. Predictive analytics, which detects traffic bottlenecks before they occur, is one of the advantages of smart load balancers. As a result, the smart load balancer provides actionable data to a business. These are essential for automation and can aid in commercial decision-making. There are fewer defective or strained components for system managers to deal with. Instead of a single device doing a lot of work, load balancing distributes the workload among numerous devices. The following links contain useful information regarding load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing","text":"A load balancer sits in front of your server and acts as a \"traffic cop,\" directing client requests across all servers. It aids in the distribution of traffic among a cluster of servers in order to improve application, website, or database responsiveness and availability. While distributing requests, LB also maintains track of the status of all the resources. A load balancer can be a real device, a virtualized instance, or a software process that runs on specialized hardware. Load Balancer will stop transmitting traffic to a server if it is not available to accept new requests, is not responding, or has a high error rate. We can strive to balance the load at each tier of the system to achieve full scalability and redundancy. Load balancers can be added in three places: Between the web server and the user Between web servers and a platform layer within the company, such as application servers or cache servers Between the database and the internal platform layer.","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing-algorithms","text":"There are several load balancing methods available, each of which employs a different algorithm to meet the needs of the user. 1. Least Connection Method : Traffic is sent to the server with the fewest active connections using this strategy. When there are a significant number of persistent client connections that are distributed unevenly throughout the servers, this strategy is quite useful. 2. Least Response Time Method : Traffic is routed to the server with the fewest active connections and the fastest average response time. 3. Least Bandwidth Method : This approach chooses the server with the least amount of traffic (measured in megabits per second) at the time (Mbps). 4. Round Robin Method : This approach sends each new request to the next server after cycling through a list of servers. It restarts from the beginning when it reaches the end of the list. It's most useful when the servers are of similar specs and there aren't a lot of permanent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is intended to accommodate servers with varying processing capacities more effectively. A weight is allocated to each server (an integer value that indicates the processing capacity). Higher-weighted servers receive new connections before lower-weighted servers, and higher-weighted servers receive more connections than lower-weighted servers. 6. IP Hash : To redirect the request to a server, this approach calculates a hash of the client's IP address.","title":"Load Balancing Algorithms"},{"location":"LoadBalancing/#redundant-load-balancers","text":"The load balancer can be a single point of failure; to avoid this, link a second load balancer to the first to form a cluster. Each load balancer keeps an eye on the other's health, and since they're both capable of providing traffic and detecting failures, if the main load balancer fails, the second load balancer takes over.","title":"Redundant Load Balancers"},{"location":"LoadBalancing/#benefits-of-load-balancing","text":"Users benefit from faster and more consistent service. Users will not have to wait for a single underperforming server to complete its earlier responsibilities. Instead, their queries are forwarded to a more readily available resource right away. There is reduced downtime and higher throughput for service providers. Even a complete server failure will have no effect on the end user experience because the load balancer will simply redirect traffic to a healthy server. Load balancing makes it easier for system administrators to deal with incoming requests while reducing the amount of time users have to wait. Predictive analytics, which detects traffic bottlenecks before they occur, is one of the advantages of smart load balancers. As a result, the smart load balancer provides actionable data to a business. These are essential for automation and can aid in commercial decision-making. There are fewer defective or strained components for system managers to deal with. Instead of a single device doing a lot of work, load balancing distributes the workload among numerous devices. The following links contain useful information regarding load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Benefits of Load Balancing"},{"location":"PraticeDesignProblems/","text":"Pratice on full Screen","title":"Pratice Design Problems"},{"location":"PreRequisites/","text":"Pre Requisites In this part, we'll presume that you: Have worked with a relational database before ( like MySQL ). Have a basic understanding of NoSQL databases. Know the fundamentals of the following: 1. Concurrency : Are you familiar with the terms threads, deadlock, and starvation? When numerous processes / threads attempt to edit the same data, what happens? A fundamental grasp of read and write locks is required. 2. Networking : Do you have a basic understanding of networking protocols such as TCP and UDP? Do you know what switches and routers are for? 3. File systems : You should have a good understanding of the systems you're working with. Do you have a basic understanding of how an operating system, file system, and database work? Do you understand the several levels of caching in a modern operating system?","title":"Pre Requisites"},{"location":"PreRequisites/#pre-requisites","text":"In this part, we'll presume that you: Have worked with a relational database before ( like MySQL ). Have a basic understanding of NoSQL databases. Know the fundamentals of the following: 1. Concurrency : Are you familiar with the terms threads, deadlock, and starvation? When numerous processes / threads attempt to edit the same data, what happens? A fundamental grasp of read and write locks is required. 2. Networking : Do you have a basic understanding of networking protocols such as TCP and UDP? Do you know what switches and routers are for? 3. File systems : You should have a good understanding of the systems you're working with. Do you have a basic understanding of how an operating system, file system, and database work? Do you understand the several levels of caching in a modern operating system?","title":"Pre Requisites"},{"location":"Proxies/","text":"Proxy Servers A proxy server is a server that sits in the middle of the client and the back-end server. Clients connect to proxy servers to request services such as a web page, a file, a connection, and so on. In a nutshell, a proxy server is a piece of software or hardware that acts as a middleman between clients and other servers when they request resources. Proxy servers are commonly used to filter requests, log requests, and occasionally alter requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another benefit of a proxy server is that it can handle a large number of requests in its cache. If a resource is accessed by numerous clients, the proxy server can cache it and serve it to all clients without having to go to the remote server. Types of Proxy Servers Proxies can be found on the client's local server or between the client and remote servers. Here are a few well-known proxy server types: Open Proxy An open proxy is a proxy server that anyone on the Internet can visit. In general, a proxy server only permits users within a network group (i.e., a closed proxy) to store and forward Internet services like DNS or web pages in order to reduce and regulate the group's bandwidth usage. An open proxy, on the other hand, allows any Internet user to use this forwarding service. There are two well-known open proxy types: Anonymous Proxy - This proxy identifies itself as a server but does not reveal the client's real IP address. Though this proxy server is easily detectable, it can be beneficial to some users because it hides their IP address. Tr\u0430nsp\u0430rent Proxy \u2013 tr\u0430nsp\u0430rent Proxy \u2013 This proxy server identifies itself, and the first IP address can be viewed with the use of HTTP headers. The main advantage of using this type of server is its ability to cache web pages. Reverse Proxy A reverse proxy retrieves resources from one or more servers on behalf of a client. These resources are then returned to the client, making it look as if they came from the proxy server. References Open Proxy Reverse Proxy","title":"Proxies"},{"location":"Proxies/#proxy-servers","text":"A proxy server is a server that sits in the middle of the client and the back-end server. Clients connect to proxy servers to request services such as a web page, a file, a connection, and so on. In a nutshell, a proxy server is a piece of software or hardware that acts as a middleman between clients and other servers when they request resources. Proxy servers are commonly used to filter requests, log requests, and occasionally alter requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another benefit of a proxy server is that it can handle a large number of requests in its cache. If a resource is accessed by numerous clients, the proxy server can cache it and serve it to all clients without having to go to the remote server.","title":"Proxy Servers"},{"location":"Proxies/#types-of-proxy-servers","text":"Proxies can be found on the client's local server or between the client and remote servers. Here are a few well-known proxy server types:","title":"Types of Proxy Servers"},{"location":"Proxies/#open-proxy","text":"An open proxy is a proxy server that anyone on the Internet can visit. In general, a proxy server only permits users within a network group (i.e., a closed proxy) to store and forward Internet services like DNS or web pages in order to reduce and regulate the group's bandwidth usage. An open proxy, on the other hand, allows any Internet user to use this forwarding service. There are two well-known open proxy types: Anonymous Proxy - This proxy identifies itself as a server but does not reveal the client's real IP address. Though this proxy server is easily detectable, it can be beneficial to some users because it hides their IP address. Tr\u0430nsp\u0430rent Proxy \u2013 tr\u0430nsp\u0430rent Proxy \u2013 This proxy server identifies itself, and the first IP address can be viewed with the use of HTTP headers. The main advantage of using this type of server is its ability to cache web pages.","title":"Open Proxy"},{"location":"Proxies/#reverse-proxy","text":"A reverse proxy retrieves resources from one or more servers on behalf of a client. These resources are then returned to the client, making it look as if they came from the proxy server. References Open Proxy Reverse Proxy","title":"Reverse Proxy"},{"location":"RedundancyReplication/","text":"Replication and Redundancy Redundancy is the duplication of important components or functions in a system with the goal of boosting the system's reliability, usually as a backup or fail-safe, or improving real system performance. If a single copy of a file is stored on a single server, for example, losing that server means losing the file. Because losing data is rarely a good thing, we can remedy this problem by making duplicate or redundant copies of the file. Redundancy is important because it eliminates single points of failure in the system and offers backups in the event of a disaster. For instance, if one of two service instances running in production fails, the system can failover to the other. Replication is the process of exchanging data among redundant resources, such as software or hardware components, in order to improve dependability, fault tolerance, or accessibility. Many database management systems (DBMS) use replication, which is commonly done using a master-slave connection between the original and the copies. The master receives all updates, which are then sent to the slaves. Each slave sends a message indicating that the update was properly received, enabling for the delivery of following updates. References Replication Redundancy Fault Tolerance","title":"Redundancy and Replication"},{"location":"RedundancyReplication/#replication-and-redundancy","text":"Redundancy is the duplication of important components or functions in a system with the goal of boosting the system's reliability, usually as a backup or fail-safe, or improving real system performance. If a single copy of a file is stored on a single server, for example, losing that server means losing the file. Because losing data is rarely a good thing, we can remedy this problem by making duplicate or redundant copies of the file. Redundancy is important because it eliminates single points of failure in the system and offers backups in the event of a disaster. For instance, if one of two service instances running in production fails, the system can failover to the other. Replication is the process of exchanging data among redundant resources, such as software or hardware components, in order to improve dependability, fault tolerance, or accessibility. Many database management systems (DBMS) use replication, which is commonly done using a master-slave connection between the original and the copies. The master receives all updates, which are then sent to the slaves. Each slave sends a message indicating that the update was properly received, enabling for the delivery of following updates. References Replication Redundancy Fault Tolerance","title":"Replication and Redundancy"},{"location":"SQLvsNoSQL/","text":"SQL vs. NoSQL There are two sorts of database solutions in the world: SQL and NoSQL (or relational databases and non-relational databases). The manner they were designed, the type of information they hold, and the storage mechanism they use are all different. Relational databases, like phone books that record phone numbers and addresses, are structured and have predetermined schemas. Non-relational databases, like file folders, are unstructured, distributed, and feature a dynamic schema that stores everything from a person's address and phone number to their Facebook \"likes\" and online shopping preferences. SQL Data is stored in rows and columns in relational databases. Each row holds all of the data for a single entity, whereas each column has all of the individual data points. MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB are some of the most popular relational databases. NoSQL The most prevalent forms of NoSQL are as follows: Key-Value Stores: Data is kept as a key-value pair array. A 'key' is a term for an attribute that is associated to a 'value.' Redis, Voldemort, and Dynamo are examples of well-known key-value stores. Document Databases: Data is kept in documents (rather than rows and columns in a table) in these databases, and these documents are grouped together in collections. The structure of any document can be completely different. CouchDB and MongoDB are two document databases. Wide-Column Databases: Column families, rather than 'tables,' are containers for rows in columnar databases. We don't need to know all the columns up front, and each row doesn't have to have the same number of columns as a relational database. Columnar databases, such as Cassandra and HBase, are ideally suited for analyzing massive datasets. Graph Databases: These databases are used to hold information that can best be represented as a graph. Nodes (entities), attributes (information about the entities), and lines are used to store data in graph topologies (connections between the entities). Neo4J and Infinite Graph are two examples of graph databases. Differences in SQL vs NoSQL on a high level Storage: SQL saves data in tables, with each row representing an entity and each column representing a data point about that entity; for example, if we're keeping a car entity in a table, distinct columns might be 'Color, Make, Model,' and so on. The data storage models used by NoSQL databases vary. Key-value, document, graph, and columnar are the most common types. Below, we'll compare and contrast these two databases. Schema: Each record in SQL must adhere to a fixed schema, which means that the columns must be determined and chosen prior to data entry, and each row must have data for each field. Later, the schema can be changed, but it will require updating the entire database and going offline. Schemas in NoSQL are dynamic. Columns can be added at any time, and each 'row' (or equivalent) does not need to include data for each 'column.' Querying: SQL (structured query language) is a powerful language for defining and manipulating data in SQL databases. Queries in a NoSQL database are focused on a set of documents. UnQL is another name for it (Unstructured Query Language). The syntax for using UnQL varies depending on the database. Scalability: In most cases, SQL databases are vertically scalable, meaning that they may be scaled up by increasing the hardware's horsepower (memory, CPU, etc.), which can be quite costly. A relational database can be scaled across numerous servers, but it is a difficult and time-consuming operation. NoSQL databases, on the other hand, are horizontally scalable, which means we can quickly add more servers to our NoSQL database infrastructure to manage a large amount of traffic. NoSQL databases may be hosted on any cheap commodity hardware or cloud instances, making it much more cost-effective than vertical scaling. Many NoSQL technologies also automatically spread data across servers. Atomicity, Consistency, Isolation, and Durability (ACID) Compliance: ACID compliance is seen in the vast majority of relational databases. So, when it comes to data consistency and transaction security, SQL databases are still the preferable option. For performance and scalability, most NoSQL solutions forgo ACID compliance. SQL vs. NoSQL: Which is Better? There is no such thing as a one-size-fits-all solution when it comes to database technology. As a result, many firms use both relational and non-relational databases for various purposes. Despite the fact that NoSQL databases are becoming more popular due to their speed and scalability, there are still occasions where a highly organized SQL database may outperform; the proper technology depends on the use case. Benefits of Using a SQL Database Here are some of the benefits of using a SQL database: We must ensure that ACID compliance is met. By specifying exactly how transactions interact with the database, ACID compliance lowers anomalies and maintains the integrity of your database. In general, NoSQL databases forego ACID compliance in favor of scalability and processing speed, yet an ACID-compliant database remains the preferred solution for many e-commerce and financial applications. Your data is well-organized and stable. There may be no reason to employ a system built to support a range of data kinds and large traffic volume if your firm is not seeing enormous expansion that would necessitate more servers and if you're simply working with data that is consistent. Benefits of Using a NoSQL Database When all of our application's other components are running smoothly, NoSQL databases keep data from becoming a bottleneck. Big data is helping NoSQL databases achieve great success, owing to the fact that it processes data differently than traditional relational databases. MongoDB, CouchDB, Cassandra, and HBase are some examples of NoSQL databases. Managing enormous amounts of data with little to no organization. A NoSQL database has no restrictions on the types of data that can be stored together and allows us to add new types as our needs evolve. You can store data in one place with document-based databases without needing to describe what \"kind\" of data you're storing in advance. Using cloud computing and storage to its full potential. Cloud storage is a great way to save money, but it requires data to be easily transferred over several servers in order to scale up. Using commodity (cheap, smaller) hardware on-site or in the cloud eliminates the need for additional software, and NoSQL databases like Cassandra are built to scale across numerous data centers without causing a lot of headaches out of the box. Rapid progress. Because NoSQL does not require any prior preparation, it is ideal for quick development. A relational database will slow you down if you're working on quick iterations of your system that require frequent adjustments to the data structure with little downtime between versions.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql-vs-nosql","text":"There are two sorts of database solutions in the world: SQL and NoSQL (or relational databases and non-relational databases). The manner they were designed, the type of information they hold, and the storage mechanism they use are all different. Relational databases, like phone books that record phone numbers and addresses, are structured and have predetermined schemas. Non-relational databases, like file folders, are unstructured, distributed, and feature a dynamic schema that stores everything from a person's address and phone number to their Facebook \"likes\" and online shopping preferences.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql","text":"Data is stored in rows and columns in relational databases. Each row holds all of the data for a single entity, whereas each column has all of the individual data points. MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB are some of the most popular relational databases.","title":"SQL"},{"location":"SQLvsNoSQL/#nosql","text":"The most prevalent forms of NoSQL are as follows: Key-Value Stores: Data is kept as a key-value pair array. A 'key' is a term for an attribute that is associated to a 'value.' Redis, Voldemort, and Dynamo are examples of well-known key-value stores. Document Databases: Data is kept in documents (rather than rows and columns in a table) in these databases, and these documents are grouped together in collections. The structure of any document can be completely different. CouchDB and MongoDB are two document databases. Wide-Column Databases: Column families, rather than 'tables,' are containers for rows in columnar databases. We don't need to know all the columns up front, and each row doesn't have to have the same number of columns as a relational database. Columnar databases, such as Cassandra and HBase, are ideally suited for analyzing massive datasets. Graph Databases: These databases are used to hold information that can best be represented as a graph. Nodes (entities), attributes (information about the entities), and lines are used to store data in graph topologies (connections between the entities). Neo4J and Infinite Graph are two examples of graph databases.","title":"NoSQL"},{"location":"SQLvsNoSQL/#differences-in-sql-vs-nosql-on-a-high-level","text":"Storage: SQL saves data in tables, with each row representing an entity and each column representing a data point about that entity; for example, if we're keeping a car entity in a table, distinct columns might be 'Color, Make, Model,' and so on. The data storage models used by NoSQL databases vary. Key-value, document, graph, and columnar are the most common types. Below, we'll compare and contrast these two databases. Schema: Each record in SQL must adhere to a fixed schema, which means that the columns must be determined and chosen prior to data entry, and each row must have data for each field. Later, the schema can be changed, but it will require updating the entire database and going offline. Schemas in NoSQL are dynamic. Columns can be added at any time, and each 'row' (or equivalent) does not need to include data for each 'column.' Querying: SQL (structured query language) is a powerful language for defining and manipulating data in SQL databases. Queries in a NoSQL database are focused on a set of documents. UnQL is another name for it (Unstructured Query Language). The syntax for using UnQL varies depending on the database. Scalability: In most cases, SQL databases are vertically scalable, meaning that they may be scaled up by increasing the hardware's horsepower (memory, CPU, etc.), which can be quite costly. A relational database can be scaled across numerous servers, but it is a difficult and time-consuming operation. NoSQL databases, on the other hand, are horizontally scalable, which means we can quickly add more servers to our NoSQL database infrastructure to manage a large amount of traffic. NoSQL databases may be hosted on any cheap commodity hardware or cloud instances, making it much more cost-effective than vertical scaling. Many NoSQL technologies also automatically spread data across servers. Atomicity, Consistency, Isolation, and Durability (ACID) Compliance: ACID compliance is seen in the vast majority of relational databases. So, when it comes to data consistency and transaction security, SQL databases are still the preferable option. For performance and scalability, most NoSQL solutions forgo ACID compliance.","title":"Differences in SQL vs NoSQL on a high level"},{"location":"SQLvsNoSQL/#sql-vs-nosql-which-is-better","text":"There is no such thing as a one-size-fits-all solution when it comes to database technology. As a result, many firms use both relational and non-relational databases for various purposes. Despite the fact that NoSQL databases are becoming more popular due to their speed and scalability, there are still occasions where a highly organized SQL database may outperform; the proper technology depends on the use case.","title":"SQL vs. NoSQL: Which is Better?"},{"location":"SQLvsNoSQL/#benefits-of-using-a-sql-database","text":"Here are some of the benefits of using a SQL database: We must ensure that ACID compliance is met. By specifying exactly how transactions interact with the database, ACID compliance lowers anomalies and maintains the integrity of your database. In general, NoSQL databases forego ACID compliance in favor of scalability and processing speed, yet an ACID-compliant database remains the preferred solution for many e-commerce and financial applications. Your data is well-organized and stable. There may be no reason to employ a system built to support a range of data kinds and large traffic volume if your firm is not seeing enormous expansion that would necessitate more servers and if you're simply working with data that is consistent.","title":"Benefits of Using a SQL Database"},{"location":"SQLvsNoSQL/#benefits-of-using-a-nosql-database","text":"When all of our application's other components are running smoothly, NoSQL databases keep data from becoming a bottleneck. Big data is helping NoSQL databases achieve great success, owing to the fact that it processes data differently than traditional relational databases. MongoDB, CouchDB, Cassandra, and HBase are some examples of NoSQL databases. Managing enormous amounts of data with little to no organization. A NoSQL database has no restrictions on the types of data that can be stored together and allows us to add new types as our needs evolve. You can store data in one place with document-based databases without needing to describe what \"kind\" of data you're storing in advance. Using cloud computing and storage to its full potential. Cloud storage is a great way to save money, but it requires data to be easily transferred over several servers in order to scale up. Using commodity (cheap, smaller) hardware on-site or in the cloud eliminates the need for additional software, and NoSQL databases like Cassandra are built to scale across numerous data centers without causing a lot of headaches out of the box. Rapid progress. Because NoSQL does not require any prior preparation, it is ideal for quick development. A relational database will slow you down if you're working on quick iterations of your system that require frequent adjustments to the data structure with little downtime between versions.","title":"Benefits of Using a NoSQL Database"},{"location":"StepsToApproachAProblem/","text":"Steps To Approach A Problem It is suggested that you solve the problem by following the procedures below. Feature Expectations (First 2 Minutes): As previously said, there is no such thing as a bad design. There are only excellent and terrible designs, and the same solution may be good for one use case but awful for another. As a result, it is critical to have a thorough understanding of the question's requirements. Approximations ( 2-5 mins ) The next step is usually to calculate the system's scale. The purpose of this step is to figure out how much sharding is needed (if any) and to narrow down the system's design goals. For example, if all of the system's data fits on a single machine, we might not need to worry about sharding of the other issues that come with a distributed system architecture. Alternatively, if the most frequently used data fits on a single machine, caching might be done there. Design Objectives ( 1 mins ) Determine what the system's most critical objectives are. It's possible that some systems are latency systems, in which case a solution that ignores this could result in poor design. The design's skeleton ( 4 - 5 mins ) There isn't enough time to go over each component in depth in 30-40 minutes. As a result, a solid method is to talk with the interviewer at a general level and then dive into the components that the interviewer has asked about. Extensive research ( 20-30 mins ) Using low level and high level design, go over the problem in detail. The more detailed question needs to think and asked can be seen in next session In the next session, we'll look at the more detailed questions that need to be considered and asked.","title":"Steps To Approach A Problem"},{"location":"StepsToApproachAProblem/#steps-to-approach-a-problem","text":"It is suggested that you solve the problem by following the procedures below.","title":"Steps To Approach A Problem"},{"location":"StepsToApproachAProblem/#feature-expectations-first-2-minutes","text":"As previously said, there is no such thing as a bad design. There are only excellent and terrible designs, and the same solution may be good for one use case but awful for another. As a result, it is critical to have a thorough understanding of the question's requirements.","title":"Feature Expectations (First 2 Minutes):"},{"location":"StepsToApproachAProblem/#approximations-2-5-mins","text":"The next step is usually to calculate the system's scale. The purpose of this step is to figure out how much sharding is needed (if any) and to narrow down the system's design goals. For example, if all of the system's data fits on a single machine, we might not need to worry about sharding of the other issues that come with a distributed system architecture. Alternatively, if the most frequently used data fits on a single machine, caching might be done there.","title":"Approximations ( 2-5 mins )"},{"location":"StepsToApproachAProblem/#design-objectives-1-mins","text":"Determine what the system's most critical objectives are. It's possible that some systems are latency systems, in which case a solution that ignores this could result in poor design.","title":"Design Objectives ( 1 mins )"},{"location":"StepsToApproachAProblem/#the-designs-skeleton-4-5-mins","text":"There isn't enough time to go over each component in depth in 30-40 minutes. As a result, a solid method is to talk with the interviewer at a general level and then dive into the components that the interviewer has asked about.","title":"The design's skeleton ( 4 - 5 mins )"},{"location":"StepsToApproachAProblem/#extensive-research-20-30-mins","text":"Using low level and high level design, go over the problem in detail. The more detailed question needs to think and asked can be seen in next session In the next session, we'll look at the more detailed questions that need to be considered and asked.","title":"Extensive research ( 20-30 mins )"},{"location":"SystemDesignBasic/","text":"System Design Basics Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basics"},{"location":"SystemDesignBasic/#system-design-basics","text":"Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basics"},{"location":"SystemDesignInterviews/","text":"A Step-by-Step Guide to System Design Interviews Many software developers have difficulty with system design interviews (SDIs) for three reasons: SDIs are unstructured in nature, with applicants being asked to work on an open-ended design challenge with no standard solution. Candidates are lacking in intricate and large-scale system development experience. Candidates did not devote enough time on SDI preparation. Candidates who haven't put out a concerted effort to prepare for SDIs, like those who haven't put forth a concerted effort to prepare for coding interviews, typically perform poorly, particularly at top organizations such as Google, Facebook, Amazon, Microsoft, and others. Candidates who do not perform above average at these companies have a slim probability of being hired. A good performance, on the other hand, invariably leads to a better offer (a higher job and compensation), as it demonstrates the candidate's capacity to manage a complicated system. This article will take a step-by-step approach to solving numerous design issues. Let's begin with the steps below: Step 1: Clarification of requirements It's always a good idea to ask about the scope of the problem we're attempting to solve. Because design questions are typically open-ended and don't have a single proper solution, it's vital to clear up any misunderstandings early in the interview. Candidates that take the effort to establish the system's end goals have a better chance of succeeding in the interview. We should also define which aspects of the system we will be working on given we only have 35-40 minutes to build a (apparently) vast system. Let's have a look at a real-world example of how to construct a Twitter-like service. Before moving on to the next steps, you should answer the following questions about Twitter design: Will our service's users be able to publish tweets and follow others? Should we also design for the user's timeline to be created and displayed? Will images and videos be included in tweets? Are we concentrating just on the backend or are we also working on the front-end? Will users be able to use Twitter to search for tweets? Is it necessary to show hot subjects that are currently trending? Will new (or important) tweets be notified via push notification? All of these considerations will influence the final design. Step 2: Estimation from the back of the envelope Estimating the scope of the system we're going to design is always a smart idea. This will also come in handy later when we're dealing with scaling, partitioning, load balancing, and caching. On what scale should the system operate (e.g., number of new tweets, tweet views, timeline generation per second, etc.)? What kind of storage will we require? If users may include photographs and videos in their tweets, we will have different storage needs. What kind of network bandwidth utilization do we anticipate? This will be critical in determining how we will manage traffic and load balance among servers. Step 3: Define the system interface Define which APIs the system should provide. This will not only define the exact contract that the system should deliver, but it will also guarantee that we haven't missed any criteria. APIs for our Twitter-like service will include the following: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026) Step 4: Defining the data model Defining the data model early in the interview will help to define how data will move between the system's various components. It will later provide instructions for data partitioning and administration. The candidate must be able to recognize various system entities, how they will interact with one another, and various areas of data management such as storage, transportation, encryption, and so on. For our Twitter-like service, here are some entities: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Should we use a relational database system or a relational database system? Will a NoSQL database, such as Cassandra, be the greatest fit for our purposes, or should we go with a MySQL-like solution? To store photographs and videos, what type of block storage should we use? Step 5: High-level design Create a block diagram with 5-6 boxes to illustrate the system's essential components. We should determine how many components are required to tackle the problem from beginning to conclusion. At a high level, numerous application servers will be required to service all read/write requests for Twitter, with load balancers in front of them to distribute traffic. If we anticipate a high volume of read traffic (in comparison to write traffic), we can set up separate servers to handle these circumstances. We'll need an efficient database on the backend to hold all of the tweets and support a large number of reads. For storing photographs and movies, we'll also need a distributed file storage system. Step 6: Detailed design Examine two or three important components in further depth; the interviewer's input should always point us in the direction of which portions of the system require more study. We should be able to demonstrate many approaches, their benefits and drawbacks, and explain why we prefer one over the other. Remember that there is no one-size-fits-all solution; the most important thing is to weigh the benefits and drawbacks of various solutions while keeping system limits in mind. How should we split our data to distribute it across various databases, given that we will be storing a large amount of data? Should we aim to keep all of a user's data in the same database? What kind of problem may it cause? How will we deal with popular users who send out a lot of tweets or have a lot of followers? Should we aim to store our data in such a way that it is optimized for scanning the most current (and relevant) tweets, as users' timelines will contain the most recent (and relevant) tweets? To speed things up, how much and at which layer should we introduce cache? Which components require improved load balancing? Step 7: Identifying bottlenecks and fixing them Try to cover as many bottlenecks as possible, as well as potential solutions. Does our system have any single points of failure? So, what are we going to do about it? Do we have enough data copies so we can continue serve our users if a few servers fail? Do we have enough copies of different services running such that a few failures do not result in the entire system being shut down? How do we keep track of our service's performance? Do we be notified if one of our important components fails or if its performance deteriorates? Conclusion In short, the keys to success in system design interviews are preparation and organization during the interview. The processes outlined above should help you stay on track and cover all of the major components of system design. Let's use the recommendations above to develop a few systems that are commonly asked for in SDIs. If you understand the concept and basic technologies mentioned in the previous section, you can skip Level 2 and go straight to Level 3 problem solution.","title":"System Design Interviews"},{"location":"SystemDesignInterviews/#a-step-by-step-guide-to-system-design-interviews","text":"Many software developers have difficulty with system design interviews (SDIs) for three reasons: SDIs are unstructured in nature, with applicants being asked to work on an open-ended design challenge with no standard solution. Candidates are lacking in intricate and large-scale system development experience. Candidates did not devote enough time on SDI preparation. Candidates who haven't put out a concerted effort to prepare for SDIs, like those who haven't put forth a concerted effort to prepare for coding interviews, typically perform poorly, particularly at top organizations such as Google, Facebook, Amazon, Microsoft, and others. Candidates who do not perform above average at these companies have a slim probability of being hired. A good performance, on the other hand, invariably leads to a better offer (a higher job and compensation), as it demonstrates the candidate's capacity to manage a complicated system. This article will take a step-by-step approach to solving numerous design issues. Let's begin with the steps below:","title":"A Step-by-Step Guide to System Design Interviews"},{"location":"SystemDesignInterviews/#step-1-clarification-of-requirements","text":"It's always a good idea to ask about the scope of the problem we're attempting to solve. Because design questions are typically open-ended and don't have a single proper solution, it's vital to clear up any misunderstandings early in the interview. Candidates that take the effort to establish the system's end goals have a better chance of succeeding in the interview. We should also define which aspects of the system we will be working on given we only have 35-40 minutes to build a (apparently) vast system. Let's have a look at a real-world example of how to construct a Twitter-like service. Before moving on to the next steps, you should answer the following questions about Twitter design: Will our service's users be able to publish tweets and follow others? Should we also design for the user's timeline to be created and displayed? Will images and videos be included in tweets? Are we concentrating just on the backend or are we also working on the front-end? Will users be able to use Twitter to search for tweets? Is it necessary to show hot subjects that are currently trending? Will new (or important) tweets be notified via push notification? All of these considerations will influence the final design.","title":"Step 1: Clarification of requirements"},{"location":"SystemDesignInterviews/#step-2-estimation-from-the-back-of-the-envelope","text":"Estimating the scope of the system we're going to design is always a smart idea. This will also come in handy later when we're dealing with scaling, partitioning, load balancing, and caching. On what scale should the system operate (e.g., number of new tweets, tweet views, timeline generation per second, etc.)? What kind of storage will we require? If users may include photographs and videos in their tweets, we will have different storage needs. What kind of network bandwidth utilization do we anticipate? This will be critical in determining how we will manage traffic and load balance among servers.","title":"Step 2: Estimation from the back of the envelope"},{"location":"SystemDesignInterviews/#step-3-define-the-system-interface","text":"Define which APIs the system should provide. This will not only define the exact contract that the system should deliver, but it will also guarantee that we haven't missed any criteria. APIs for our Twitter-like service will include the following: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026)","title":"Step 3: Define the system interface"},{"location":"SystemDesignInterviews/#step-4-defining-the-data-model","text":"Defining the data model early in the interview will help to define how data will move between the system's various components. It will later provide instructions for data partitioning and administration. The candidate must be able to recognize various system entities, how they will interact with one another, and various areas of data management such as storage, transportation, encryption, and so on. For our Twitter-like service, here are some entities: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Should we use a relational database system or a relational database system? Will a NoSQL database, such as Cassandra, be the greatest fit for our purposes, or should we go with a MySQL-like solution? To store photographs and videos, what type of block storage should we use?","title":"Step 4: Defining the data model"},{"location":"SystemDesignInterviews/#step-5-high-level-design","text":"Create a block diagram with 5-6 boxes to illustrate the system's essential components. We should determine how many components are required to tackle the problem from beginning to conclusion. At a high level, numerous application servers will be required to service all read/write requests for Twitter, with load balancers in front of them to distribute traffic. If we anticipate a high volume of read traffic (in comparison to write traffic), we can set up separate servers to handle these circumstances. We'll need an efficient database on the backend to hold all of the tweets and support a large number of reads. For storing photographs and movies, we'll also need a distributed file storage system.","title":"Step 5: High-level design"},{"location":"SystemDesignInterviews/#step-6-detailed-design","text":"Examine two or three important components in further depth; the interviewer's input should always point us in the direction of which portions of the system require more study. We should be able to demonstrate many approaches, their benefits and drawbacks, and explain why we prefer one over the other. Remember that there is no one-size-fits-all solution; the most important thing is to weigh the benefits and drawbacks of various solutions while keeping system limits in mind. How should we split our data to distribute it across various databases, given that we will be storing a large amount of data? Should we aim to keep all of a user's data in the same database? What kind of problem may it cause? How will we deal with popular users who send out a lot of tweets or have a lot of followers? Should we aim to store our data in such a way that it is optimized for scanning the most current (and relevant) tweets, as users' timelines will contain the most recent (and relevant) tweets? To speed things up, how much and at which layer should we introduce cache? Which components require improved load balancing?","title":"Step 6: Detailed design"},{"location":"SystemDesignInterviews/#step-7-identifying-bottlenecks-and-fixing-them","text":"Try to cover as many bottlenecks as possible, as well as potential solutions. Does our system have any single points of failure? So, what are we going to do about it? Do we have enough data copies so we can continue serve our users if a few servers fail? Do we have enough copies of different services running such that a few failures do not result in the entire system being shut down? How do we keep track of our service's performance? Do we be notified if one of our important components fails or if its performance deteriorates?","title":"Step 7: Identifying bottlenecks and fixing them"},{"location":"SystemDesignInterviews/#conclusion","text":"In short, the keys to success in system design interviews are preparation and organization during the interview. The processes outlined above should help you stay on track and cover all of the major components of system design. Let's use the recommendations above to develop a few systems that are commonly asked for in SDIs. If you understand the concept and basic technologies mentioned in the previous section, you can skip Level 2 and go straight to Level 3 problem solution.","title":"Conclusion"},{"location":"about/","text":"About This document is dedicated to system design interview preparation. A systematic approach to system design is referred to as systems design. It may take a bottom-up or top-down approach, but the process is systematic in that it considers all related variables of the system that needs to be created\u2014from the architecture to the required hardware and software, all the way down to the data and how it travels and transforms throughout its journey through the system. The terms systems design, systems analysis, systems engineering, and systems architecture are all used interchangeably. Engineers were striving to address complicated control and communications difficulties just before World War II when the systems design method first surfaced. They needed to be able to formalize their work and use suitable methodologies, especially in new domains like information theory, operations research, and computer science in general. copyright2022@jayaemekar","title":"About"},{"location":"about/#about","text":"This document is dedicated to system design interview preparation. A systematic approach to system design is referred to as systems design. It may take a bottom-up or top-down approach, but the process is systematic in that it considers all related variables of the system that needs to be created\u2014from the architecture to the required hardware and software, all the way down to the data and how it travels and transforms throughout its journey through the system. The terms systems design, systems analysis, systems engineering, and systems architecture are all used interchangeably. Engineers were striving to address complicated control and communications difficulties just before World War II when the systems design method first surfaced. They needed to be able to formalize their work and use suitable methodologies, especially in new domains like information theory, operations research, and computer science in general. copyright2022@jayaemekar","title":"About"}]}