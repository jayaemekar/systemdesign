{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is System Design? The process of defining the architecture, interfaces, and data for a system to meet specified criteria is referred to as system design. When constructing a huge system, there are a few considerations to keep in mind: What are the many architectural components that can be utilized? What is the relationship between these components? What are the best tradeoffs to make in order to get the most out of these components? The issue of system design is wide. There are numerous materials on system design principles dispersed over the internet. Many tech organizations require system design as part of the technical interview process in addition to coding interviews. Building and engineering systems demand a systematic approach to system design. A smart system design considers everything in an infrastructure, from hardware and software to data and how it's kept.","title":"Home"},{"location":"#what-is-system-design","text":"The process of defining the architecture, interfaces, and data for a system to meet specified criteria is referred to as system design. When constructing a huge system, there are a few considerations to keep in mind: What are the many architectural components that can be utilized? What is the relationship between these components? What are the best tradeoffs to make in order to get the most out of these components? The issue of system design is wide. There are numerous materials on system design principles dispersed over the internet. Many tech organizations require system design as part of the technical interview process in addition to coding interviews. Building and engineering systems demand a systematic approach to system design. A smart system design considers everything in an infrastructure, from hardware and software to data and how it's kept.","title":"What is System Design?"},{"location":"BasicTerminologies/","text":"Terminologies Fundamentals We've attempted to explain some of the terms in plain English. For a more formal definition, go to wiki. Reliability The probability of a system failing in a given period is known as reliability. A distributed system is considered reliable if it continues to provide services despite the failure of one or more of its software or hardware components. Replication Replication is the process of copying data across numerous devices on a regular basis. Multiple copies of the data exist among devices after replication. This could be useful if one or more of the machines dies due to a malfunction. Consistency Assuming you have a storage system with multiple machines, consistency means the data is consistent throughout the cluster, so you can read or write to/from any node and get the same results. Eventual consistency : Consistency in the long run Exactly as the name implies. If numerous machines in a cluster store the same data, an eventual consistent model implies that all machines will eventually have the same data. It's possible that those machines have various copies of the same data at any given time (temporarily inconsistent), but they'll eventually all have the same data. Availability It's a basic metric that shows how long a system, service, or machine has been operational under typical conditions. Availability in the context of a database cluster refers to the ability to always respond to queries (read or write) regardless of node failure. The Difference Between Reliability and Availability A system is available if it is reliable. However, just because something is available does not mean it is trustworthy. Partition Tolerance In the context of a database cluster, Even if there is a \"partition\" (communications break) between two nodes in a database cluster, the cluster continues to function (both nodes are up, but can't communicate). Scaling Scalability refers to a system's, process's, or network's ability to grow and manage rising demand. Horizontal and vertical scaling are the two types of scaling. To put it another way, scaling horizontally means adding more servers. To scale vertically, the server's resources must be increased ( RAM, CPU, storage, etc. ). Sharding Sharding is a data distribution strategy that uses many machines to distribute data. Data does not fit on a single machine in most large systems. In these circumstances, sharding refers to the division of a huge database into smaller, quicker, and more manageable data shards.","title":"Basic Terminologies"},{"location":"BasicTerminologies/#terminologies-fundamentals","text":"We've attempted to explain some of the terms in plain English. For a more formal definition, go to wiki.","title":"Terminologies Fundamentals"},{"location":"BasicTerminologies/#reliability","text":"The probability of a system failing in a given period is known as reliability. A distributed system is considered reliable if it continues to provide services despite the failure of one or more of its software or hardware components.","title":"Reliability"},{"location":"BasicTerminologies/#replication","text":"Replication is the process of copying data across numerous devices on a regular basis. Multiple copies of the data exist among devices after replication. This could be useful if one or more of the machines dies due to a malfunction.","title":"Replication"},{"location":"BasicTerminologies/#consistency","text":"Assuming you have a storage system with multiple machines, consistency means the data is consistent throughout the cluster, so you can read or write to/from any node and get the same results. Eventual consistency : Consistency in the long run Exactly as the name implies. If numerous machines in a cluster store the same data, an eventual consistent model implies that all machines will eventually have the same data. It's possible that those machines have various copies of the same data at any given time (temporarily inconsistent), but they'll eventually all have the same data.","title":"Consistency"},{"location":"BasicTerminologies/#availability","text":"It's a basic metric that shows how long a system, service, or machine has been operational under typical conditions. Availability in the context of a database cluster refers to the ability to always respond to queries (read or write) regardless of node failure. The Difference Between Reliability and Availability A system is available if it is reliable. However, just because something is available does not mean it is trustworthy.","title":"Availability"},{"location":"BasicTerminologies/#partition-tolerance","text":"In the context of a database cluster, Even if there is a \"partition\" (communications break) between two nodes in a database cluster, the cluster continues to function (both nodes are up, but can't communicate).","title":"Partition Tolerance"},{"location":"BasicTerminologies/#scaling","text":"Scalability refers to a system's, process's, or network's ability to grow and manage rising demand. Horizontal and vertical scaling are the two types of scaling. To put it another way, scaling horizontally means adding more servers. To scale vertically, the server's resources must be increased ( RAM, CPU, storage, etc. ).","title":"Scaling"},{"location":"BasicTerminologies/#sharding","text":"Sharding is a data distribution strategy that uses many machines to distribute data. Data does not fit on a single machine in most large systems. In these circumstances, sharding refers to the division of a huge database into smaller, quicker, and more manageable data shards.","title":"Sharding"},{"location":"CAPTheorem/","text":"CAP Theorem According to the CAP theorem, a distributed software system cannot simultaneously provide more than two of the three guarantees (CAP): consistency, availability, and partition tolerance. Trading off among CAP is almost the first thing we want to think about when designing a distributed system. According to the CAP theorem, we can only choose two of the following three possibilities when constructing a distributed system: Consistency: At the same time, all nodes see the same data. Before enabling more reads, some nodes are updated to ensure consistency. Availability: Every request receives a success/failure response. Data is replicated across multiple servers to ensure availability. Partition tolerance: Despite communication loss or partial failure, the system continues to function. A partition-tolerant system may withstand any level of network failure without causing the entire network to fail. Data is appropriately replicated across a variety of nodes and networks to ensure that the system remains operational during outages. We won't be able to create a general data store that is always available, sequentially consistent, and resilient to partition faults. Only two of these three traits can be combined in a system. Because all nodes should see the same set of changes in the same order in order to remain consistent. If the network is partitioned, however, updates in one partition may not reach the other partitions before a client reads from the out-of-date partition after reading from the current one. The only way to deal with this issue is to stop servicing requests from the out-of-date partition, however this reduces the service's availability to zero.","title":"CAP Theorem"},{"location":"CAPTheorem/#cap-theorem","text":"According to the CAP theorem, a distributed software system cannot simultaneously provide more than two of the three guarantees (CAP): consistency, availability, and partition tolerance. Trading off among CAP is almost the first thing we want to think about when designing a distributed system. According to the CAP theorem, we can only choose two of the following three possibilities when constructing a distributed system: Consistency: At the same time, all nodes see the same data. Before enabling more reads, some nodes are updated to ensure consistency. Availability: Every request receives a success/failure response. Data is replicated across multiple servers to ensure availability. Partition tolerance: Despite communication loss or partial failure, the system continues to function. A partition-tolerant system may withstand any level of network failure without causing the entire network to fail. Data is appropriately replicated across a variety of nodes and networks to ensure that the system remains operational during outages. We won't be able to create a general data store that is always available, sequentially consistent, and resilient to partition faults. Only two of these three traits can be combined in a system. Because all nodes should see the same set of changes in the same order in order to remain consistent. If the network is partitioned, however, updates in one partition may not reach the other partitions before a client reads from the out-of-date partition after reading from the current one. The only way to deal with this issue is to stop servicing requests from the out-of-date partition, however this reduces the service's availability to zero.","title":"CAP Theorem"},{"location":"Caching/","text":"Caching What is Cache and how does it work? A cache is a type of short-term memory with a finite quantity of storage space. It is often quicker than the original data source. Caching is made up of several steps. Perform a preliminary calculation (e.g. the number of visits from each referring domain for the previous day) Creating pricey indexes in advance (for example, suggested stories based on a user's click history) Using a quicker backend (e.g. Memcache instead of PostgreSQL) to store copies of frequently accessed data. Caches in several tiers 1 Client-side Use case: Improve the speed with which web content is retrieved from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Improve the speed with which web material is retrieved from web/app servers. Web Session Management (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Improve the performance of your applications and data access. Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: It essentially stores a cache on the Application server. If local, cached data exists, the node will promptly return it when a request is made to the service. If not, the data will be queried by the requesting node by accessing network storage, such as a database. When the application server is expanded to a large number of nodes, the following concerns may arise: The load balancer distributes requests across the nodes at random. The same request can be sent to many nodes, resulting in cache misses. Additional storage is required since the same data will be stored in two or more nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Decrease the latency of database query requests Tech: Database buffers, Key/Value data stores Solutions: The database's default setup normally contains some level of caching, which is tuned for a generic use case. These variables can be tweaked for unique usage patterns, and Redis and Memcached can also be used to improve performance. 6 Content Distribution Network (CDN) Use case: Remove the burden of providing static media from your application servers by distributing it geographically. Solutions: Amazon CloudFront, Akamai Note: If your system isn't big enough for CDN, you can build it this way: Use a lightweight HTTP server to serve static material from a separate subdomain (e.g. Nginx, Apache). Switch this subdomain's DNS to a CDN layer. 7 Other Cache CPU Cache: Small memories on or near the CPU can operate at a faster pace than the main memory, which is much larger. Since the 1980s, most CPUs have used one or more caches, sometimes in a cascaded configuration; modern high-end embedded, desktop, and server microprocessors may have as many as six cache types (between levels and functions) Cache for the GPU Disk Cache: While CPU caches are often maintained fully by hardware, other caches are managed by a variety of software. The operating system kernel manages the page cache in main memory, which is an example of disk cache. Cache Invalidation If data is changed in the database, it should be invalidated in the cache; otherwise, the program may behave inconsistently. Write-through cache, Write-around cache, and Write-back cache are the three main types of caching systems. Cache invalidation is a technique for resolving this issue; there are three main schemes: Write-through cache: With this policy, data is simultaneously written to the cache and the appropriate database. Because the identical data is written in the permanent storage, we will have total data consistency between the cache and the store. This technique also assures that nothing is lost in the event of a breakdown, power outage, or other system failure. Although write through reduces the possibility of data loss, this technique has the disadvantage of increasing write latency because every write operation must be performed twice before providing success to the client. Write-around cache: This technique is similar to write through cache, except that data is written directly to permanent storage rather than traveling through the cache. This can help prevent the cache from being flooded with write operations that will not be re-read, but it also means that a read request for recently written data will result in a \"cache miss,\" requiring the data to be read from slower back-end storage and resulting in increased latency. Write-back cache: With this policy, data is written to the cache alone, and the client is notified as soon as it is complete. The persistent storage is written to at predetermined intervals or under specific conditions. For write-intensive applications, this results in low latency and high throughput; but, because the sole copy of the written data is in the cache, this speed comes with the danger of data loss in the event of a crash or other adverse event. Cache eviction policies Some of the most typical cache eviction policies are as follows: First In First Out (FIFO): The cache evicts the first block accessed first, regardless of how often or how many times it was previously accessed. Last In First Out (LIFO): The cache evicts the block that has been accessed the most recently first, regardless of how often or how many times it has been accessed previously. Least Recently Used (LRU): Tosses out the items that haven't been utilized in a long time first. Most Recently Used (MRU): Discards the most recently used objects first, as opposed to LRU. Least Frequently Used (LFU): Determines how frequently an item is required. The ones that are utilized the least are the first to be discarded. Random Replacement (RR): Selects a candidate item at random and discards it when space is needed. The following links include useful information regarding caching: Cache Introduction to architecting systems","title":"Caching"},{"location":"Caching/#caching","text":"","title":"Caching"},{"location":"Caching/#what-is-cache-and-how-does-it-work","text":"A cache is a type of short-term memory with a finite quantity of storage space. It is often quicker than the original data source. Caching is made up of several steps. Perform a preliminary calculation (e.g. the number of visits from each referring domain for the previous day) Creating pricey indexes in advance (for example, suggested stories based on a user's click history) Using a quicker backend (e.g. Memcache instead of PostgreSQL) to store copies of frequently accessed data.","title":"What is Cache and how does it work?"},{"location":"Caching/#caches-in-several-tiers","text":"1 Client-side Use case: Improve the speed with which web content is retrieved from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Improve the speed with which web material is retrieved from web/app servers. Web Session Management (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Improve the performance of your applications and data access. Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: It essentially stores a cache on the Application server. If local, cached data exists, the node will promptly return it when a request is made to the service. If not, the data will be queried by the requesting node by accessing network storage, such as a database. When the application server is expanded to a large number of nodes, the following concerns may arise: The load balancer distributes requests across the nodes at random. The same request can be sent to many nodes, resulting in cache misses. Additional storage is required since the same data will be stored in two or more nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Decrease the latency of database query requests Tech: Database buffers, Key/Value data stores Solutions: The database's default setup normally contains some level of caching, which is tuned for a generic use case. These variables can be tweaked for unique usage patterns, and Redis and Memcached can also be used to improve performance. 6 Content Distribution Network (CDN) Use case: Remove the burden of providing static media from your application servers by distributing it geographically. Solutions: Amazon CloudFront, Akamai Note: If your system isn't big enough for CDN, you can build it this way: Use a lightweight HTTP server to serve static material from a separate subdomain (e.g. Nginx, Apache). Switch this subdomain's DNS to a CDN layer. 7 Other Cache CPU Cache: Small memories on or near the CPU can operate at a faster pace than the main memory, which is much larger. Since the 1980s, most CPUs have used one or more caches, sometimes in a cascaded configuration; modern high-end embedded, desktop, and server microprocessors may have as many as six cache types (between levels and functions) Cache for the GPU Disk Cache: While CPU caches are often maintained fully by hardware, other caches are managed by a variety of software. The operating system kernel manages the page cache in main memory, which is an example of disk cache.","title":"Caches in several tiers"},{"location":"Caching/#cache-invalidation","text":"If data is changed in the database, it should be invalidated in the cache; otherwise, the program may behave inconsistently. Write-through cache, Write-around cache, and Write-back cache are the three main types of caching systems. Cache invalidation is a technique for resolving this issue; there are three main schemes: Write-through cache: With this policy, data is simultaneously written to the cache and the appropriate database. Because the identical data is written in the permanent storage, we will have total data consistency between the cache and the store. This technique also assures that nothing is lost in the event of a breakdown, power outage, or other system failure. Although write through reduces the possibility of data loss, this technique has the disadvantage of increasing write latency because every write operation must be performed twice before providing success to the client. Write-around cache: This technique is similar to write through cache, except that data is written directly to permanent storage rather than traveling through the cache. This can help prevent the cache from being flooded with write operations that will not be re-read, but it also means that a read request for recently written data will result in a \"cache miss,\" requiring the data to be read from slower back-end storage and resulting in increased latency. Write-back cache: With this policy, data is written to the cache alone, and the client is notified as soon as it is complete. The persistent storage is written to at predetermined intervals or under specific conditions. For write-intensive applications, this results in low latency and high throughput; but, because the sole copy of the written data is in the cache, this speed comes with the danger of data loss in the event of a crash or other adverse event.","title":"Cache Invalidation"},{"location":"Caching/#cache-eviction-policies","text":"Some of the most typical cache eviction policies are as follows: First In First Out (FIFO): The cache evicts the first block accessed first, regardless of how often or how many times it was previously accessed. Last In First Out (LIFO): The cache evicts the block that has been accessed the most recently first, regardless of how often or how many times it has been accessed previously. Least Recently Used (LRU): Tosses out the items that haven't been utilized in a long time first. Most Recently Used (MRU): Discards the most recently used objects first, as opposed to LRU. Least Frequently Used (LFU): Determines how frequently an item is required. The ones that are utilized the least are the first to be discarded. Random Replacement (RR): Selects a candidate item at random and discards it when space is needed. The following links include useful information regarding caching: Cache Introduction to architecting systems","title":"Cache eviction policies"},{"location":"ConsistentHashing/","text":"Consistent Hashing One of the most important components in distributed scalable systems is the Distributed Hash Table (DHT). Hash tables require a key, a value, and a hash function, which maps the key to a storage place for the value. index = hash_function(key) Let's pretend we're working on a distributed caching system. An sensible hash function for 'n' cache servers would be 'key percent n'. It's straightforward and widely utilized. However, it has two fundamental flaws: It is not scalable horizontally. All current mappings are broken whenever a new cache host is introduced to the system. If the caching system has a lot of data, it will be difficult to maintain. In practice, scheduling a downtime to update all caching mappings becomes problematic. It may not be load balanced, especially if the data is not spread consistently. In practice, it is reasonable to expect that the data will not be dispersed evenly. For the caching system, this means that some caches will become hot and saturated while others will remain inactive and nearly empty. Consistent hashing is an excellent method to improve the caching system in these scenarios. What is Consistent Hashing, and how does it work? For distributed caching systems and DHTs, consistent hashing is a particularly beneficial method. It allows us to distribute data across a cluster in such a way that nodes are added or withdrawn with minimal rearrangement. As a result, scaling up or down the caching system will be much easier. Only 'k/n' keys need to be remapped in Consistent Hashing when the hash table is resized (e.g. a new cache host is added to the system), where 'k' is the total number of keys and 'n' is the total number of servers. Remember that in a caching system that uses the hash function'mod,' all keys must be remapped. If possible, items in Consistent Hashing are mapped to the same host. When a host is removed from the system, its objects are shared by other hosts; when a new host is introduced, it takes its share from a few hosts without affecting the shares of others. How does it function? Consistent hashing is a hash function that maps a key to an integer. Assume the hash function's output is in the range [0, 256]. Consider placing the integers in the range on a ring and wrapping the values around it. The following is an example of how consistent hashing works: Hash a list of cache servers into integers in the range given. To associate a key with a server, a. Convert it to a single integer hash. b. Work your way around the ring clockwise until you reach the first cache. c. That cache is the one that contains the key. As an example, consider the following: Key1 corresponds to cache A, while key2 corresponds to cache C. When a new server, say D, is added, keys that were previously stored on C will be divided. Some will be transferred to D, while others will remain unchanged. To remove a cache or if a cache fails, say A, all keys mapped to A will fall into B, and only those keys will need to be relocated to B; other keys will not be affected. As we noted in the outset, real data for load balancing is fundamentally randomly distributed and hence may not be uniform. It's possible that the keys on caches will become imbalanced as a result. We add \"virtual replicas\" for caches to address this issue. We map each cache to many points on the ring, i.e. replicas, rather than a single point on the ring. As a result, each cache is linked to numerous ring segments. If the hash function \"mixes well,\" the keys will become more balanced as the number of replicates grows.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#consistent-hashing","text":"One of the most important components in distributed scalable systems is the Distributed Hash Table (DHT). Hash tables require a key, a value, and a hash function, which maps the key to a storage place for the value. index = hash_function(key) Let's pretend we're working on a distributed caching system. An sensible hash function for 'n' cache servers would be 'key percent n'. It's straightforward and widely utilized. However, it has two fundamental flaws: It is not scalable horizontally. All current mappings are broken whenever a new cache host is introduced to the system. If the caching system has a lot of data, it will be difficult to maintain. In practice, scheduling a downtime to update all caching mappings becomes problematic. It may not be load balanced, especially if the data is not spread consistently. In practice, it is reasonable to expect that the data will not be dispersed evenly. For the caching system, this means that some caches will become hot and saturated while others will remain inactive and nearly empty. Consistent hashing is an excellent method to improve the caching system in these scenarios.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#what-is-consistent-hashing-and-how-does-it-work","text":"For distributed caching systems and DHTs, consistent hashing is a particularly beneficial method. It allows us to distribute data across a cluster in such a way that nodes are added or withdrawn with minimal rearrangement. As a result, scaling up or down the caching system will be much easier. Only 'k/n' keys need to be remapped in Consistent Hashing when the hash table is resized (e.g. a new cache host is added to the system), where 'k' is the total number of keys and 'n' is the total number of servers. Remember that in a caching system that uses the hash function'mod,' all keys must be remapped. If possible, items in Consistent Hashing are mapped to the same host. When a host is removed from the system, its objects are shared by other hosts; when a new host is introduced, it takes its share from a few hosts without affecting the shares of others.","title":"What is Consistent Hashing, and how does it work?"},{"location":"ConsistentHashing/#how-does-it-function","text":"Consistent hashing is a hash function that maps a key to an integer. Assume the hash function's output is in the range [0, 256]. Consider placing the integers in the range on a ring and wrapping the values around it. The following is an example of how consistent hashing works: Hash a list of cache servers into integers in the range given. To associate a key with a server, a. Convert it to a single integer hash. b. Work your way around the ring clockwise until you reach the first cache. c. That cache is the one that contains the key. As an example, consider the following: Key1 corresponds to cache A, while key2 corresponds to cache C. When a new server, say D, is added, keys that were previously stored on C will be divided. Some will be transferred to D, while others will remain unchanged. To remove a cache or if a cache fails, say A, all keys mapped to A will fall into B, and only those keys will need to be relocated to B; other keys will not be affected. As we noted in the outset, real data for load balancing is fundamentally randomly distributed and hence may not be uniform. It's possible that the keys on caches will become imbalanced as a result. We add \"virtual replicas\" for caches to address this issue. We map each cache to many points on the ring, i.e. replicas, rather than a single point on the ring. As a result, each cache is linked to numerous ring segments. If the hash function \"mixes well,\" the keys will become more balanced as the number of replicates grows.","title":"How does it function?"},{"location":"DataPartitioning%20copy/","text":"Data Partitioning Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers. Partitioning Methods There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications. a. Horizontal partitioning: In this scheme, we put different rows into different tables. For example, if we are storing different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table. This is also called a range based partitioning as we are storing different ranges of data in separate tables. Horizontal partitioning is also called as Data Sharding. The key problem with this approach is that if the value whose range is used for partitioning isn\u2019t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting location based on their zip codes assumes that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities. b. Vertical Partitioning: In this scheme, we divide our data to store tables related to a specific feature in their own server. For example, if we are building Instagram like application - where we need to store data related to users, photos they upload, and people they follow - we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server. Vertical partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a feature specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users). c. Directory Based Partitioning: A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application. Partitioning Criteria a. Key or Hash-based partitioning: Under this scheme, we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. For example, if we have 100 DB servers and our ID is a numeric value that gets incremented by one each time a new record is inserted. In this example, the hash function could be \u2018ID % 100\u2019, which will give us the server number where we can store/read that record. This approach should ensure a uniform allocation of data among servers. The fundamental problem with this approach is that it effectively fixes the total number of DB servers, since adding new servers means changing the hash function which would require redistribution of data and downtime for the service. A workaround for this problem is to use Consistent Hashing. b. List partitioning: In this scheme, each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries. c. Round-robin partitioning: This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n). d. Composite partitioning: Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed. Common Problems of Data Partitioning On a partitioned database, there are certain extra constraints on the different operations that can be performed. Most of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server. Below are some of the constraints and additional complexities introduced by partitioning: a. Joins and Denormalization: Performing joins on a database which is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with all the perils of denormalization such as data inconsistency. b. Referential integrity: As we saw that performing a cross-partition query on a partitioned database is not feasible, similarly, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult. Most of RDBMS do not support foreign keys constraints across databases on different database servers. Which means that applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references. c. Rebalancing: There could be many reasons we have to change our partitioning scheme: The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition. There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos. In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).","title":"Data Partitioning"},{"location":"DataPartitioning%20copy/#data-partitioning","text":"Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.","title":"Data Partitioning"},{"location":"DataPartitioning%20copy/#partitioning-methods","text":"There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications.","title":"Partitioning Methods"},{"location":"DataPartitioning%20copy/#a-horizontal-partitioning","text":"In this scheme, we put different rows into different tables. For example, if we are storing different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table. This is also called a range based partitioning as we are storing different ranges of data in separate tables. Horizontal partitioning is also called as Data Sharding. The key problem with this approach is that if the value whose range is used for partitioning isn\u2019t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting location based on their zip codes assumes that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities.","title":"a. Horizontal partitioning:"},{"location":"DataPartitioning%20copy/#b-vertical-partitioning","text":"In this scheme, we divide our data to store tables related to a specific feature in their own server. For example, if we are building Instagram like application - where we need to store data related to users, photos they upload, and people they follow - we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server. Vertical partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a feature specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users).","title":"b. Vertical Partitioning:"},{"location":"DataPartitioning%20copy/#c-directory-based-partitioning","text":"A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application.","title":"c. Directory Based Partitioning:"},{"location":"DataPartitioning%20copy/#partitioning-criteria","text":"","title":"Partitioning Criteria"},{"location":"DataPartitioning%20copy/#a-key-or-hash-based-partitioning","text":"Under this scheme, we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. For example, if we have 100 DB servers and our ID is a numeric value that gets incremented by one each time a new record is inserted. In this example, the hash function could be \u2018ID % 100\u2019, which will give us the server number where we can store/read that record. This approach should ensure a uniform allocation of data among servers. The fundamental problem with this approach is that it effectively fixes the total number of DB servers, since adding new servers means changing the hash function which would require redistribution of data and downtime for the service. A workaround for this problem is to use Consistent Hashing.","title":"a. Key or Hash-based partitioning:"},{"location":"DataPartitioning%20copy/#b-list-partitioning","text":"In this scheme, each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries.","title":"b. List partitioning:"},{"location":"DataPartitioning%20copy/#c-round-robin-partitioning","text":"This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n).","title":"c. Round-robin partitioning:"},{"location":"DataPartitioning%20copy/#d-composite-partitioning","text":"Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed.","title":"d. Composite partitioning:"},{"location":"DataPartitioning%20copy/#common-problems-of-data-partitioning","text":"On a partitioned database, there are certain extra constraints on the different operations that can be performed. Most of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server. Below are some of the constraints and additional complexities introduced by partitioning:","title":"Common Problems of Data Partitioning"},{"location":"DataPartitioning%20copy/#a-joins-and-denormalization","text":"Performing joins on a database which is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with all the perils of denormalization such as data inconsistency.","title":"a. Joins and Denormalization:"},{"location":"DataPartitioning%20copy/#b-referential-integrity","text":"As we saw that performing a cross-partition query on a partitioned database is not feasible, similarly, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult. Most of RDBMS do not support foreign keys constraints across databases on different database servers. Which means that applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references.","title":"b. Referential integrity:"},{"location":"DataPartitioning%20copy/#c-rebalancing","text":"There could be many reasons we have to change our partitioning scheme: The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition. There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos. In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).","title":"c. Rebalancing:"},{"location":"DataPartitioning/","text":"Data Partitioning Data partitioning is a method of dividing a large database (DB) into multiple smaller sections. It is the process of dividing a database/table among numerous machines in order to increase an application's manageability, performance, availability, and load balancing. The reason for data partitioning is that after a certain scalability point, scaling horizontally by adding more machines is cheaper and more viable than scaling vertically by adding beefier servers. Partitioning Methods There are a variety of approaches that can be used to divide an application database into many smaller databases. Three of the most common methods employed by various large-scale applications are listed below. Horizontal partitioning is a technique for dividing a space horizontally (often called sharding). Each partition is a separate data store in this strategy, although they all share the same schema. A shard is a partition that holds a specific subset of data, for as all orders for a certain set of clients. Vertical Partitioning is a term that refers to the division of space vertically. Each partition in this strategy stores a subset of the fields for objects in the data store. The fields are separated into groups based on how they are used. For example, frequently accessible fields could be grouped together in one vertical division while less frequently visited fields are grouped together in another. Functional partitioning Data is aggregated in this technique based on how each bounded context in the system uses it. An e-commerce system, for example, might keep invoice data in one partition and product inventory data in the other. Directory Based Partitioning Creating a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code is a loosely connected solution to work around concerns highlighted in the above schemes. To determine the location of a specific data entity, we query the directory server that stores the mapping between each tuple key and its DB server. Because of this loose coupling, we can conduct operations such as adding servers to the DB pool or modifying our partitioning scheme without affecting the application. Partitioning Criteria a. Key or Hash-based partitioning: We use a hash function on some key features of the item we're storing to generate a partition number. b. List partitioning: Each partition has a set of values allocated to it, so if we want to add a new record, we'll look to see which partition holds our key and save it there. For example, we could specify that all users from Iceland, Norway, Sweden, Finland, or Denmark should be stored in a Nordic division. c. Round-robin partitioning: This is a straightforward approach for ensuring data distribution consistency. The I tuple is assigned to partition with 'n' partitions (i mod n). d. Composite partitioning: We combine any of the aforementioned partitioning schemes to create a new scheme in this scheme. For instance, apply a list partitioning strategy first, then a hash-based partitioning method. Consistent hashing is a hybrid of hash and list partitioning, in which the hash shrinks the key space to a size that can be listed.","title":"Data Partitioning"},{"location":"DataPartitioning/#data-partitioning","text":"Data partitioning is a method of dividing a large database (DB) into multiple smaller sections. It is the process of dividing a database/table among numerous machines in order to increase an application's manageability, performance, availability, and load balancing. The reason for data partitioning is that after a certain scalability point, scaling horizontally by adding more machines is cheaper and more viable than scaling vertically by adding beefier servers.","title":"Data Partitioning"},{"location":"DataPartitioning/#partitioning-methods","text":"There are a variety of approaches that can be used to divide an application database into many smaller databases. Three of the most common methods employed by various large-scale applications are listed below. Horizontal partitioning is a technique for dividing a space horizontally (often called sharding). Each partition is a separate data store in this strategy, although they all share the same schema. A shard is a partition that holds a specific subset of data, for as all orders for a certain set of clients. Vertical Partitioning is a term that refers to the division of space vertically. Each partition in this strategy stores a subset of the fields for objects in the data store. The fields are separated into groups based on how they are used. For example, frequently accessible fields could be grouped together in one vertical division while less frequently visited fields are grouped together in another. Functional partitioning Data is aggregated in this technique based on how each bounded context in the system uses it. An e-commerce system, for example, might keep invoice data in one partition and product inventory data in the other. Directory Based Partitioning Creating a lookup service that knows your current partitioning scheme and abstracts it away from the DB access code is a loosely connected solution to work around concerns highlighted in the above schemes. To determine the location of a specific data entity, we query the directory server that stores the mapping between each tuple key and its DB server. Because of this loose coupling, we can conduct operations such as adding servers to the DB pool or modifying our partitioning scheme without affecting the application.","title":"Partitioning Methods"},{"location":"DataPartitioning/#partitioning-criteria","text":"a. Key or Hash-based partitioning: We use a hash function on some key features of the item we're storing to generate a partition number. b. List partitioning: Each partition has a set of values allocated to it, so if we want to add a new record, we'll look to see which partition holds our key and save it there. For example, we could specify that all users from Iceland, Norway, Sweden, Finland, or Denmark should be stored in a Nordic division. c. Round-robin partitioning: This is a straightforward approach for ensuring data distribution consistency. The I tuple is assigned to partition with 'n' partitions (i mod n). d. Composite partitioning: We combine any of the aforementioned partitioning schemes to create a new scheme in this scheme. For instance, apply a list partitioning strategy first, then a hash-based partitioning method. Consistent hashing is a hybrid of hash and list partitioning, in which the hash shrinks the key space to a size that can be listed.","title":"Partitioning Criteria"},{"location":"DesignCache/","text":"Design a distributed key value caching system, like Memcached or Redis. Features: This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote. Q: What is the amount of data that we need to cache? A: Let's assume we are looking to cache on the scale of Google or Twitter. The total size of the cache would be a few TBs. Q: What should be the eviction strategy? A: It is possible that we might get entries when we would not have space to accommodate new entries. In such cases, we would need to remove one or more entries to make space for the new entry. Which entry should we evict? There are multiple strategies possible. Following are a few standard ones : FIFO ( First in, first out ) : The entry that was first added to the queue would be evicted/removed first. In other words, Elements are evicted in the same order as they come in. This is the simplest to implement and performs well when either, access pattern is completely random ( All elements are equally probably to be accessed ) OR if the use of an element makes it less likely to be used in the future. LRU ( Least Recently Used ) : Default. As the name suggests, the element evicted is the ones which has the oldest last access time. The last accessed timestamp is updated when an element is put into the cache or an element is retrieved from the cache with a get call. This is by far the most popular eviction strategy. LFU ( Least Frequently Used ) : Again, as the name suggests, every entry has a frequency associated with it. At the time of eviction, the entry with lowest frequency is evicted. This is most effective in cases when most of access is limited to a very small portion of the data ( pareto distribution ). As you can see, every eviction strategy has its own set of cases when they are most effective. The choice of which eviction strategy to choose is largely dependent on the expected access pattern. We will go with the default here which is LRU. Q. What should be the access pattern for the given cache? A: There are majorly three kinds of caching systems : Write through cache : This is a caching system where writes go through the cache and write is confirmed as success only if writes to DB and the cache BOTH succeed. This is really useful for applications which write and re-read the information quickly. However, write latency will be higher in this case as there are writes to 2 separate systems. Write around cache : This is a caching system where write directly goes to the DB. The cache system reads the information from DB incase of a miss. While this ensures lower write load to the cache and faster writes, this can lead to higher read latency incase of applications which write and re-read the information quickly. Write back cache : This is a caching system where the write is directly done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the DB. This would lead to a really quick write latency and high write throughput. But, as is the case with any non-persistent / in-memory write, we stand the risk of losing the data incase the caching layer dies. We can improve our odds by introducing having more than one replica acknowledging the write ( so that we don\u2019t lose data if just one of the replica dies ). Estimations: This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier. Q: What is the kind of QPS we expect for the system? A: This estimation is important to understand the number of machines we will need to answer the queries. For example, if our estimations state that a single machine is going to handle 1M QPS, we run into a high risk of high latency / the machine dying because of queries not being answered fast enough and hence ending up in the backlog queue. Again, let's assume the scale of Twitter / Google. We can expect around 10M QPS if not more. Q: What is the number of machines required to cache? A: A cache has to be inherently of low latency. Which means all cache data has to reside in main memory. A production level caching machine would be 72G or 144G of RAM. Assuming beefier cache machines, we have 72G of main memory for 1 machine. Min. number of machine required = 30 TB / 72G which is close to 420 machines. Do know that this is the absolute minimum. Its possible we might need more machines because the QPS per machine is higher than we want it to be. Design Goals: Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other. Q: Is Latency a very important metric for us? A: Yes. The whole point of caching is low latency. Q: Consistency vs Availability? A: Unavailability in a caching system means that the caching machine goes down, which in turn means that we have a cache miss which leads to a high latency. As said before, we are caching for a Twitter / Google like system. When fetching a timeline for a user, I would be okay if I miss on a few tweets which were very recently posted as long as I eventually see them in reasonable time. Unavailability could lead to latency spikes and increased load on DB. Choosing from consistency and availability, we should prioritize for availability. Skeleton of the design: The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction. Deep Dive: Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes). Q: How would a LRU cache work on a single machine which is single threaded? Q: What if we never had to remove entries from the LRU cache because we had enough space, what would you use to support and get and set? A: A simple map / hashmap would suffice. Q: How should we modify our approach if we also have to evict keys at some stage? A: We need a data structure which at any given instance can give me the least recently used objects in order. Let's see if we can maintain a linked list to do it. We try to keep the list ordered by the order in which they are used. So whenever, a get operation happens, we would need to move that object from a certain position in the list to the front of the list. Which means a delete followed by insert at the beginning. Insert at the beginning of the list is trivial. How do we achieve erase of the object from a random position in least time possible? How about we maintain another map which stores the value to the corresponding linked list node. Ok, now when we know the node, we would need to know its previous and next node in the list to enable the deletion of the node from the list. We can get the next in the list from next pointer ? What about the previous node ? To encounter that, we make the list doubly linked list. A: Since we only have one thread to work with, we cannot do things in parallel. So we will take a simple approach and implement a LRU cache using a linked list and a map. The Map stores the value to the corresponding linked list node and is useful to move the recently accessed node to the front of the list. Q: How would a LRU cache work on a single machine which is multi threaded ? Q: How would you break down cache write and read into multiple instructions? A: Read path : Read a value corresponding to a key. This requires : Operation 1 : A read from the HashMap and then, Operation 2 : An update in the doubly LinkedList Write path : Insert a new key-value entry to the LRU cache. This requires : If the cache is full, then Operation 3: Figure out the least recently used item from the linkedList Operation 4: Remove it from the hashMap Operation 5: Remove the entry from the linkedList. Operation 6: Insert the new item in the hashMap Operation 7: Insert the new item in the linkedList. Q: How would you prioritize above operations to keep latency to a minimum for our system? A: As is the case with most concurrent systems, writes compete with reads and other writes. That requires some form of locking when a write is in progress. We can choose to have writes as granular as possible to help with performance. Read path is going to be highly frequent. As latency is our design goal, Operation 1 needs to be really fast and should require minimum locks. Operation 2 can happen asynchronously. Similarly, all of the write path can happen asynchronously and the client\u2019s latency need not be affected by anything other than Operation 1. Let's dig deeper into Operation 1. What are the things that Hashmap is dealing with? Hashmap deals with Operation 1, 4 and 6 with Operation 4 and 6 being write operations. One simple, but not so efficient way of handling read/write would be to acquire a higher level Read lock for Operation 1 and Write lock for Operation 4 and 6. However, Operation 1 as stressed earlier is the most frequent ( by a huge margin ) operation and its performance is critical to how our caching system works. Q: How would you implement HashMap? A: The HashMap itself could be implemented in multiple ways. One common way could be hashing with linked list (colliding values linked together in a linkedList) : Let's say our hashmap size is N and we wish to add (k,v) to it Let H = size N array of pointers with every element initialized to NULL For a given key k, generate g = hash(k) % N newEntry = LinkedList Node with value = v newEntry.next = H[g] H[g] = newEntry More details at https://en.wikipedia.org/wiki/Hash_table Given this implementation, we can see that instead of having a lock on a hashmap level, we can have it for every single row. This way, a read for row i and a write for row j would not affect each other if i != j. Note that we would try to keep N as high as possible here to increase granularity.","title":"Design a distributed key value caching system, like Memcached or Redis."},{"location":"DesignCache/#design-a-distributed-key-value-caching-system-like-memcached-or-redis","text":"","title":"Design a distributed key value caching system, like Memcached or Redis."},{"location":"DesignCache/#features","text":"This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote.","title":"Features:"},{"location":"DesignCache/#estimations","text":"This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier.","title":"Estimations:"},{"location":"DesignCache/#design-goals","text":"Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other.","title":"Design Goals:"},{"location":"DesignCache/#skeleton-of-the-design","text":"The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction.","title":"Skeleton of the design:"},{"location":"DesignCache/#deep-dive","text":"Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes).","title":"Deep Dive:"},{"location":"DesignTicketmaster/","text":"Design Ticketmaster ( New ) Let's design an online ticketing system that sells movie tickets like Ticketmaster or BookMyShow. Similar Services: bookmyshow.com, ticketmaster.com Difficulty Level: Hard 1. What is an online movie ticket booking system? A movie ticket booking system provides its customers the ability to purchase theatre seats online. E-ticketing systems allow the customers to browse through movies currently being played and to book seats, anywhere anytime. 2. Requirements and Goals of the System Our ticket booking service should meet the following requirements: Functional Requirements: Our ticket booking service should be able to list different cities where its affiliate cinemas are located. Once the user selects the city, the service should display the movies released in that particular city. Once the user selects a movie, the service should display the cinemas running that movie and its available show times. The user should be able to choose a show at a particular cinema and book their tickets. The service should be able to show the user the seating arrangement of the cinema hall. The user should be able to select multiple seats according to their preference. The user should be able to distinguish available seats from booked ones. Users should be able to put a hold on the seats for five minutes before they make a payment to finalize the booking. The user should be able to wait if there is a chance that the seats might become available, e.g., when holds by other users expire. Waiting customers should be serviced in a fair, first come, first serve manner. Non-Functional Requirements: The system would need to be highly concurrent. There will be multiple booking requests for the same seat at any particular point in time. The service should handle this gracefully and fairly. The core thing of the service is ticket booking, which means financial transactions. This means that the system should be secure and the database ACID compliant. 3. Some Design Considerations For simplicity, let\u2019s assume our service does not require any user authentication. The system will not handle partial ticket orders. Either user gets all the tickets they want or they get nothing. Fairness is mandatory for the system. To stop system abuse, we can restrict users from booking more than ten seats at a time. We can assume that traffic would spike on popular/much-awaited movie releases and the seats would fill up pretty fast. The system should be scalable and highly available to keep up with the surge in traffic. 4. Capacity Estimation Traffic estimates: Let\u2019s assume that our service has 3 billion page views per month and sells 10 million tickets a month. Storage estimates: Let\u2019s assume that we have 500 cities and, on average each city has ten cinemas. If there are 2000 seats in each cinema and on average, there are two shows every day. Let\u2019s assume each seat booking needs 50 bytes (IDs, NumberOfSeats, ShowID, MovieID, SeatNumbers, SeatStatus, Timestamp, etc.) to store in the database. We would also need to store information about movies and cinemas; let\u2019s assume it\u2019ll take 50 bytes. So, to store all the data about all shows of all cinemas of all cities for a day: 500 cities * 10 cinemas * 2000 seats * 2 shows * (50+50) bytes = 2GB / day To store five years of this data, we would need around 3.6TB. 5. System APIs We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the APIs to search movie shows and reserve seats. SearchMovies(api_dev_key, keyword, city, lat_long, radius, start_datetime, end_datetime, postal_code, includeSpellcheck, results_per_page, sorting_order) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. keyword (string): Keyword to search on. city (string): City to filter movies by. lat_long (string): Latitude and longitude to filter by. radius (number): Radius of the area in which we want to search for events. start_datetime (string): Filter movies with a starting datetime. end_datetime (string): Filter movies with an ending datetime. postal_code (string): Filter movies by postal code / zipcode. includeSpellcheck (Enum: \u201cyes\u201d or \u201cno\u201d): Yes, to include spell check suggestions in the response. results_per_page (number): Number of results to return per page. Maximum is 30. sorting_order (string): Sorting order of the search result. Some allowable values : \u2018name,asc\u2019, \u2018name,desc\u2019, \u2018date,asc\u2019, \u2018date,desc\u2019, \u2018distance,asc\u2019, \u2018name,date,asc\u2019, \u2018name,date,desc\u2019, \u2018date,name,asc\u2019, \u2018date,name,desc\u2019. Returns: (JSON) Here is a sample list of movies and their shows: [ { \"MovieID\": 1, \"ShowID\": 1, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"14:00\", \"EndTime\": \"16:00\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Almost Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Available\" } ] }, { \"MovieID\": 1, \"ShowID\": 2, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"16:30\", \"EndTime\": \"18:30\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Almost Full\" } ] }, ] ReserveSeats(api_dev_key, session_id, movie_id, show_id, seats_to_reserve[]) Parameters: - api_dev_key (string): same as above - session_id (string): User\u2019s session ID to track this reservation. Once the reservation time expires, user\u2019s reservation on the server will be removed using this ID. - movie_id (string): Movie to reserve. - show_id (string): Show to reserve. - seats_to_reserve (number): An array containing seat IDs to reserve. Returns: (JSON) Returns the status of the reservation, which would be one of the following: 1) \u201cReservation Successful\u201d 2) \u201cReservation Failed - Show Full,\u201d 3) \u201cReservation Failed - Retry, as other users are holding reserved seats\u201d. 6. Database Design Here are a few observations about the data we are going to store: Each City can have multiple Cinemas. Each Cinema will have multiple halls. Each Movie will have many Shows and each Show will have multiple Bookings. A user can have multiple bookings. 7. High Level Design At a high-level, our web servers will manage users\u2019 sessions and application servers will handle all the ticket management, storing data in the databases as well as working with the cache servers to process reservations. 8. Detailed Component Design First, let\u2019s try to build our service assuming it is being served from a single server. Ticket Booking Workflow: The following would be a typical ticket booking workflow: The user searches for a movie. The user selects a movie. The user is shown the available shows of the movie. The user selects a show. The user selects the number of seats to be reserved. If the required number of seats are available, the user is shown a map of the theater to select seats. If not, the user is taken to \u2018step 8\u2019 below. Once the user selects the seat, the system will try to reserve those selected seats. If seats can\u2019t be reserved, we have the following options: Show is full; the user is shown the error message. The seats the user wants to reserve are no longer available, but there are other seats available, so the user is taken back to the theater map to choose different seats. There are no seats available to reserve, but all the seats are not booked yet, as there are some seats that other users are holding in the reservation pool and have not booked yet. The user will be taken to a waiting page where they can wait until the required seats get freed from the reservation pool. This waiting could result in the following options: If the required number of seats become available, the user is taken to the theater map page where they can choose seats. While waiting, if all seats get booked or there are fewer seats in the reservation pool than the user intend to book, the user is shown the error message. User cancels the waiting and is taken back to the movie search page. At maximum, a user can wait one hour, after that user\u2019s session gets expired and the user is taken back to the movie search page. If seats are reserved successfully, the user has five minutes to pay for the reservation. After payment, booking is marked complete. If the user is not able to pay within five minutes, all their reserved seats are freed to become available to other users. How would the server keep track of all the active reservation that haven\u2019t been booked yet? And how would the server keep track of all the waiting customers? We need two daemon services, one to keep track of all active reservations and remove any expired reservation from the system; let\u2019s call it ActiveReservationService. The other service would be keeping track of all the waiting user requests and, as soon as the required number of seats become available, it will notify the (the longest waiting) user to choose the seats; let\u2019s call it WaitingUserService. a. ActiveReservationsService We can keep all the reservations of a \u2018show\u2019 in memory in a data structure similar to Linked HashMap or a TreeMap in addition to keeping all the data in the database. We will need a linked HashMap kind of data structure that allows us to jump to any reservation to remove it when the booking is complete. Also, since we will have expiry time associated with each reservation, the head of the HashMap will always point to the oldest reservation record so that the reservation can be expired when the timeout is reached. To store every reservation for every show, we can have a HashTable where the \u2018key\u2019 would be \u2018ShowID\u2019 and the \u2018value\u2019 would be the Linked HashMap containing \u2018BookingID\u2019 and creation \u2018Timestamp\u2019. In the database, we will store the reservation in the \u2018Booking\u2019 table and the expiry time will be in the Timestamp column. The \u2018Status\u2019 field will have a value of \u2018Reserved (1)\u2019 and, as soon as a booking is complete, the system will update the \u2018Status\u2019 to \u2018Booked (2)\u2019 and remove the reservation record from the Linked HashMap of the relevant show. When the reservation is expired, we can either remove it from the Booking table or mark it \u2018Expired (3)\u2019 in addition to removing it from memory. ActiveReservationsService will also work with the external financial service to process user payments. Whenever a booking is completed, or a reservation gets expired, WaitingUsersService will get a signal so that any waiting customer can be served. b. WaitingUsersService Just like ActiveReservationsService, we can keep all the waiting users of a show in memory in a Linked HashMap or a TreeMap. We need a data structure similar to Linked HashMap so that we can jump to any user to remove them from the HashMap when the user cancels their request. Also, since we are serving in a first-come-first-serve manner, the head of the Linked HashMap would always be pointing to the longest waiting user, so that whenever seats become available, we can serve users in a fair manner. We will have a HashTable to store all the waiting users for every Show. The \u2018key\u2019 would be 'ShowID, and the \u2018value\u2019 would be a Linked HashMap containing \u2018UserIDs\u2019 and their wait-start-time. Clients can use Long Polling for keeping themselves updated for their reservation status. Whenever seats become available, the server can use this request to notify the user. Reservation Expiration On the server, ActiveReservationsService keeps track of expiry (based on reservation time) of active reservations. As the client will be shown a timer (for the expiration time), which could be a little out of sync with the server, we can add a buffer of five seconds on the server to safeguard from a broken experience, such that the client never times out after the server, preventing a successful purchase. 9. Concurrency How to handle concurrency, such that no two users are able to book same seat. We can use transactions in SQL databases to avoid any clashes. For example, if we are using an SQL server we can utilize Transaction Isolation Levels to lock the rows before we can update them. Here is the sample code: SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; BEGIN TRANSACTION; -- Suppose we intend to reserve three seats (IDs: 54, 55, 56) for ShowID=99 Select * From Show_Seat where ShowID=99 && ShowSeatID in (54, 55, 56) && Status=0 -- free -- if the number of rows returned by the above statement is three, we can update to -- return success otherwise return failure to the user. update Show_Seat ... update Booking ... COMMIT TRANSACTION; \u2018Serializable\u2019 is the highest isolation level and guarantees safety from Dirty, Nonrepeatable, and Phantoms reads. One thing to note here; within a transaction, if we read rows, we get a write lock on them so that they can\u2019t be updated by anyone else. Once the above database transaction is successful, we can start tracking the reservation in ActiveReservationService. 10. Fault Tolerance What happens when ActiveReservationsService or WaitingUsersService crashes? Whenever ActiveReservationsService crashes, we can read all the active reservations from the \u2018Booking\u2019 table. Remember that we keep the \u2018Status\u2019 column as \u2018Reserved (1)\u2019 until a reservation gets booked. Another option is to have a master-slave configuration so that, when the master crashes, the slave can take over. We are not storing the waiting users in the database, so, when WaitingUsersService crashes, we don\u2019t have any means to recover that data unless we have a master-slave setup. Similarly, we\u2019ll have a master-slave setup for databases to make them fault tolerant. 11. Data Partitioning Database partitioning: If we partition by \u2018MovieID\u2019, then all the Shows of a movie will be on a single server. For a very hot movie, this could cause a lot of load on that server. A better approach would be to partition based on ShowID; this way, the load gets distributed among different servers. ActiveReservationService and WaitingUserService partitioning: Our web servers will manage all the active users\u2019 sessions and handle all the communication with the users. We can use the Consistent Hashing to allocate application servers for both ActiveReservationService and WaitingUserService based upon the \u2018ShowID\u2019. This way, all reservations and waiting users of a particular show will be handled by a certain set of servers. Let\u2019s assume for load balancing our Consistent Hashing allocates three servers for any Show, so whenever a reservation is expired, the server holding that reservation will do the following things: Update database to remove the Booking (or mark it expired) and update the seats\u2019 Status in \u2018Show_Seats\u2019 table. Remove the reservation from the Linked HashMap. Notify the user that their reservation has expired. Broadcast a message to all WaitingUserService servers that are holding waiting users of that Show to figure out the longest waiting user. Consistent Hashing scheme will tell what servers are holding these users. Send a message to the WaitingUserService server holding the longest waiting user to process their request if required seats have become available. Whenever a reservation is successful, following things will happen: The server holding that reservation sends a message to all servers holding the waiting users of that Show, so that those servers can expire all the waiting users that need more seats than the available seats. Upon receiving the above message, all servers holding the waiting users will query the database to find how many free seats are available now. A database cache would greatly help here to run this query only once. Expire all waiting users who want to reserve more seats than the available seats. For this, WaitingUserService has to iterate through the Linked HashMap of all the waiting users.","title":"Design Ticketmaster"},{"location":"DesignTicketmaster/#design-ticketmaster-new","text":"Let's design an online ticketing system that sells movie tickets like Ticketmaster or BookMyShow. Similar Services: bookmyshow.com, ticketmaster.com Difficulty Level: Hard","title":"Design Ticketmaster (New)"},{"location":"DesignTicketmaster/#1-what-is-an-online-movie-ticket-booking-system","text":"A movie ticket booking system provides its customers the ability to purchase theatre seats online. E-ticketing systems allow the customers to browse through movies currently being played and to book seats, anywhere anytime.","title":"1. What is an online movie ticket booking system?"},{"location":"DesignTicketmaster/#2-requirements-and-goals-of-the-system","text":"Our ticket booking service should meet the following requirements: Functional Requirements: Our ticket booking service should be able to list different cities where its affiliate cinemas are located. Once the user selects the city, the service should display the movies released in that particular city. Once the user selects a movie, the service should display the cinemas running that movie and its available show times. The user should be able to choose a show at a particular cinema and book their tickets. The service should be able to show the user the seating arrangement of the cinema hall. The user should be able to select multiple seats according to their preference. The user should be able to distinguish available seats from booked ones. Users should be able to put a hold on the seats for five minutes before they make a payment to finalize the booking. The user should be able to wait if there is a chance that the seats might become available, e.g., when holds by other users expire. Waiting customers should be serviced in a fair, first come, first serve manner. Non-Functional Requirements: The system would need to be highly concurrent. There will be multiple booking requests for the same seat at any particular point in time. The service should handle this gracefully and fairly. The core thing of the service is ticket booking, which means financial transactions. This means that the system should be secure and the database ACID compliant.","title":"2. Requirements and Goals of the System"},{"location":"DesignTicketmaster/#3-some-design-considerations","text":"For simplicity, let\u2019s assume our service does not require any user authentication. The system will not handle partial ticket orders. Either user gets all the tickets they want or they get nothing. Fairness is mandatory for the system. To stop system abuse, we can restrict users from booking more than ten seats at a time. We can assume that traffic would spike on popular/much-awaited movie releases and the seats would fill up pretty fast. The system should be scalable and highly available to keep up with the surge in traffic.","title":"3. Some Design Considerations"},{"location":"DesignTicketmaster/#4-capacity-estimation","text":"Traffic estimates: Let\u2019s assume that our service has 3 billion page views per month and sells 10 million tickets a month. Storage estimates: Let\u2019s assume that we have 500 cities and, on average each city has ten cinemas. If there are 2000 seats in each cinema and on average, there are two shows every day. Let\u2019s assume each seat booking needs 50 bytes (IDs, NumberOfSeats, ShowID, MovieID, SeatNumbers, SeatStatus, Timestamp, etc.) to store in the database. We would also need to store information about movies and cinemas; let\u2019s assume it\u2019ll take 50 bytes. So, to store all the data about all shows of all cinemas of all cities for a day: 500 cities * 10 cinemas * 2000 seats * 2 shows * (50+50) bytes = 2GB / day To store five years of this data, we would need around 3.6TB.","title":"4. Capacity Estimation"},{"location":"DesignTicketmaster/#5-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the APIs to search movie shows and reserve seats. SearchMovies(api_dev_key, keyword, city, lat_long, radius, start_datetime, end_datetime, postal_code, includeSpellcheck, results_per_page, sorting_order) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. keyword (string): Keyword to search on. city (string): City to filter movies by. lat_long (string): Latitude and longitude to filter by. radius (number): Radius of the area in which we want to search for events. start_datetime (string): Filter movies with a starting datetime. end_datetime (string): Filter movies with an ending datetime. postal_code (string): Filter movies by postal code / zipcode. includeSpellcheck (Enum: \u201cyes\u201d or \u201cno\u201d): Yes, to include spell check suggestions in the response. results_per_page (number): Number of results to return per page. Maximum is 30. sorting_order (string): Sorting order of the search result. Some allowable values : \u2018name,asc\u2019, \u2018name,desc\u2019, \u2018date,asc\u2019, \u2018date,desc\u2019, \u2018distance,asc\u2019, \u2018name,date,asc\u2019, \u2018name,date,desc\u2019, \u2018date,name,asc\u2019, \u2018date,name,desc\u2019. Returns: (JSON) Here is a sample list of movies and their shows: [ { \"MovieID\": 1, \"ShowID\": 1, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"14:00\", \"EndTime\": \"16:00\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Almost Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Available\" } ] }, { \"MovieID\": 1, \"ShowID\": 2, \"Title\": \"Cars 2\", \"Description\": \"About cars\", \"Duration\": 120, \"Genre\": \"Animation\", \"Language\": \"English\", \"ReleaseDate\": \"8th Oct. 2014\", \"Country\": USA, \"StartTime\": \"16:30\", \"EndTime\": \"18:30\", \"Seats\": [ { \"Type\": \"Regular\" \"Price\": 14.99 \"Status: \"Full\" }, { \"Type\": \"Premium\" \"Price\": 24.99 \"Status: \"Almost Full\" } ] }, ] ReserveSeats(api_dev_key, session_id, movie_id, show_id, seats_to_reserve[]) Parameters: - api_dev_key (string): same as above - session_id (string): User\u2019s session ID to track this reservation. Once the reservation time expires, user\u2019s reservation on the server will be removed using this ID. - movie_id (string): Movie to reserve. - show_id (string): Show to reserve. - seats_to_reserve (number): An array containing seat IDs to reserve. Returns: (JSON) Returns the status of the reservation, which would be one of the following: 1) \u201cReservation Successful\u201d 2) \u201cReservation Failed - Show Full,\u201d 3) \u201cReservation Failed - Retry, as other users are holding reserved seats\u201d.","title":"5. System APIs"},{"location":"DesignTicketmaster/#6-database-design","text":"Here are a few observations about the data we are going to store: Each City can have multiple Cinemas. Each Cinema will have multiple halls. Each Movie will have many Shows and each Show will have multiple Bookings. A user can have multiple bookings.","title":"6. Database Design"},{"location":"DesignTicketmaster/#7-high-level-design","text":"At a high-level, our web servers will manage users\u2019 sessions and application servers will handle all the ticket management, storing data in the databases as well as working with the cache servers to process reservations.","title":"7. High Level Design"},{"location":"DesignTicketmaster/#8-detailed-component-design","text":"First, let\u2019s try to build our service assuming it is being served from a single server. Ticket Booking Workflow: The following would be a typical ticket booking workflow: The user searches for a movie. The user selects a movie. The user is shown the available shows of the movie. The user selects a show. The user selects the number of seats to be reserved. If the required number of seats are available, the user is shown a map of the theater to select seats. If not, the user is taken to \u2018step 8\u2019 below. Once the user selects the seat, the system will try to reserve those selected seats. If seats can\u2019t be reserved, we have the following options: Show is full; the user is shown the error message. The seats the user wants to reserve are no longer available, but there are other seats available, so the user is taken back to the theater map to choose different seats. There are no seats available to reserve, but all the seats are not booked yet, as there are some seats that other users are holding in the reservation pool and have not booked yet. The user will be taken to a waiting page where they can wait until the required seats get freed from the reservation pool. This waiting could result in the following options: If the required number of seats become available, the user is taken to the theater map page where they can choose seats. While waiting, if all seats get booked or there are fewer seats in the reservation pool than the user intend to book, the user is shown the error message. User cancels the waiting and is taken back to the movie search page. At maximum, a user can wait one hour, after that user\u2019s session gets expired and the user is taken back to the movie search page. If seats are reserved successfully, the user has five minutes to pay for the reservation. After payment, booking is marked complete. If the user is not able to pay within five minutes, all their reserved seats are freed to become available to other users. How would the server keep track of all the active reservation that haven\u2019t been booked yet? And how would the server keep track of all the waiting customers? We need two daemon services, one to keep track of all active reservations and remove any expired reservation from the system; let\u2019s call it ActiveReservationService. The other service would be keeping track of all the waiting user requests and, as soon as the required number of seats become available, it will notify the (the longest waiting) user to choose the seats; let\u2019s call it WaitingUserService. a. ActiveReservationsService We can keep all the reservations of a \u2018show\u2019 in memory in a data structure similar to Linked HashMap or a TreeMap in addition to keeping all the data in the database. We will need a linked HashMap kind of data structure that allows us to jump to any reservation to remove it when the booking is complete. Also, since we will have expiry time associated with each reservation, the head of the HashMap will always point to the oldest reservation record so that the reservation can be expired when the timeout is reached. To store every reservation for every show, we can have a HashTable where the \u2018key\u2019 would be \u2018ShowID\u2019 and the \u2018value\u2019 would be the Linked HashMap containing \u2018BookingID\u2019 and creation \u2018Timestamp\u2019. In the database, we will store the reservation in the \u2018Booking\u2019 table and the expiry time will be in the Timestamp column. The \u2018Status\u2019 field will have a value of \u2018Reserved (1)\u2019 and, as soon as a booking is complete, the system will update the \u2018Status\u2019 to \u2018Booked (2)\u2019 and remove the reservation record from the Linked HashMap of the relevant show. When the reservation is expired, we can either remove it from the Booking table or mark it \u2018Expired (3)\u2019 in addition to removing it from memory. ActiveReservationsService will also work with the external financial service to process user payments. Whenever a booking is completed, or a reservation gets expired, WaitingUsersService will get a signal so that any waiting customer can be served. b. WaitingUsersService Just like ActiveReservationsService, we can keep all the waiting users of a show in memory in a Linked HashMap or a TreeMap. We need a data structure similar to Linked HashMap so that we can jump to any user to remove them from the HashMap when the user cancels their request. Also, since we are serving in a first-come-first-serve manner, the head of the Linked HashMap would always be pointing to the longest waiting user, so that whenever seats become available, we can serve users in a fair manner. We will have a HashTable to store all the waiting users for every Show. The \u2018key\u2019 would be 'ShowID, and the \u2018value\u2019 would be a Linked HashMap containing \u2018UserIDs\u2019 and their wait-start-time. Clients can use Long Polling for keeping themselves updated for their reservation status. Whenever seats become available, the server can use this request to notify the user. Reservation Expiration On the server, ActiveReservationsService keeps track of expiry (based on reservation time) of active reservations. As the client will be shown a timer (for the expiration time), which could be a little out of sync with the server, we can add a buffer of five seconds on the server to safeguard from a broken experience, such that the client never times out after the server, preventing a successful purchase.","title":"8. Detailed Component Design"},{"location":"DesignTicketmaster/#9-concurrency","text":"How to handle concurrency, such that no two users are able to book same seat. We can use transactions in SQL databases to avoid any clashes. For example, if we are using an SQL server we can utilize Transaction Isolation Levels to lock the rows before we can update them. Here is the sample code: SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; BEGIN TRANSACTION; -- Suppose we intend to reserve three seats (IDs: 54, 55, 56) for ShowID=99 Select * From Show_Seat where ShowID=99 && ShowSeatID in (54, 55, 56) && Status=0 -- free -- if the number of rows returned by the above statement is three, we can update to -- return success otherwise return failure to the user. update Show_Seat ... update Booking ... COMMIT TRANSACTION; \u2018Serializable\u2019 is the highest isolation level and guarantees safety from Dirty, Nonrepeatable, and Phantoms reads. One thing to note here; within a transaction, if we read rows, we get a write lock on them so that they can\u2019t be updated by anyone else. Once the above database transaction is successful, we can start tracking the reservation in ActiveReservationService.","title":"9. Concurrency"},{"location":"DesignTicketmaster/#10-fault-tolerance","text":"What happens when ActiveReservationsService or WaitingUsersService crashes? Whenever ActiveReservationsService crashes, we can read all the active reservations from the \u2018Booking\u2019 table. Remember that we keep the \u2018Status\u2019 column as \u2018Reserved (1)\u2019 until a reservation gets booked. Another option is to have a master-slave configuration so that, when the master crashes, the slave can take over. We are not storing the waiting users in the database, so, when WaitingUsersService crashes, we don\u2019t have any means to recover that data unless we have a master-slave setup. Similarly, we\u2019ll have a master-slave setup for databases to make them fault tolerant.","title":"10. Fault Tolerance"},{"location":"DesignTicketmaster/#11-data-partitioning","text":"Database partitioning: If we partition by \u2018MovieID\u2019, then all the Shows of a movie will be on a single server. For a very hot movie, this could cause a lot of load on that server. A better approach would be to partition based on ShowID; this way, the load gets distributed among different servers. ActiveReservationService and WaitingUserService partitioning: Our web servers will manage all the active users\u2019 sessions and handle all the communication with the users. We can use the Consistent Hashing to allocate application servers for both ActiveReservationService and WaitingUserService based upon the \u2018ShowID\u2019. This way, all reservations and waiting users of a particular show will be handled by a certain set of servers. Let\u2019s assume for load balancing our Consistent Hashing allocates three servers for any Show, so whenever a reservation is expired, the server holding that reservation will do the following things: Update database to remove the Booking (or mark it expired) and update the seats\u2019 Status in \u2018Show_Seats\u2019 table. Remove the reservation from the Linked HashMap. Notify the user that their reservation has expired. Broadcast a message to all WaitingUserService servers that are holding waiting users of that Show to figure out the longest waiting user. Consistent Hashing scheme will tell what servers are holding these users. Send a message to the WaitingUserService server holding the longest waiting user to process their request if required seats have become available. Whenever a reservation is successful, following things will happen: The server holding that reservation sends a message to all servers holding the waiting users of that Show, so that those servers can expire all the waiting users that need more seats than the available seats. Upon receiving the above message, all servers holding the waiting users will query the database to find how many free seats are available now. A database cache would greatly help here to run this query only once. Expire all waiting users who want to reserve more seats than the available seats. For this, WaitingUserService has to iterate through the Linked HashMap of all the waiting users.","title":"11. Data Partitioning"},{"location":"DesignURLShorteningService/","text":"Question: Designing a URL Shortening service like TinyURL Design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy Pratice on full Screen Hints to solve the problem 1. Think about functional Non functional Requirements. 2. Capacity Estimation and Constraints like traffic, bandwidth and stroage estimates. 3. Think about System API's. 4. What about database system design 5. What about Data Partitioning and Replication 6. Think about Cache and Load Balancer","title":"Question: Designing a URL Shortening service like TinyURL"},{"location":"DesignURLShorteningService/#question-designing-a-url-shortening-service-like-tinyurl","text":"Design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy Pratice on full Screen","title":"Question: Designing a URL Shortening service like TinyURL"},{"location":"DesigningAPIRateLimiter/","text":"Designing an API Rate Limiter Let's design an API Rate Limiter which will throttle users based upon the number of the requests they are sending. Difficulty Level: Medium 1. What is a Rate Limiter? Imagine we have a service which is receiving a huge number of requests, but it can only serve a limited number of requests per second. To handle this problem we would need some kind of throttling or rate limiting mechanism that would allow only a certain number of requests so our service can respond to all of them. A rate limiter, at a high-level, limits the number of events an entity (user, device, IP, etc.) can perform in a particular time window. For example: A user can send only one message per second. A user is allowed only three failed credit card transactions per day. A single IP can only create twenty accounts per day. In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks requests once the cap is reached. 2. Why do we need API rate limiting? Rate Limiting helps to protect services against abusive behaviors targeting the application layer like Denial-of-service (DOS) attacks, brute-force password attempts, brute-force credit card transactions, etc. These attacks are usually a barrage of HTTP/S requests which may look like they are coming from real users, but are typically generated by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API. Rate limiting is also used to prevent revenue loss, to reduce infrastructure costs, to stop spam, and to stop online harassment. Following is a list of scenarios that can benefit from Rate limiting by making a service (or API) more reliable: Misbehaving clients/scripts: Either intentionally or unintentionally, some entities can overwhelm a service by sending a large number of requests. Another scenario could be when a user is sending a lot of lower-priority requests and we want to make sure that it doesn\u2019t affect the high-priority traffic. For example, users sending a high volume of requests for analytics data should not be allowed to hamper critical transactions for other users. Security: By limiting the number of the second-factor attempts (in 2-factor auth) that the users are allowed to perform, for example, the number of times they\u2019re allowed to try with a wrong password. To prevent abusive behavior and bad design practices: Without API limits, developers of client applications would use sloppy development tactics, for example, requesting the same information over and over again. To keep costs and resource usage under control: Services are generally designed for normal input behavior, for example, a user writing a single post in a minute. Computers could easily push thousands/second through an API. Rate limiter enables controls on service APIs. Revenue: Certain services might want to limit operations based on the tier of their customer\u2019s service and thus create a revenue model based on rate limiting. There could be default limits for all the APIs a service offers. To go beyond that, the user has to buy higher limits To eliminate spikiness in traffic: Make sure the service stays up for everyone else. 3. Requirements and Goals of the System Our Rate Limiter should meet the following requirements: Functional Requirements: Limit the number of requests an entity can send to an API within a time window, e.g., 15 requests per second. The APIs are accessible through a cluster, so the rate limit should be considered across different servers. The user should get an error message whenever the defined threshold is crossed within a single server or across a combination of servers. Non-Functional Requirements: The system should be highly available. The rate limiter should always work since it protects our service from external attacks. Our rate limiter should not introduce substantial latencies affecting the user experience. 4. How to do Rate Limiting? Rate Limiting is a process that is used to define the rate and speed at which consumers can access APIs. Throttling is the process of controlling the usage of the APIs by customers during a given period. Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the server returns HTTP status \u201c429 - Too many requests\". 5. What are different types of throttling? Here are the three famous throttling types that are used by different services: Hard Throttling: The number of API requests cannot exceed the throttle limit. Soft Throttling: In this type, we can set the API request limit to exceed a certain percentage. For example, if we have rate-limit of 100 messages a minute and 10% exceed-limit, our rate limiter will allow up to 110 messages per minute. Elastic or Dynamic Throttling: Under Elastic throttling, the number of requests can go beyond the threshold if the system has some resources available. For example, if a user is allowed only 100 messages a minute, we can let the user send more than 100 messages a minute when there are free resources available in the system. 6. What are different types of algorithms used for Rate Limiting? Following are the two types of algorithms used for Rate Limiting: Fixed Window Algorithm: In this algorithm, the time window is considered from the start of the time-unit to the end of the time-unit. For example, a period would be considered 0-60 seconds for a minute irrespective of the time frame at which the API request has been made. In the diagram below, there are two messages between 0-1 second and three messages between 1-2 seconds. If we have a rate limiting of two messages a second, this algorithm will throttle only \u2018m5\u2019. Rolling Window Algorithm: In this algorithm, the time window is considered from the fraction of the time at which the request is made plus the time window length. For example, if there are two messages sent at the 300th millisecond and 400th millisecond of a second, we\u2019ll count them as two messages from the 300th millisecond of that second up to the 300th millisecond of next second. In the above diagram, keeping two messages a second, we\u2019ll throttle \u2018m3\u2019 and \u2018m4\u2019. 7. High level design for Rate Limiter Rate Limiter will be responsible for deciding which request will be served by the API servers and which request will be declined. Once a new request arrives, the Web Server first asks the Rate Limiter to decide if it will be served or throttled. If the request is not throttled, then it\u2019ll be passed to the API servers. 8. Basic System Design and Algorithm Let\u2019s take the example where we want to limit the number of requests per user. Under this scenario, for each unique user, we would keep a count representing how many requests the user has made and a timestamp when we started counting the requests. We can keep it in a hashtable, where the \u2018key\u2019 would be the \u2018UserID\u2019 and \u2018value\u2019 would be a structure containing an integer for the \u2018Count\u2019 and an integer for the Epoch time: Let\u2019s assume our rate limiter is allowing three requests per minute per user, so whenever a new request comes in, our rate limiter will perform the following steps: If the \u2018UserID\u2019 is not present in the hash-table, insert it, set the \u2018Count\u2019 to 1, set \u2018StartTime\u2019 to the current time (normalized to a minute), and allow the request. Otherwise, find the record of the \u2018UserID\u2019 and if CurrentTime \u2013 StartTime >= 1 min, set the \u2018StartTime\u2019 to the current time, \u2018Count\u2019 to 1, and allow the request. If CurrentTime - StartTime <= 1 min and If \u2018Count < 3\u2019, increment the Count and allow the request. If \u2018Count >= 3\u2019, reject the request. What are some of the problems with our algorithm? This is a Fixed Window algorithm since we\u2019re resetting the \u2018StartTime\u2019 at the end of every minute, which means it can potentially allow twice the number of requests per minute. Imagine if Kristie sends three requests at the last second of a minute, then she can immediately send three more requests at the very first second of the next minute, resulting in 6 requests in the span of two seconds. The solution to this problem would be a sliding window algorithm which we\u2019ll discuss later. 2. Atomicity: In a distributed environment, the \u201cread-and-then-write\u201d behavior can create a race condition. Imagine if Kristie\u2019s current \u2018Count\u2019 is \u201c2\u201d and that she issues two more requests. If two separate processes served each of these requests and concurrently read the Count before either of them updated it, each process would think that Kristie could have one more request and that she had not hit the rate limit. If we are using Redis to store our key-value, one solution to resolve the atomicity problem is to use Redis lock for the duration of the read-update operation. This, however, would come at the expense of slowing down concurrent requests from the same user and introducing another layer of complexity. We can use Memcached, but it would have comparable complications. If we are using a simple hash-table, we can have a custom implementation for \u2018locking\u2019 each record to solve our atomicity problems. How much memory would we need to store all of the user data? Let\u2019s assume the simple solution where we are keeping all of the data in a hash-table. Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Let\u2019s also assume a 2 byte \u2018Count\u2019, which can count up to 65k, is sufficient for our use case. Although epoch time will need 4 bytes, we can choose to store only the minute and second part, which can fit into 2 bytes. Hence, we need a total of 12 bytes to store a user\u2019s data: 8 + 2 + 2 = 12 bytes Let\u2019s assume our hash-table has an overhead of 20 bytes for each record. If we need to track one million users at any time, the total memory we would need would be 32MB: (12 + 20) bytes * 1 million => 32MB If we assume that we would need a 4-byte number to lock each user\u2019s record to resolve our atomicity problems, we would require a total 36MB memory. This can easily fit on a single server; however we would not like to route all of our traffic through a single machine. Also, if we assume a rate limit of 10 requests per second, this would translate into 10 million QPS for our rate limiter! This would be too much for a single server. Practically, we can assume we would use a Redis or Memcached kind of a solution in a distributed setup. We\u2019ll be storing all the data in the remote Redis servers and all the Rate Limiter servers will read (and update) these servers before serving or throttling any request. 9. Sliding Window algorithm We can maintain a sliding window if we can keep track of each request per user. We can store the timestamp of each request in a Redis Sorted Set in our \u2018value\u2019 field of hash-table. Let\u2019s assume our rate limiter is allowing three requests per minute per user, so, whenever a new request comes in, the Rate Limiter will perform following steps: Remove all the timestamps from the Sorted Set that are older than \u201cCurrentTime - 1 minute\u201d. Count the total number of elements in the sorted set. Reject the request if this count is greater than our throttling limit of \u201c3\u201d. Insert the current time in the sorted set and accept the request. How much memory would we need to store all of the user data for sliding window? Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Each epoch time will require 4 bytes. Let\u2019s suppose we need a rate limiting of 500 requests per hour. Let\u2019s assume 20 bytes overhead for hash-table and 20 bytes overhead for the Sorted Set. At max, we would need a total of 12KB to store one user\u2019s data: 8 + (4 + 20 (sorted set overhead)) * 500 + 20 (hash-table overhead) = 12KB Here we are reserving 20 bytes overhead per element. In a sorted set, we can assume that we need at least two pointers to maintain order among elements \u2014 one pointer to the previous element and one to the next element. On a 64bit machine, each pointer will cost 8 bytes. So we will need 16 bytes for pointers. We added an extra word (4 bytes) for storing other overhead. If we need to track one million users at any time, total memory we would need would be 12GB: 12KB * 1 million ~= 12GB Sliding Window Algorithm takes a lot of memory compared to the Fixed Window; this would be a scalability issue. What if we can combine the above two algorithms to optimize our memory usage? 10. Sliding Window with Counters What if we keep track of request counts for each user using multiple fixed time windows, e.g., 1/60th the size of our rate limit\u2019s time window. For example, if we have an hourly rate limit we can keep a count for each minute and calculate the sum of all counters in the past hour when we receive a new request to calculate the throttling limit. This would reduce our memory footprint. Let\u2019s take an example where we rate-limit at 500 requests per hour with an additional limit of 10 requests per minute. This means that when the sum of the counters with timestamps in the past hour exceeds the request threshold (500), Kristie has exceeded the rate limit. In addition to that, she can\u2019t send more than ten requests per minute. This would be a reasonable and practical consideration, as none of the real users would send frequent requests. Even if they do, they will see success with retries since their limits get reset every minute. We can store our counters in a Redis Hash since it offers incredibly efficient storage for fewer than 100 keys. When each request increments a counter in the hash, it also sets the hash to expire an hour later. We will normalize each \u2018time\u2019 to a minute. How much memory we would need to store all the user data for sliding window with counters? Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Each epoch time will need 4 bytes, and the Counter would need 2 bytes. Let\u2019s suppose we need a rate limiting of 500 requests per hour. Assume 20 bytes overhead for hash-table and 20 bytes for Redis hash. Since we\u2019ll keep a count for each minute, at max, we would need 60 entries for each user. We would need a total of 1.6KB to store one user\u2019s data: 8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB If we need to track one million users at any time, total memory we would need would be 1.6GB: 1.6KB * 1 million ~= 1.6GB So, our \u2018Sliding Window with Counters\u2019 algorithm uses 86% less memory than the simple sliding window algorithm. 11. Data Sharding and Caching We can shard based on the \u2018UserID\u2019 to distribute the user\u2019s data. For fault tolerance and replication we should use Consistent Hashing. If we want to have different throttling limits for different APIs, we can choose to shard per user per API. Take the example of URL Shortener; we can have different rate limiter for createURL() and deleteURL() APIs for each user or IP. If our APIs are partitioned, a practical consideration could be to have a separate (somewhat smaller) rate limiter for each API shard as well. Let\u2019s take the example of our URL Shortener where we want to limit each user not to create more than 100 short URLs per hour. Assuming we are using Hash-Based Partitioning for our createURL() API, we can rate limit each partition to allow a user to create not more than three short URLs per minute in addition to 100 short URLs per hour. Our system can get huge benefits from caching recent active users. Application servers can quickly check if the cache has the desired record before hitting backend servers. Our rate limiter can significantly benefit from the Write-back cache by updating all counters and timestamps in cache only. The write to the permanent storage can be done at fixed intervals. This way we can ensure minimum latency added to the user\u2019s requests by the rate limiter. The reads can always hit the cache first; which will be extremely useful once the user has hit their maximum limit and the rate limiter will only be reading data without any updates. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system. 12. Should we rate limit by IP or by user? Let\u2019s discuss the pros and cons of using each one of these schemes: IP: In this scheme, we throttle requests per-IP; although it\u2019s not optimal in terms of differentiating between \u2018good\u2019 and \u2018bad\u2019 actors, it\u2019s still better than not have rate limiting at all. The biggest problem with IP based throttling is when multiple users share a single public IP like in an internet cafe or smartphone users that are using the same gateway. One bad user can cause throttling to other users. Another issue could arise while caching IP-based limits, as there are a huge number of IPv6 addresses available to a hacker from even one computer, it\u2019s trivial to make a server run out of memory tracking IPv6 addresses! User: Rate limiting can be done on APIs after user authentication. Once authenticated, the user will be provided with a token which the user will pass with each request. This will ensure that we will rate limit against a particular API that has a valid authentication token. But what if we have to rate limit on the login API itself? The weakness of this rate-limiting would be that a hacker can perform a denial of service attack against a user by entering wrong credentials up to the limit; after that the actual user will not be able to log-in. How about if we combine the above two schemes? Hybrid: A right approach could be to do both per-IP and per-user rate limiting, as they both have weaknesses when implemented alone, though, this will result in more cache entries with more details per entry, hence requiring more memory and storage.","title":"Designing an API Rate Limiter"},{"location":"DesigningAPIRateLimiter/#designing-an-api-rate-limiter","text":"Let's design an API Rate Limiter which will throttle users based upon the number of the requests they are sending. Difficulty Level: Medium","title":"Designing an API Rate Limiter"},{"location":"DesigningAPIRateLimiter/#1-what-is-a-rate-limiter","text":"Imagine we have a service which is receiving a huge number of requests, but it can only serve a limited number of requests per second. To handle this problem we would need some kind of throttling or rate limiting mechanism that would allow only a certain number of requests so our service can respond to all of them. A rate limiter, at a high-level, limits the number of events an entity (user, device, IP, etc.) can perform in a particular time window. For example: A user can send only one message per second. A user is allowed only three failed credit card transactions per day. A single IP can only create twenty accounts per day. In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks requests once the cap is reached.","title":"1. What is a Rate Limiter?"},{"location":"DesigningAPIRateLimiter/#2-why-do-we-need-api-rate-limiting","text":"Rate Limiting helps to protect services against abusive behaviors targeting the application layer like Denial-of-service (DOS) attacks, brute-force password attempts, brute-force credit card transactions, etc. These attacks are usually a barrage of HTTP/S requests which may look like they are coming from real users, but are typically generated by machines (or bots). As a result, these attacks are often harder to detect and can more easily bring down a service, application, or an API. Rate limiting is also used to prevent revenue loss, to reduce infrastructure costs, to stop spam, and to stop online harassment. Following is a list of scenarios that can benefit from Rate limiting by making a service (or API) more reliable: Misbehaving clients/scripts: Either intentionally or unintentionally, some entities can overwhelm a service by sending a large number of requests. Another scenario could be when a user is sending a lot of lower-priority requests and we want to make sure that it doesn\u2019t affect the high-priority traffic. For example, users sending a high volume of requests for analytics data should not be allowed to hamper critical transactions for other users. Security: By limiting the number of the second-factor attempts (in 2-factor auth) that the users are allowed to perform, for example, the number of times they\u2019re allowed to try with a wrong password. To prevent abusive behavior and bad design practices: Without API limits, developers of client applications would use sloppy development tactics, for example, requesting the same information over and over again. To keep costs and resource usage under control: Services are generally designed for normal input behavior, for example, a user writing a single post in a minute. Computers could easily push thousands/second through an API. Rate limiter enables controls on service APIs. Revenue: Certain services might want to limit operations based on the tier of their customer\u2019s service and thus create a revenue model based on rate limiting. There could be default limits for all the APIs a service offers. To go beyond that, the user has to buy higher limits To eliminate spikiness in traffic: Make sure the service stays up for everyone else.","title":"2. Why do we need API rate limiting?"},{"location":"DesigningAPIRateLimiter/#3-requirements-and-goals-of-the-system","text":"Our Rate Limiter should meet the following requirements: Functional Requirements: Limit the number of requests an entity can send to an API within a time window, e.g., 15 requests per second. The APIs are accessible through a cluster, so the rate limit should be considered across different servers. The user should get an error message whenever the defined threshold is crossed within a single server or across a combination of servers. Non-Functional Requirements: The system should be highly available. The rate limiter should always work since it protects our service from external attacks. Our rate limiter should not introduce substantial latencies affecting the user experience.","title":"3. Requirements and Goals of the System"},{"location":"DesigningAPIRateLimiter/#4-how-to-do-rate-limiting","text":"Rate Limiting is a process that is used to define the rate and speed at which consumers can access APIs. Throttling is the process of controlling the usage of the APIs by customers during a given period. Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the server returns HTTP status \u201c429 - Too many requests\".","title":"4. How to do Rate Limiting?"},{"location":"DesigningAPIRateLimiter/#5-what-are-different-types-of-throttling","text":"Here are the three famous throttling types that are used by different services: Hard Throttling: The number of API requests cannot exceed the throttle limit. Soft Throttling: In this type, we can set the API request limit to exceed a certain percentage. For example, if we have rate-limit of 100 messages a minute and 10% exceed-limit, our rate limiter will allow up to 110 messages per minute. Elastic or Dynamic Throttling: Under Elastic throttling, the number of requests can go beyond the threshold if the system has some resources available. For example, if a user is allowed only 100 messages a minute, we can let the user send more than 100 messages a minute when there are free resources available in the system.","title":"5. What are different types of throttling?"},{"location":"DesigningAPIRateLimiter/#6-what-are-different-types-of-algorithms-used-for-rate-limiting","text":"Following are the two types of algorithms used for Rate Limiting: Fixed Window Algorithm: In this algorithm, the time window is considered from the start of the time-unit to the end of the time-unit. For example, a period would be considered 0-60 seconds for a minute irrespective of the time frame at which the API request has been made. In the diagram below, there are two messages between 0-1 second and three messages between 1-2 seconds. If we have a rate limiting of two messages a second, this algorithm will throttle only \u2018m5\u2019. Rolling Window Algorithm: In this algorithm, the time window is considered from the fraction of the time at which the request is made plus the time window length. For example, if there are two messages sent at the 300th millisecond and 400th millisecond of a second, we\u2019ll count them as two messages from the 300th millisecond of that second up to the 300th millisecond of next second. In the above diagram, keeping two messages a second, we\u2019ll throttle \u2018m3\u2019 and \u2018m4\u2019.","title":"6. What are different types of algorithms used for Rate Limiting?"},{"location":"DesigningAPIRateLimiter/#7-high-level-design-for-rate-limiter","text":"Rate Limiter will be responsible for deciding which request will be served by the API servers and which request will be declined. Once a new request arrives, the Web Server first asks the Rate Limiter to decide if it will be served or throttled. If the request is not throttled, then it\u2019ll be passed to the API servers.","title":"7. High level design for Rate Limiter"},{"location":"DesigningAPIRateLimiter/#8-basic-system-design-and-algorithm","text":"Let\u2019s take the example where we want to limit the number of requests per user. Under this scenario, for each unique user, we would keep a count representing how many requests the user has made and a timestamp when we started counting the requests. We can keep it in a hashtable, where the \u2018key\u2019 would be the \u2018UserID\u2019 and \u2018value\u2019 would be a structure containing an integer for the \u2018Count\u2019 and an integer for the Epoch time: Let\u2019s assume our rate limiter is allowing three requests per minute per user, so whenever a new request comes in, our rate limiter will perform the following steps: If the \u2018UserID\u2019 is not present in the hash-table, insert it, set the \u2018Count\u2019 to 1, set \u2018StartTime\u2019 to the current time (normalized to a minute), and allow the request. Otherwise, find the record of the \u2018UserID\u2019 and if CurrentTime \u2013 StartTime >= 1 min, set the \u2018StartTime\u2019 to the current time, \u2018Count\u2019 to 1, and allow the request. If CurrentTime - StartTime <= 1 min and If \u2018Count < 3\u2019, increment the Count and allow the request. If \u2018Count >= 3\u2019, reject the request. What are some of the problems with our algorithm? This is a Fixed Window algorithm since we\u2019re resetting the \u2018StartTime\u2019 at the end of every minute, which means it can potentially allow twice the number of requests per minute. Imagine if Kristie sends three requests at the last second of a minute, then she can immediately send three more requests at the very first second of the next minute, resulting in 6 requests in the span of two seconds. The solution to this problem would be a sliding window algorithm which we\u2019ll discuss later. 2. Atomicity: In a distributed environment, the \u201cread-and-then-write\u201d behavior can create a race condition. Imagine if Kristie\u2019s current \u2018Count\u2019 is \u201c2\u201d and that she issues two more requests. If two separate processes served each of these requests and concurrently read the Count before either of them updated it, each process would think that Kristie could have one more request and that she had not hit the rate limit. If we are using Redis to store our key-value, one solution to resolve the atomicity problem is to use Redis lock for the duration of the read-update operation. This, however, would come at the expense of slowing down concurrent requests from the same user and introducing another layer of complexity. We can use Memcached, but it would have comparable complications. If we are using a simple hash-table, we can have a custom implementation for \u2018locking\u2019 each record to solve our atomicity problems. How much memory would we need to store all of the user data? Let\u2019s assume the simple solution where we are keeping all of the data in a hash-table. Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Let\u2019s also assume a 2 byte \u2018Count\u2019, which can count up to 65k, is sufficient for our use case. Although epoch time will need 4 bytes, we can choose to store only the minute and second part, which can fit into 2 bytes. Hence, we need a total of 12 bytes to store a user\u2019s data: 8 + 2 + 2 = 12 bytes Let\u2019s assume our hash-table has an overhead of 20 bytes for each record. If we need to track one million users at any time, the total memory we would need would be 32MB: (12 + 20) bytes * 1 million => 32MB If we assume that we would need a 4-byte number to lock each user\u2019s record to resolve our atomicity problems, we would require a total 36MB memory. This can easily fit on a single server; however we would not like to route all of our traffic through a single machine. Also, if we assume a rate limit of 10 requests per second, this would translate into 10 million QPS for our rate limiter! This would be too much for a single server. Practically, we can assume we would use a Redis or Memcached kind of a solution in a distributed setup. We\u2019ll be storing all the data in the remote Redis servers and all the Rate Limiter servers will read (and update) these servers before serving or throttling any request.","title":"8. Basic System Design and Algorithm"},{"location":"DesigningAPIRateLimiter/#9-sliding-window-algorithm","text":"We can maintain a sliding window if we can keep track of each request per user. We can store the timestamp of each request in a Redis Sorted Set in our \u2018value\u2019 field of hash-table. Let\u2019s assume our rate limiter is allowing three requests per minute per user, so, whenever a new request comes in, the Rate Limiter will perform following steps: Remove all the timestamps from the Sorted Set that are older than \u201cCurrentTime - 1 minute\u201d. Count the total number of elements in the sorted set. Reject the request if this count is greater than our throttling limit of \u201c3\u201d. Insert the current time in the sorted set and accept the request. How much memory would we need to store all of the user data for sliding window? Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Each epoch time will require 4 bytes. Let\u2019s suppose we need a rate limiting of 500 requests per hour. Let\u2019s assume 20 bytes overhead for hash-table and 20 bytes overhead for the Sorted Set. At max, we would need a total of 12KB to store one user\u2019s data: 8 + (4 + 20 (sorted set overhead)) * 500 + 20 (hash-table overhead) = 12KB Here we are reserving 20 bytes overhead per element. In a sorted set, we can assume that we need at least two pointers to maintain order among elements \u2014 one pointer to the previous element and one to the next element. On a 64bit machine, each pointer will cost 8 bytes. So we will need 16 bytes for pointers. We added an extra word (4 bytes) for storing other overhead. If we need to track one million users at any time, total memory we would need would be 12GB: 12KB * 1 million ~= 12GB Sliding Window Algorithm takes a lot of memory compared to the Fixed Window; this would be a scalability issue. What if we can combine the above two algorithms to optimize our memory usage?","title":"9. Sliding Window algorithm"},{"location":"DesigningAPIRateLimiter/#10-sliding-window-with-counters","text":"What if we keep track of request counts for each user using multiple fixed time windows, e.g., 1/60th the size of our rate limit\u2019s time window. For example, if we have an hourly rate limit we can keep a count for each minute and calculate the sum of all counters in the past hour when we receive a new request to calculate the throttling limit. This would reduce our memory footprint. Let\u2019s take an example where we rate-limit at 500 requests per hour with an additional limit of 10 requests per minute. This means that when the sum of the counters with timestamps in the past hour exceeds the request threshold (500), Kristie has exceeded the rate limit. In addition to that, she can\u2019t send more than ten requests per minute. This would be a reasonable and practical consideration, as none of the real users would send frequent requests. Even if they do, they will see success with retries since their limits get reset every minute. We can store our counters in a Redis Hash since it offers incredibly efficient storage for fewer than 100 keys. When each request increments a counter in the hash, it also sets the hash to expire an hour later. We will normalize each \u2018time\u2019 to a minute. How much memory we would need to store all the user data for sliding window with counters? Let\u2019s assume \u2018UserID\u2019 takes 8 bytes. Each epoch time will need 4 bytes, and the Counter would need 2 bytes. Let\u2019s suppose we need a rate limiting of 500 requests per hour. Assume 20 bytes overhead for hash-table and 20 bytes for Redis hash. Since we\u2019ll keep a count for each minute, at max, we would need 60 entries for each user. We would need a total of 1.6KB to store one user\u2019s data: 8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB If we need to track one million users at any time, total memory we would need would be 1.6GB: 1.6KB * 1 million ~= 1.6GB So, our \u2018Sliding Window with Counters\u2019 algorithm uses 86% less memory than the simple sliding window algorithm.","title":"10. Sliding Window with Counters"},{"location":"DesigningAPIRateLimiter/#11-data-sharding-and-caching","text":"We can shard based on the \u2018UserID\u2019 to distribute the user\u2019s data. For fault tolerance and replication we should use Consistent Hashing. If we want to have different throttling limits for different APIs, we can choose to shard per user per API. Take the example of URL Shortener; we can have different rate limiter for createURL() and deleteURL() APIs for each user or IP. If our APIs are partitioned, a practical consideration could be to have a separate (somewhat smaller) rate limiter for each API shard as well. Let\u2019s take the example of our URL Shortener where we want to limit each user not to create more than 100 short URLs per hour. Assuming we are using Hash-Based Partitioning for our createURL() API, we can rate limit each partition to allow a user to create not more than three short URLs per minute in addition to 100 short URLs per hour. Our system can get huge benefits from caching recent active users. Application servers can quickly check if the cache has the desired record before hitting backend servers. Our rate limiter can significantly benefit from the Write-back cache by updating all counters and timestamps in cache only. The write to the permanent storage can be done at fixed intervals. This way we can ensure minimum latency added to the user\u2019s requests by the rate limiter. The reads can always hit the cache first; which will be extremely useful once the user has hit their maximum limit and the rate limiter will only be reading data without any updates. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.","title":"11. Data Sharding and Caching"},{"location":"DesigningAPIRateLimiter/#12-should-we-rate-limit-by-ip-or-by-user","text":"Let\u2019s discuss the pros and cons of using each one of these schemes: IP: In this scheme, we throttle requests per-IP; although it\u2019s not optimal in terms of differentiating between \u2018good\u2019 and \u2018bad\u2019 actors, it\u2019s still better than not have rate limiting at all. The biggest problem with IP based throttling is when multiple users share a single public IP like in an internet cafe or smartphone users that are using the same gateway. One bad user can cause throttling to other users. Another issue could arise while caching IP-based limits, as there are a huge number of IPv6 addresses available to a hacker from even one computer, it\u2019s trivial to make a server run out of memory tracking IPv6 addresses! User: Rate limiting can be done on APIs after user authentication. Once authenticated, the user will be provided with a token which the user will pass with each request. This will ensure that we will rate limit against a particular API that has a valid authentication token. But what if we have to rate limit on the login API itself? The weakness of this rate-limiting would be that a hacker can perform a denial of service attack against a user by entering wrong credentials up to the limit; after that the actual user will not be able to log-in. How about if we combine the above two schemes? Hybrid: A right approach could be to do both per-IP and per-user rate limiting, as they both have weaknesses when implemented alone, though, this will result in more cache entries with more details per entry, hence requiring more memory and storage.","title":"12. Should we rate limit by IP or by user?"},{"location":"DesigningDropbox/","text":"Designing Dropbox Let's design a file hosting service like Dropbox or Google Drive. Cloud file storage enables users to store their data on remote servers. Usually, these servers are maintained by cloud storage providers and made available to users over a network (typically through the Internet). Users pay for their cloud data storage on a monthly basis. Similar Services: OneDrive, Google Drive Difficulty Level: Medium 1. Why Cloud Storage? Cloud file storage services have become very popular recently as they simplify the storage and exchange of digital resources among multiple devices. The shift from using single personal computers to using multiple devices with different platforms and operating systems such as smartphones and tablets each with portable access from various geographical locations at any time, is believed to be accountable for the huge popularity of cloud storage services. Following are some of the top benefits of such services: Availability: The motto of cloud storage services is to have data availability anywhere, anytime. Users can access their files/photos from any device whenever and wherever they like. Reliability: and Durability: Another benefit of cloud storage is that it offers 100% reliability and durability of data. Cloud storage ensures that users will never lose their data by keeping multiple copies of the data stored on different geographically located servers. Scalability: Users will never have to worry about getting out of storage space. With cloud storage you have unlimited storage as long as you are ready to pay for it. If you haven\u2019t used dropbox.com before, we would highly recommend creating an account there and uploading/editing a file and also going through the different options their service offers. This will help you a lot in understanding this chapter. 2. Requirements and Goals of the System You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind. Users should be able to upload and download their files/photos from any device. Users should be able to share files or folders with other users. Our service should support automatic synchronization between devices, i.e., after updating a file on one device, it should get synchronized on all devices. The system should support storing large files up to a GB. ACID-ity is required. Atomicity, Consistency, Isolation and Durability of all file operations should be guaranteed. Our system should support offline editing. Users should be able to add/delete/modify files while offline, and as soon as they come online, all their changes should be synced to the remote servers and other online devices. Extended Requirements The system should support snapshotting of the data, so that users can go back to any version of the files. 3. Some Design Considerations We should expect huge read and write volumes. Read to write ratio is expected to be nearly the same. Internally, files can be stored in small parts or chunks (say 4MB); this can provide a lot of benefits i.e. all failed operations shall only be retried for smaller parts of a file. If a user fails to upload a file, then only the failing chunk will be retried. We can reduce the amount of data exchange by transferring updated chunks only. By removing duplicate chunks, we can save storage space and bandwidth usage. Keeping a local copy of the metadata (file name, size, etc.) with the client can save us a lot of round trips to the server. For small changes, clients can intelligently upload the diffs instead of the whole chunk. 4. Capacity Estimation and Constraints Let\u2019s assume that we have 500M total users, and 100M daily active users (DAU). Let\u2019s assume that on average each user connects from three different devices. On average if a user has 200 files/photos, we will have 100 billion total files. Let\u2019s assume that average file size is 100KB, this would give us ten petabytes of total storage. 100B * 100KB => 10PB Let\u2019s also assume that we will have one million active connections per minute. 5. High Level Design The user will specify a folder as the workspace on their device. Any file/photo/folder placed in this folder will be uploaded to the cloud, and whenever a file is modified or deleted, it will be reflected in the same way in the cloud storage. The user can specify similar workspaces on all their devices and any modification done on one device will be propagated to all other devices to have the same view of the workspace everywhere. At a high level, we need to store files and their metadata information like File Name, File Size, Directory, etc., and who this file is shared with. So, we need some servers that can help the clients to upload/download files to Cloud Storage and some servers that can facilitate updating metadata about files and users. We also need some mechanism to notify all clients whenever an update happens so they can synchronize their files. As shown in the diagram below, Block servers will work with the clients to upload/download files from cloud storage and Metadata servers will keep metadata of files updated in a SQL or NoSQL database. Synchronization servers will handle the workflow of notifying all clients about different changes for synchronization. 6. Component Design Let\u2019s go through the major components of our system one by one: a. Client The Client Application monitors the workspace folder on the user\u2019s machine and syncs all files/folders in it with the remote Cloud Storage. The client application will work with the storage servers to upload, download, and modify actual files to backend Cloud Storage. The client also interacts with the remote Synchronization Service to handle any file metadata updates, e.g., change in the file name, size, modification date, etc. Here are some of the essential operations for the client: Upload and download files. Detect file changes in the workspace folder. Handle conflict due to offline or concurrent updates. How do we handle file transfer efficiently? As mentioned above, we can break each file into smaller chunks so that we transfer only those chunks that are modified and not the whole file. Let\u2019s say we divide each file into fixed sizes of 4MB chunks. We can statically calculate what could be an optimal chunk size based on 1) Storage devices we use in the cloud to optimize space utilization and input/output operations per second (IOPS) 2) Network bandwidth 3) Average file size in the storage etc. In our metadata, we should also keep a record of each file and the chunks that constitute it. Should we keep a copy of metadata with Client? Keeping a local copy of metadata not only enable us to do offline updates but also saves a lot of round trips to update remote metadata. How can clients efficiently listen to changes happening with other clients? One solution could be that the clients periodically check with the server if there are any changes. The problem with this approach is that we will have a delay in reflecting changes locally as clients will be checking for changes periodically compared to a server notifying whenever there is some change. If the client frequently checks the server for changes, it will not only be wasting bandwidth, as the server has to return an empty response most of the time, but will also be keeping the server busy. Pulling information in this manner is not scalable. A solution to the above problem could be to use HTTP long polling. With long polling the client requests information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends an HTTP/S response to the client, completing the open HTTP/S Request. Upon receipt of the server response, the client can immediately issue another server request for future updates. Based on the above considerations, we can divide our client into following four parts: I. Internal Metadata Database will keep track of all the files, chunks, their versions, and their location in the file system. II. Chunker will split the files into smaller pieces called chunks. It will also be responsible for reconstructing a file from its chunks. Our chunking algorithm will detect the parts of the files that have been modified by the user and only transfer those parts to the Cloud Storage; this will save us bandwidth and synchronization time. III. Watcher will monitor the local workspace folders and notify the Indexer (discussed below) of any action performed by the users, e.g. when users create, delete, or update files or folders. Watcher also listens to any changes happening on other clients that are broadcasted by Synchronization service. IV. Indexer will process the events received from the Watcher and update the internal metadata database with information about the chunks of the modified files. Once the chunks are successfully submitted/downloaded to the Cloud Storage, the Indexer will communicate with the remote Synchronization Service to broadcast changes to other clients and update remote metadata database. How should clients handle slow servers? Clients should exponentially back-off if the server is busy/not-responding. Meaning, if a server is too slow to respond, clients should delay their retries and this delay should increase exponentially. Should mobile clients sync remote changes immediately? Unlike desktop or web clients, mobile clients usually sync on demand to save user\u2019s bandwidth and space. b. Metadata Database The Metadata Database is responsible for maintaining the versioning and metadata information about files/chunks, users, and workspaces. The Metadata Database can be a relational database such as MySQL, or a NoSQL database service such as DynamoDB. Regardless of the type of the database, the Synchronization Service should be able to provide a consistent view of the files using a database, especially if more than one user is working with the same file simultaneously. Since NoSQL data stores do not support ACID properties in favor of scalability and performance, we need to incorporate the support for ACID properties programmatically in the logic of our Synchronization Service in case we opt for this kind of database. However, using a relational database can simplify the implementation of the Synchronization Service as they natively support ACID properties. The metadata Database should be storing information about following objects: Chunks Files User Devices Workspace (sync folders) c. Synchronization Service The Synchronization Service is the component that processes file updates made by a client and applies these changes to other subscribed clients. It also synchronizes clients\u2019 local databases with the information stored in the remote Metadata DB. The Synchronization Service is the most important part of the system architecture due to its critical role in managing the metadata and synchronizing users\u2019 files. Desktop clients communicate with the Synchronization Service to either obtain updates from the Cloud Storage or send files and updates to the Cloud Storage and, potentially, other users. If a client was offline for a period, it polls the system for new updates as soon as they come online. When the Synchronization Service receives an update request, it checks with the Metadata Database for consistency and then proceeds with the update. Subsequently, a notification is sent to all subscribed users or devices to report the file update. The Synchronization Service should be designed in such a way that it transmits less data between clients and the Cloud Storage to achieve a better response time. To meet this design goal, the Synchronization Service can employ a differencing algorithm to reduce the amount of the data that needs to be synchronized. Instead of transmitting entire files from clients to the server or vice versa, we can just transmit the difference between two versions of a file. Therefore, only the part of the file that has been changed is transmitted. This also decreases bandwidth consumption and cloud data storage for the end user. As described above, we will be dividing our files into 4MB chunks and will be transferring modified chunks only. Server and clients can calculate a hash (e.g., SHA-256) to see whether to update the local copy of a chunk or not. On the server, if we already have a chunk with a similar hash (even from another user), we don\u2019t need to create another copy, we can use the same chunk. This is discussed in detail later under Data Deduplication. To be able to provide an efficient and scalable synchronization protocol we can consider using a communication middleware between clients and the Synchronization Service. The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies. This way, multiple Synchronization Service instances can receive requests from a global request Queue, and the communication middleware will be able to balance its load. d. Message Queuing Service An important part of our architecture is a messaging middleware that should be able to handle a substantial number of requests. A scalable Message Queuing Service that supports asynchronous message-based communication between clients and the Synchronization Service best fits the requirements of our application. The Message Queuing Service supports asynchronous and loosely coupled message-based communication between distributed components of the system. The Message Queuing Service should be able to efficiently store any number of messages in a highly available, reliable and scalable queue. The Message Queuing Service will implement two types of queues in our system. The Request Queue is a global queue and all clients will share it. Clients\u2019 requests to update the Metadata Database will be sent to the Request Queue first, from there the Synchronization Service will take it to update metadata. The Response Queues that correspond to individual subscribed clients are responsible for delivering the update messages to each client. Since a message will be deleted from the queue once received by a client, we need to create separate Response Queues for each subscribed client to share update messages. e. Cloud/Block Storage Cloud/Block Storage stores chunks of files uploaded by the users. Clients directly interact with the storage to send and receive objects from it. Separation of the metadata from storage enables us to use any storage either in the cloud or in-house. 7. File Processing Workflow The sequence below shows the interaction between the components of the application in a scenario when Client A updates a file that is shared with Client B and C, so they should receive the update too. If the other clients are not online at the time of the update, the Message Queuing Service keeps the update notifications in separate response queues for them until they come online later. Client A uploads chunks to cloud storage. Client A updates metadata and commits changes. Client A gets confirmation and notifications are sent to Clients B and C about the changes. Client B and C receive metadata changes and download updated chunks. 8. Data Deduplication Data deduplication is a technique used for eliminating duplicate copies of data to improve storage utilization. It can also be applied to network data transfers to reduce the number of bytes that must be sent. For each new incoming chunk, we can calculate a hash of it and compare that hash with all the hashes of the existing chunks to see if we already have the same chunk present in our storage. We can implement deduplication in two ways in our system: a. Post-process deduplication With post-process deduplication, new chunks are first stored on the storage device and later some process analyzes the data looking for duplication. The benefit is that clients will not need to wait for the hash calculation or lookup to complete before storing the data, thereby ensuring that there is no degradation in storage performance. Drawbacks of this approach are 1) We will unnecessarily be storing duplicate data, though for a short time, 2) Duplicate data will be transferred consuming bandwidth. b. In-line deduplication Alternatively, deduplication hash calculations can be done in real-time as the clients are entering data on their device. If our system identifies a chunk that it has already stored, only a reference to the existing chunk will be added in the metadata, rather than a full copy of the chunk. This approach will give us optimal network and storage usage 9. Metadata Partitioning To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks. We need to come up with a partitioning scheme that would divide and store our data in different DB servers. 1. Vertical Partitioning: We can partition our database in such a way that we store tables related to one particular feature on one server. For example, we can store all the user related tables in one database and all files/chunks related tables in another database. Although this approach is straightforward to implement it has some issues: Will we still have scale issues? What if we have trillions of chunks to be stored and our database cannot support storing such a huge number of records? How would we further partition such tables? Joining two tables in two separate databases can cause performance and consistency issues. How frequently do we have to join user and file tables? 2. Range Based Partitioning: What if we store files/chunks in separate partitions based on the first letter of the File Path? In that case, we save all the files starting with the letter \u2018A\u2019 in one partition and those that start with the letter \u2018B\u2019 into another partition and so on. This approach is called range based partitioning. We can even combine certain less frequently occurring letters into one database partition. We should come up with this partitioning scheme statically so that we can always store/find a file in a predictable manner. The main problem with this approach is that it can lead to unbalanced servers. For example, if we decide to put all files starting with the letter \u2018E\u2019 into a DB partition, and later we realize that we have too many files that start with the letter \u2018E\u2019, to such an extent that we cannot fit them into one DB partition. 3. Hash-Based Partitioning: In this scheme we take a hash of the object we are storing and based on this hash we figure out the DB partition to which this object should go. In our case, we can take the hash of the \u2018FileID\u2019 of the File object we are storing to determine the partition the file will be stored. Our hashing function will randomly distribute objects into different partitions, e.g., our hashing function can always map any ID to a number between [1\u2026256], and this number would be the partition we will store our object. This approach can still lead to overloaded partitions, which can be solved by using Consistent Hashing. 10. Caching We can have two kinds of caches in our system. To deal with hot files/chunks we can introduce a cache for Block storage. We can use an off-the-shelf solution like Memcached that can store whole chunks with its respective IDs/Hashes and Block servers before hitting Block storage can quickly check if the cache has desired chunk. Based on clients\u2019 usage pattern we can determine how many cache servers we need. A high-end commercial server can have 144GB of memory; one such server can cache 36K chunks. Which cache replacement policy would best fit our needs? When the cache is full, and we want to replace a chunk with a newer/hotter chunk, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently used chunk first. Load Similarly, we can have a cache for Metadata DB. 11. Load Balancer (LB) We can add the Load balancing layer at two places in our system: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers. Initially, a simple Round Robin approach can be adopted that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is, it won\u2019t take server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that. 12. Security, Permissions and File Sharing One of the primary concerns users will have while storing their files in the cloud is the privacy and security of their data, especially since in our system users can share their files with other users or even make them public to share it with everyone. To handle this, we will be storing the permissions of each file in our metadata DB to reflect what files are visible or modifiable by any user","title":"Designing Dropbox"},{"location":"DesigningDropbox/#designing-dropbox","text":"Let's design a file hosting service like Dropbox or Google Drive. Cloud file storage enables users to store their data on remote servers. Usually, these servers are maintained by cloud storage providers and made available to users over a network (typically through the Internet). Users pay for their cloud data storage on a monthly basis. Similar Services: OneDrive, Google Drive Difficulty Level: Medium","title":"Designing Dropbox"},{"location":"DesigningDropbox/#1-why-cloud-storage","text":"Cloud file storage services have become very popular recently as they simplify the storage and exchange of digital resources among multiple devices. The shift from using single personal computers to using multiple devices with different platforms and operating systems such as smartphones and tablets each with portable access from various geographical locations at any time, is believed to be accountable for the huge popularity of cloud storage services. Following are some of the top benefits of such services: Availability: The motto of cloud storage services is to have data availability anywhere, anytime. Users can access their files/photos from any device whenever and wherever they like. Reliability: and Durability: Another benefit of cloud storage is that it offers 100% reliability and durability of data. Cloud storage ensures that users will never lose their data by keeping multiple copies of the data stored on different geographically located servers. Scalability: Users will never have to worry about getting out of storage space. With cloud storage you have unlimited storage as long as you are ready to pay for it. If you haven\u2019t used dropbox.com before, we would highly recommend creating an account there and uploading/editing a file and also going through the different options their service offers. This will help you a lot in understanding this chapter.","title":"1. Why Cloud Storage?"},{"location":"DesigningDropbox/#2-requirements-and-goals-of-the-system","text":"You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind. Users should be able to upload and download their files/photos from any device. Users should be able to share files or folders with other users. Our service should support automatic synchronization between devices, i.e., after updating a file on one device, it should get synchronized on all devices. The system should support storing large files up to a GB. ACID-ity is required. Atomicity, Consistency, Isolation and Durability of all file operations should be guaranteed. Our system should support offline editing. Users should be able to add/delete/modify files while offline, and as soon as they come online, all their changes should be synced to the remote servers and other online devices. Extended Requirements The system should support snapshotting of the data, so that users can go back to any version of the files.","title":"2. Requirements and Goals of the System"},{"location":"DesigningDropbox/#3-some-design-considerations","text":"We should expect huge read and write volumes. Read to write ratio is expected to be nearly the same. Internally, files can be stored in small parts or chunks (say 4MB); this can provide a lot of benefits i.e. all failed operations shall only be retried for smaller parts of a file. If a user fails to upload a file, then only the failing chunk will be retried. We can reduce the amount of data exchange by transferring updated chunks only. By removing duplicate chunks, we can save storage space and bandwidth usage. Keeping a local copy of the metadata (file name, size, etc.) with the client can save us a lot of round trips to the server. For small changes, clients can intelligently upload the diffs instead of the whole chunk.","title":"3. Some Design Considerations"},{"location":"DesigningDropbox/#4-capacity-estimation-and-constraints","text":"Let\u2019s assume that we have 500M total users, and 100M daily active users (DAU). Let\u2019s assume that on average each user connects from three different devices. On average if a user has 200 files/photos, we will have 100 billion total files. Let\u2019s assume that average file size is 100KB, this would give us ten petabytes of total storage. 100B * 100KB => 10PB Let\u2019s also assume that we will have one million active connections per minute.","title":"4. Capacity Estimation and Constraints"},{"location":"DesigningDropbox/#5-high-level-design","text":"The user will specify a folder as the workspace on their device. Any file/photo/folder placed in this folder will be uploaded to the cloud, and whenever a file is modified or deleted, it will be reflected in the same way in the cloud storage. The user can specify similar workspaces on all their devices and any modification done on one device will be propagated to all other devices to have the same view of the workspace everywhere. At a high level, we need to store files and their metadata information like File Name, File Size, Directory, etc., and who this file is shared with. So, we need some servers that can help the clients to upload/download files to Cloud Storage and some servers that can facilitate updating metadata about files and users. We also need some mechanism to notify all clients whenever an update happens so they can synchronize their files. As shown in the diagram below, Block servers will work with the clients to upload/download files from cloud storage and Metadata servers will keep metadata of files updated in a SQL or NoSQL database. Synchronization servers will handle the workflow of notifying all clients about different changes for synchronization.","title":"5. High Level Design"},{"location":"DesigningDropbox/#6-component-design","text":"Let\u2019s go through the major components of our system one by one:","title":"6. Component Design"},{"location":"DesigningDropbox/#a-client","text":"The Client Application monitors the workspace folder on the user\u2019s machine and syncs all files/folders in it with the remote Cloud Storage. The client application will work with the storage servers to upload, download, and modify actual files to backend Cloud Storage. The client also interacts with the remote Synchronization Service to handle any file metadata updates, e.g., change in the file name, size, modification date, etc. Here are some of the essential operations for the client: Upload and download files. Detect file changes in the workspace folder. Handle conflict due to offline or concurrent updates. How do we handle file transfer efficiently? As mentioned above, we can break each file into smaller chunks so that we transfer only those chunks that are modified and not the whole file. Let\u2019s say we divide each file into fixed sizes of 4MB chunks. We can statically calculate what could be an optimal chunk size based on 1) Storage devices we use in the cloud to optimize space utilization and input/output operations per second (IOPS) 2) Network bandwidth 3) Average file size in the storage etc. In our metadata, we should also keep a record of each file and the chunks that constitute it. Should we keep a copy of metadata with Client? Keeping a local copy of metadata not only enable us to do offline updates but also saves a lot of round trips to update remote metadata. How can clients efficiently listen to changes happening with other clients? One solution could be that the clients periodically check with the server if there are any changes. The problem with this approach is that we will have a delay in reflecting changes locally as clients will be checking for changes periodically compared to a server notifying whenever there is some change. If the client frequently checks the server for changes, it will not only be wasting bandwidth, as the server has to return an empty response most of the time, but will also be keeping the server busy. Pulling information in this manner is not scalable. A solution to the above problem could be to use HTTP long polling. With long polling the client requests information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends an HTTP/S response to the client, completing the open HTTP/S Request. Upon receipt of the server response, the client can immediately issue another server request for future updates. Based on the above considerations, we can divide our client into following four parts: I. Internal Metadata Database will keep track of all the files, chunks, their versions, and their location in the file system. II. Chunker will split the files into smaller pieces called chunks. It will also be responsible for reconstructing a file from its chunks. Our chunking algorithm will detect the parts of the files that have been modified by the user and only transfer those parts to the Cloud Storage; this will save us bandwidth and synchronization time. III. Watcher will monitor the local workspace folders and notify the Indexer (discussed below) of any action performed by the users, e.g. when users create, delete, or update files or folders. Watcher also listens to any changes happening on other clients that are broadcasted by Synchronization service. IV. Indexer will process the events received from the Watcher and update the internal metadata database with information about the chunks of the modified files. Once the chunks are successfully submitted/downloaded to the Cloud Storage, the Indexer will communicate with the remote Synchronization Service to broadcast changes to other clients and update remote metadata database. How should clients handle slow servers? Clients should exponentially back-off if the server is busy/not-responding. Meaning, if a server is too slow to respond, clients should delay their retries and this delay should increase exponentially. Should mobile clients sync remote changes immediately? Unlike desktop or web clients, mobile clients usually sync on demand to save user\u2019s bandwidth and space.","title":"a. Client"},{"location":"DesigningDropbox/#b-metadata-database","text":"The Metadata Database is responsible for maintaining the versioning and metadata information about files/chunks, users, and workspaces. The Metadata Database can be a relational database such as MySQL, or a NoSQL database service such as DynamoDB. Regardless of the type of the database, the Synchronization Service should be able to provide a consistent view of the files using a database, especially if more than one user is working with the same file simultaneously. Since NoSQL data stores do not support ACID properties in favor of scalability and performance, we need to incorporate the support for ACID properties programmatically in the logic of our Synchronization Service in case we opt for this kind of database. However, using a relational database can simplify the implementation of the Synchronization Service as they natively support ACID properties. The metadata Database should be storing information about following objects: Chunks Files User Devices Workspace (sync folders)","title":"b. Metadata Database"},{"location":"DesigningDropbox/#c-synchronization-service","text":"The Synchronization Service is the component that processes file updates made by a client and applies these changes to other subscribed clients. It also synchronizes clients\u2019 local databases with the information stored in the remote Metadata DB. The Synchronization Service is the most important part of the system architecture due to its critical role in managing the metadata and synchronizing users\u2019 files. Desktop clients communicate with the Synchronization Service to either obtain updates from the Cloud Storage or send files and updates to the Cloud Storage and, potentially, other users. If a client was offline for a period, it polls the system for new updates as soon as they come online. When the Synchronization Service receives an update request, it checks with the Metadata Database for consistency and then proceeds with the update. Subsequently, a notification is sent to all subscribed users or devices to report the file update. The Synchronization Service should be designed in such a way that it transmits less data between clients and the Cloud Storage to achieve a better response time. To meet this design goal, the Synchronization Service can employ a differencing algorithm to reduce the amount of the data that needs to be synchronized. Instead of transmitting entire files from clients to the server or vice versa, we can just transmit the difference between two versions of a file. Therefore, only the part of the file that has been changed is transmitted. This also decreases bandwidth consumption and cloud data storage for the end user. As described above, we will be dividing our files into 4MB chunks and will be transferring modified chunks only. Server and clients can calculate a hash (e.g., SHA-256) to see whether to update the local copy of a chunk or not. On the server, if we already have a chunk with a similar hash (even from another user), we don\u2019t need to create another copy, we can use the same chunk. This is discussed in detail later under Data Deduplication. To be able to provide an efficient and scalable synchronization protocol we can consider using a communication middleware between clients and the Synchronization Service. The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies. This way, multiple Synchronization Service instances can receive requests from a global request Queue, and the communication middleware will be able to balance its load.","title":"c. Synchronization Service"},{"location":"DesigningDropbox/#d-message-queuing-service","text":"An important part of our architecture is a messaging middleware that should be able to handle a substantial number of requests. A scalable Message Queuing Service that supports asynchronous message-based communication between clients and the Synchronization Service best fits the requirements of our application. The Message Queuing Service supports asynchronous and loosely coupled message-based communication between distributed components of the system. The Message Queuing Service should be able to efficiently store any number of messages in a highly available, reliable and scalable queue. The Message Queuing Service will implement two types of queues in our system. The Request Queue is a global queue and all clients will share it. Clients\u2019 requests to update the Metadata Database will be sent to the Request Queue first, from there the Synchronization Service will take it to update metadata. The Response Queues that correspond to individual subscribed clients are responsible for delivering the update messages to each client. Since a message will be deleted from the queue once received by a client, we need to create separate Response Queues for each subscribed client to share update messages.","title":"d. Message Queuing Service"},{"location":"DesigningDropbox/#e-cloudblock-storage","text":"Cloud/Block Storage stores chunks of files uploaded by the users. Clients directly interact with the storage to send and receive objects from it. Separation of the metadata from storage enables us to use any storage either in the cloud or in-house.","title":"e. Cloud/Block Storage"},{"location":"DesigningDropbox/#7-file-processing-workflow","text":"","title":"7. File Processing Workflow"},{"location":"DesigningDropbox/#_1","text":"The sequence below shows the interaction between the components of the application in a scenario when Client A updates a file that is shared with Client B and C, so they should receive the update too. If the other clients are not online at the time of the update, the Message Queuing Service keeps the update notifications in separate response queues for them until they come online later. Client A uploads chunks to cloud storage. Client A updates metadata and commits changes. Client A gets confirmation and notifications are sent to Clients B and C about the changes. Client B and C receive metadata changes and download updated chunks.","title":""},{"location":"DesigningDropbox/#8-data-deduplication","text":"Data deduplication is a technique used for eliminating duplicate copies of data to improve storage utilization. It can also be applied to network data transfers to reduce the number of bytes that must be sent. For each new incoming chunk, we can calculate a hash of it and compare that hash with all the hashes of the existing chunks to see if we already have the same chunk present in our storage. We can implement deduplication in two ways in our system: a. Post-process deduplication With post-process deduplication, new chunks are first stored on the storage device and later some process analyzes the data looking for duplication. The benefit is that clients will not need to wait for the hash calculation or lookup to complete before storing the data, thereby ensuring that there is no degradation in storage performance. Drawbacks of this approach are 1) We will unnecessarily be storing duplicate data, though for a short time, 2) Duplicate data will be transferred consuming bandwidth. b. In-line deduplication Alternatively, deduplication hash calculations can be done in real-time as the clients are entering data on their device. If our system identifies a chunk that it has already stored, only a reference to the existing chunk will be added in the metadata, rather than a full copy of the chunk. This approach will give us optimal network and storage usage","title":"8. Data Deduplication"},{"location":"DesigningDropbox/#9-metadata-partitioning","text":"","title":"9. Metadata Partitioning"},{"location":"DesigningDropbox/#_2","text":"To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks. We need to come up with a partitioning scheme that would divide and store our data in different DB servers. 1. Vertical Partitioning: We can partition our database in such a way that we store tables related to one particular feature on one server. For example, we can store all the user related tables in one database and all files/chunks related tables in another database. Although this approach is straightforward to implement it has some issues: Will we still have scale issues? What if we have trillions of chunks to be stored and our database cannot support storing such a huge number of records? How would we further partition such tables? Joining two tables in two separate databases can cause performance and consistency issues. How frequently do we have to join user and file tables? 2. Range Based Partitioning: What if we store files/chunks in separate partitions based on the first letter of the File Path? In that case, we save all the files starting with the letter \u2018A\u2019 in one partition and those that start with the letter \u2018B\u2019 into another partition and so on. This approach is called range based partitioning. We can even combine certain less frequently occurring letters into one database partition. We should come up with this partitioning scheme statically so that we can always store/find a file in a predictable manner. The main problem with this approach is that it can lead to unbalanced servers. For example, if we decide to put all files starting with the letter \u2018E\u2019 into a DB partition, and later we realize that we have too many files that start with the letter \u2018E\u2019, to such an extent that we cannot fit them into one DB partition. 3. Hash-Based Partitioning: In this scheme we take a hash of the object we are storing and based on this hash we figure out the DB partition to which this object should go. In our case, we can take the hash of the \u2018FileID\u2019 of the File object we are storing to determine the partition the file will be stored. Our hashing function will randomly distribute objects into different partitions, e.g., our hashing function can always map any ID to a number between [1\u2026256], and this number would be the partition we will store our object. This approach can still lead to overloaded partitions, which can be solved by using Consistent Hashing.","title":""},{"location":"DesigningDropbox/#10-caching","text":"","title":"10. Caching"},{"location":"DesigningDropbox/#_3","text":"We can have two kinds of caches in our system. To deal with hot files/chunks we can introduce a cache for Block storage. We can use an off-the-shelf solution like Memcached that can store whole chunks with its respective IDs/Hashes and Block servers before hitting Block storage can quickly check if the cache has desired chunk. Based on clients\u2019 usage pattern we can determine how many cache servers we need. A high-end commercial server can have 144GB of memory; one such server can cache 36K chunks. Which cache replacement policy would best fit our needs? When the cache is full, and we want to replace a chunk with a newer/hotter chunk, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently used chunk first. Load Similarly, we can have a cache for Metadata DB.","title":""},{"location":"DesigningDropbox/#11-load-balancer-lb","text":"","title":"11. Load Balancer (LB)"},{"location":"DesigningDropbox/#_4","text":"We can add the Load balancing layer at two places in our system: 1) Between Clients and Block servers and 2) Between Clients and Metadata servers. Initially, a simple Round Robin approach can be adopted that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is, it won\u2019t take server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.","title":""},{"location":"DesigningDropbox/#12-security-permissions-and-file-sharing","text":"","title":"12. Security, Permissions and File Sharing"},{"location":"DesigningDropbox/#_5","text":"One of the primary concerns users will have while storing their files in the cloud is the privacy and security of their data, especially since in our system users can share their files with other users or even make them public to share it with everyone. To handle this, we will be storing the permissions of each file in our metadata DB to reflect what files are visible or modifiable by any user","title":""},{"location":"DesigningFacebookMessenger/","text":"Designing Facebook Messenger Let's design an instant messaging service like Facebook Messenger where users can send text messages to each other through web and mobile interfaces. 1. What is Facebook Messenger? Facebook Messenger is a software application which provides text-based instant messaging services to its users. Messenger users can chat with their Facebook friends both from cell-phones and Facebook\u2019s website. 2. Requirements and Goals of the System Our Messenger should meet the following requirements: Functional Requirements: Messenger should support one-on-one conversations between users. Messenger should keep track of the online/offline statuses of its users. Messenger should support the persistent storage of chat history. Non-functional Requirements: Users should have real-time chat experience with minimum latency. Our system should be highly consistent; users should be able to see the same chat history on all their devices. Messenger\u2019s high availability is desirable; we can tolerate lower availability in the interest of consistency. Extended Requirements: Group Chats: Messenger should support multiple people talking to each other in a group. Push notifications: Messenger should be able to notify users of new messages when they are offline. 3. Capacity Estimation and Constraints Let\u2019s assume that we have 500 million daily active users and on average each user sends 40 messages daily; this gives us 20 billion messages per day. Storage Estimation: Let\u2019s assume that on average a message is 100 bytes, so to store all the messages for one day we would need 2TB of storage. 20 billion messages * 100 bytes => 2 TB/day To store five years of chat history, we would need 3.6 petabytes of storage. 2 TB * 365 days * 5 years ~= 3.6 PB Other than the chat messages, we would also need to store users\u2019 information, messages\u2019 metadata (ID, Timestamp, etc.). Not to mention, the above calculation doesn\u2019t take data compression and replication into consideration. Bandwidth Estimation: If our service is getting 2TB of data every day, this will give us 25MB of incoming data for each second. 2 TB / 86400 sec ~= 25 MB/s Since each incoming message needs to go out to another user, we will need the same amount of bandwidth 25MB/s for both upload and download. High level estimates: Total messages 20 billion per day Storage for each day 2TB Storage for 5 years 3.6PB Incomming data 25MB/s Outgoing data 25MB/s 4. High Level Design At a high-level, we will need a chat server that will be the central piece, orchestrating all the communications between users. When a user wants to send a message to another user, they will connect to the chat server and send the message to the server; the server then passes that message to the other user and also stores it in the database. The detailed workflow would look like this: User-A sends a message to User-B through the chat server. The server receives the message and sends an acknowledgment to User-A. The server stores the message in its database and sends the message to User-B. User-B receives the message and sends the acknowledgment to the server. The server notifies User-A that the message has been delivered successfully to User-B. 5. Detailed Component Design Let\u2019s try to build a simple solution first where everything runs on one server. At the high level our system needs to handle the following use cases: Receive incoming messages and deliver outgoing messages. Store and retrieve messages from the database. Keep a record of which user is online or has gone offline, and notify all the relevant users about these status changes. Let\u2019s talk about these scenarios one by one: a. Messages Handling How would we efficiently send/receive messages? To send messages, a user needs to connect to the server and post messages for the other users. To get a message from the server, the user has two options: Pull model: Users can periodically ask the server if there are any new messages for them. Push model: Users can keep a connection open with the server and can depend upon the server to notify them whenever there are new messages. If we go with our first approach, then the server needs to keep track of messages that are still waiting to be delivered, and as soon as the receiving user connects to the server to ask for any new message, the server can return all the pending messages. To minimize latency for the user, they have to check the server quite frequently, and most of the time they will be getting an empty response if there are no pending message. This will waste a lot of resources and does not look like an efficient solution. If we go with our second approach, where all the active users keep a connection open with the server, then as soon as the server receives a message it can immediately pass the message to the intended user. This way, the server does not need to keep track of the pending messages, and we will have minimum latency, as the messages are delivered instantly on the opened connection. How will clients maintain an open connection with the server? We can use HTTP Long Polling or WebSockets. In long polling, clients can request information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends the response to the client, completing the open request. Upon receipt of the server response, the client can immediately issue another server request for future updates. This gives a lot of improvements in latencies, throughputs, and performance. The long polling request can timeout or can receive a disconnect from the server, in that case, the client has to open a new request. How can the server keep track of all the opened connection to redirect messages to the users efficiently? The server can maintain a hash table, where \u201ckey\u201d would be the UserID and \u201cvalue\u201d would be the connection object. So whenever the server receives a message for a user, it looks up that user in the hash table to find the connection object and sends the message on the open request. What will happen when the server receives a message for a user who has gone offline? If the receiver has disconnected, the server can notify the sender about the delivery failure. If it is a temporary disconnect, e.g., the receiver\u2019s long-poll request just timed out, then we should expect a reconnect from the user. In that case, we can ask the sender to retry sending the message. This retry could be embedded in the client\u2019s logic so that users don\u2019t have to retype the message. The server can also store the message for a while and retry sending it once the receiver reconnects. How many chat servers we need? Let\u2019s plan for 500 million connections at any time. Assuming a modern server can handle 50K concurrent connections at any time, we would need 10K such servers. How do we know which server holds the connection to which user? We can introduce a software load balancer in front of our chat servers; that can map each UserID to a server to redirect the request. How should the server process a \u2018deliver message\u2019 request? The server needs to do the following things upon receiving a new message: 1) Store the message in the database 2) Send the message to the receiver and 3) Send an acknowledgment to the sender. The chat server will first find the server that holds the connection for the receiver and pass the message to that server to send it to the receiver. The chat server can then send the acknowledgment to the sender; we don\u2019t need to wait for storing the message in the database (this can happen in the background). Storing the message is discussed in the next section. How does the messenger maintain the sequencing of the messages? We can store a timestamp with each message, which is the time the message is received by the server. This will still not ensure correct ordering of messages for clients. The scenario where the server timestamp cannot determine the exact order of messages would look like this: User-1 sends a message M1 to the server for User-2. The server receives M1 at T1. Meanwhile, User-2 sends a message M2 to the server for User-1. The server receives the message M2 at T2, such that T2 > T1. The server sends message M1 to User-2 and M2 to User-1. So User-1 will see M1 first and then M2, whereas User-2 will see M2 first and then M1. To resolve this, we need to keep a sequence number with every message for each client. This sequence number will determine the exact ordering of messages for EACH user. With this solution, both clients will see a different view of the message sequence, but this view will be consistent for them on all devices. b. Storing and retrieving the messages from the database Whenever the chat server receives a new message, it needs to store it in the database. To do so, we have two options: Start a separate thread, which will work with the database to store the message. Send an asynchronous request to the database to store the message. We have to keep certain things in mind while designing our database: How to efficiently work with the database connection pool. How to retry failed requests. Where to log those requests that failed even after some retries. How to retry these logged requests (that failed after the retry) when all the issues have resolved. Which storage system we should use? We need to have a database that can support a very high rate of small updates and also fetch a range of records quickly. This is required because we have a huge number of small messages that need to be inserted in the database and, while querying, a user is mostly interested in sequentially accessing the messages. We cannot use RDBMS like MySQL or NoSQL like MongoDB because we cannot afford to read/write a row from the database every time a user receives/sends a message. This will not only make the basic operations of our service run with high latency but also create a huge load on databases. Both of our requirements can be easily met with a wide-column database solution like HBase. HBase is a column-oriented key-value NoSQL database that can store multiple values against one key into multiple columns. HBase is modeled after Google\u2019s BigTable and runs on top of Hadoop Distributed File System (HDFS). HBase groups data together to store new data in a memory buffer and, once the buffer is full, it dumps the data to the disk. This way of storage not only helps to store a lot of small data quickly but also fetching rows by the key or scanning ranges of rows. HBase is also an efficient database to store variable sized data, which is also required by our service. How should clients efficiently fetch data from the server? Clients should paginate while fetching data from the server. Page size could be different for different clients, e.g., cell phones have smaller screens, so we need a fewer number of message/conversations in the viewport. c. Managing user\u2019s status We need to keep track of user\u2019s online/offline status and notify all the relevant users whenever a status change happens. Since we are maintaining a connection object on the server for all active users, we can easily figure out the user\u2019s current status from this. With 500M active users at any time, if we have to broadcast each status change to all the relevant active users, it will consume a lot of resources. We can do the following optimization around this: Whenever a client starts the app, it can pull the current status of all users in their friends\u2019 list. Whenever a user sends a message to another user that has gone offline, we can send a failure to the sender and update the status on the client. Whenever a user comes online, the server can always broadcast that status with a delay of a few seconds to see if the user does not go offline immediately. Clients can pull the status from the server about those users that are being shown on the user\u2019s viewport. This should not be a frequent operation, as the server is broadcasting the online status of users and we can live with the stale offline status of users for a while. Whenever the client starts a new chat with another user, we can pull the status at that time. Design Summary: Clients will open a connection to the chat server to send a message; the server will then pass it to the requested user. All the active users will keep a connection open with the server to receive messages. Whenever a new message arrives, the chat server will push it to the receiving user on the long poll request. Messages can be stored in HBase, which supports quick small updates, and range based searches. The servers can broadcast the online status of a user to other relevant users. Clients can pull status updates for users who are visible in the client\u2019s viewport on a less frequent basis. 6. Data partitioning Since we will be storing a lot of data (3.6PB for five years), we need to distribute it onto multiple database servers. What will be our partitioning scheme? Partitioning based on UserID: Let\u2019s assume we partition based on the hash of the UserID so that we can keep all messages of a user on the same database. If one DB shard is 4TB, we will have \u201c3.6PB/4TB ~= 900\u201d shards for five years. For simplicity, let\u2019s assume we keep 1K shards. So we will find the shard number by \u201chash(UserID) % 1000\u201d and then store/retrieve the data from there. This partitioning scheme will also be very quick to fetch chat history for any user. In the beginning, we can start with fewer database servers with multiple shards residing on one physical server. Since we can have multiple database instances on a server, we can easily store multiple partitions on a single server. Our hash function needs to understand this logical partitioning scheme so that it can map multiple logical partitions on one physical server. Since we will store an unlimited history of messages, we can start with a big number of logical partitions, which will be mapped to fewer physical servers, and as our storage demand increases, we can add more physical servers to distribute our logical partitions. Partitioning based on MessageID: If we store different messages of a user on separate database shards, fetching a range of messages of a chat would be very slow, so we should not adopt this scheme. 7. Cache We can cache a few recent messages (say last 15) in a few recent conversations that are visible in a user\u2019s viewport (say last 5). Since we decided to store all of the user\u2019s messages on one shard, the cache for a user should entirely reside on one machine too. 8. Load balancing We will need a load balancer in front of our chat servers; that can map each UserID to a server that holds the connection for the user and then direct the request to that server. Similarly, we would need a load balancer for our cache servers. 9. Fault tolerance and Replication What will happen when a chat server fails? Our chat servers are holding connections with the users. If a server goes down, should we devise a mechanism to transfer those connections to some other server? It\u2019s extremely hard to failover TCP connections to other servers; an easier approach can be to have clients automatically reconnect if the connection is lost. Should we store multiple copies of user messages? We cannot have only one copy of the user\u2019s data, because if the server holding the data crashes or is down permanently, we don\u2019t have any mechanism to recover that data. For this, either we have to store multiple copies of the data on different servers or use techniques like Reed-Solomon encoding to distribute and replicate it. 10. Extended Requirements a. Group chat We can have separate group-chat objects in our system that can be stored on the chat servers. A group-chat object is identified by GroupChatID and will also maintain a list of people who are part of that chat. Our load balancer can direct each group chat message based on GroupChatID and the server handling that group chat can iterate through all the users of the chat to find the server handling the connection of each user to deliver the message. In databases, we can store all the group chats in a separate table partitioned based on GroupChatID. b. Push notifications In our current design, users can only send messages to active users and if the receiving user is offline, we send a failure to the sending user. Push notifications will enable our system to send messages to offline users. For Push notifications, each user can opt-in from their device (or a web browser) to get notifications whenever there is a new message or event. Each manufacturer maintains a set of servers that handles pushing these notifications to the user. To have push notifications in our system, we would need to set up a Notification server, which will take the messages for offline users and send them to the manufacture\u2019s push notification server, which will then send them to the user\u2019s device.","title":"Designing Facebook Messenger"},{"location":"DesigningFacebookMessenger/#designing-facebook-messenger","text":"Let's design an instant messaging service like Facebook Messenger where users can send text messages to each other through web and mobile interfaces.","title":"Designing Facebook Messenger"},{"location":"DesigningFacebookMessenger/#1-what-is-facebook-messenger","text":"Facebook Messenger is a software application which provides text-based instant messaging services to its users. Messenger users can chat with their Facebook friends both from cell-phones and Facebook\u2019s website.","title":"1. What is Facebook Messenger?"},{"location":"DesigningFacebookMessenger/#2-requirements-and-goals-of-the-system","text":"Our Messenger should meet the following requirements: Functional Requirements: Messenger should support one-on-one conversations between users. Messenger should keep track of the online/offline statuses of its users. Messenger should support the persistent storage of chat history. Non-functional Requirements: Users should have real-time chat experience with minimum latency. Our system should be highly consistent; users should be able to see the same chat history on all their devices. Messenger\u2019s high availability is desirable; we can tolerate lower availability in the interest of consistency. Extended Requirements: Group Chats: Messenger should support multiple people talking to each other in a group. Push notifications: Messenger should be able to notify users of new messages when they are offline.","title":"2. Requirements and Goals of the System"},{"location":"DesigningFacebookMessenger/#3-capacity-estimation-and-constraints","text":"Let\u2019s assume that we have 500 million daily active users and on average each user sends 40 messages daily; this gives us 20 billion messages per day. Storage Estimation: Let\u2019s assume that on average a message is 100 bytes, so to store all the messages for one day we would need 2TB of storage. 20 billion messages * 100 bytes => 2 TB/day To store five years of chat history, we would need 3.6 petabytes of storage. 2 TB * 365 days * 5 years ~= 3.6 PB Other than the chat messages, we would also need to store users\u2019 information, messages\u2019 metadata (ID, Timestamp, etc.). Not to mention, the above calculation doesn\u2019t take data compression and replication into consideration. Bandwidth Estimation: If our service is getting 2TB of data every day, this will give us 25MB of incoming data for each second. 2 TB / 86400 sec ~= 25 MB/s Since each incoming message needs to go out to another user, we will need the same amount of bandwidth 25MB/s for both upload and download. High level estimates: Total messages 20 billion per day Storage for each day 2TB Storage for 5 years 3.6PB Incomming data 25MB/s Outgoing data 25MB/s","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningFacebookMessenger/#4-high-level-design","text":"At a high-level, we will need a chat server that will be the central piece, orchestrating all the communications between users. When a user wants to send a message to another user, they will connect to the chat server and send the message to the server; the server then passes that message to the other user and also stores it in the database. The detailed workflow would look like this: User-A sends a message to User-B through the chat server. The server receives the message and sends an acknowledgment to User-A. The server stores the message in its database and sends the message to User-B. User-B receives the message and sends the acknowledgment to the server. The server notifies User-A that the message has been delivered successfully to User-B.","title":"4. High Level Design"},{"location":"DesigningFacebookMessenger/#5-detailed-component-design","text":"Let\u2019s try to build a simple solution first where everything runs on one server. At the high level our system needs to handle the following use cases: Receive incoming messages and deliver outgoing messages. Store and retrieve messages from the database. Keep a record of which user is online or has gone offline, and notify all the relevant users about these status changes. Let\u2019s talk about these scenarios one by one: a. Messages Handling How would we efficiently send/receive messages? To send messages, a user needs to connect to the server and post messages for the other users. To get a message from the server, the user has two options: Pull model: Users can periodically ask the server if there are any new messages for them. Push model: Users can keep a connection open with the server and can depend upon the server to notify them whenever there are new messages. If we go with our first approach, then the server needs to keep track of messages that are still waiting to be delivered, and as soon as the receiving user connects to the server to ask for any new message, the server can return all the pending messages. To minimize latency for the user, they have to check the server quite frequently, and most of the time they will be getting an empty response if there are no pending message. This will waste a lot of resources and does not look like an efficient solution. If we go with our second approach, where all the active users keep a connection open with the server, then as soon as the server receives a message it can immediately pass the message to the intended user. This way, the server does not need to keep track of the pending messages, and we will have minimum latency, as the messages are delivered instantly on the opened connection. How will clients maintain an open connection with the server? We can use HTTP Long Polling or WebSockets. In long polling, clients can request information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends the response to the client, completing the open request. Upon receipt of the server response, the client can immediately issue another server request for future updates. This gives a lot of improvements in latencies, throughputs, and performance. The long polling request can timeout or can receive a disconnect from the server, in that case, the client has to open a new request. How can the server keep track of all the opened connection to redirect messages to the users efficiently? The server can maintain a hash table, where \u201ckey\u201d would be the UserID and \u201cvalue\u201d would be the connection object. So whenever the server receives a message for a user, it looks up that user in the hash table to find the connection object and sends the message on the open request. What will happen when the server receives a message for a user who has gone offline? If the receiver has disconnected, the server can notify the sender about the delivery failure. If it is a temporary disconnect, e.g., the receiver\u2019s long-poll request just timed out, then we should expect a reconnect from the user. In that case, we can ask the sender to retry sending the message. This retry could be embedded in the client\u2019s logic so that users don\u2019t have to retype the message. The server can also store the message for a while and retry sending it once the receiver reconnects. How many chat servers we need? Let\u2019s plan for 500 million connections at any time. Assuming a modern server can handle 50K concurrent connections at any time, we would need 10K such servers. How do we know which server holds the connection to which user? We can introduce a software load balancer in front of our chat servers; that can map each UserID to a server to redirect the request. How should the server process a \u2018deliver message\u2019 request? The server needs to do the following things upon receiving a new message: 1) Store the message in the database 2) Send the message to the receiver and 3) Send an acknowledgment to the sender. The chat server will first find the server that holds the connection for the receiver and pass the message to that server to send it to the receiver. The chat server can then send the acknowledgment to the sender; we don\u2019t need to wait for storing the message in the database (this can happen in the background). Storing the message is discussed in the next section. How does the messenger maintain the sequencing of the messages? We can store a timestamp with each message, which is the time the message is received by the server. This will still not ensure correct ordering of messages for clients. The scenario where the server timestamp cannot determine the exact order of messages would look like this: User-1 sends a message M1 to the server for User-2. The server receives M1 at T1. Meanwhile, User-2 sends a message M2 to the server for User-1. The server receives the message M2 at T2, such that T2 > T1. The server sends message M1 to User-2 and M2 to User-1. So User-1 will see M1 first and then M2, whereas User-2 will see M2 first and then M1. To resolve this, we need to keep a sequence number with every message for each client. This sequence number will determine the exact ordering of messages for EACH user. With this solution, both clients will see a different view of the message sequence, but this view will be consistent for them on all devices. b. Storing and retrieving the messages from the database Whenever the chat server receives a new message, it needs to store it in the database. To do so, we have two options: Start a separate thread, which will work with the database to store the message. Send an asynchronous request to the database to store the message. We have to keep certain things in mind while designing our database: How to efficiently work with the database connection pool. How to retry failed requests. Where to log those requests that failed even after some retries. How to retry these logged requests (that failed after the retry) when all the issues have resolved. Which storage system we should use? We need to have a database that can support a very high rate of small updates and also fetch a range of records quickly. This is required because we have a huge number of small messages that need to be inserted in the database and, while querying, a user is mostly interested in sequentially accessing the messages. We cannot use RDBMS like MySQL or NoSQL like MongoDB because we cannot afford to read/write a row from the database every time a user receives/sends a message. This will not only make the basic operations of our service run with high latency but also create a huge load on databases. Both of our requirements can be easily met with a wide-column database solution like HBase. HBase is a column-oriented key-value NoSQL database that can store multiple values against one key into multiple columns. HBase is modeled after Google\u2019s BigTable and runs on top of Hadoop Distributed File System (HDFS). HBase groups data together to store new data in a memory buffer and, once the buffer is full, it dumps the data to the disk. This way of storage not only helps to store a lot of small data quickly but also fetching rows by the key or scanning ranges of rows. HBase is also an efficient database to store variable sized data, which is also required by our service. How should clients efficiently fetch data from the server? Clients should paginate while fetching data from the server. Page size could be different for different clients, e.g., cell phones have smaller screens, so we need a fewer number of message/conversations in the viewport. c. Managing user\u2019s status We need to keep track of user\u2019s online/offline status and notify all the relevant users whenever a status change happens. Since we are maintaining a connection object on the server for all active users, we can easily figure out the user\u2019s current status from this. With 500M active users at any time, if we have to broadcast each status change to all the relevant active users, it will consume a lot of resources. We can do the following optimization around this: Whenever a client starts the app, it can pull the current status of all users in their friends\u2019 list. Whenever a user sends a message to another user that has gone offline, we can send a failure to the sender and update the status on the client. Whenever a user comes online, the server can always broadcast that status with a delay of a few seconds to see if the user does not go offline immediately. Clients can pull the status from the server about those users that are being shown on the user\u2019s viewport. This should not be a frequent operation, as the server is broadcasting the online status of users and we can live with the stale offline status of users for a while. Whenever the client starts a new chat with another user, we can pull the status at that time. Design Summary: Clients will open a connection to the chat server to send a message; the server will then pass it to the requested user. All the active users will keep a connection open with the server to receive messages. Whenever a new message arrives, the chat server will push it to the receiving user on the long poll request. Messages can be stored in HBase, which supports quick small updates, and range based searches. The servers can broadcast the online status of a user to other relevant users. Clients can pull status updates for users who are visible in the client\u2019s viewport on a less frequent basis.","title":"5. Detailed Component Design"},{"location":"DesigningFacebookMessenger/#6-data-partitioning","text":"Since we will be storing a lot of data (3.6PB for five years), we need to distribute it onto multiple database servers. What will be our partitioning scheme? Partitioning based on UserID: Let\u2019s assume we partition based on the hash of the UserID so that we can keep all messages of a user on the same database. If one DB shard is 4TB, we will have \u201c3.6PB/4TB ~= 900\u201d shards for five years. For simplicity, let\u2019s assume we keep 1K shards. So we will find the shard number by \u201chash(UserID) % 1000\u201d and then store/retrieve the data from there. This partitioning scheme will also be very quick to fetch chat history for any user. In the beginning, we can start with fewer database servers with multiple shards residing on one physical server. Since we can have multiple database instances on a server, we can easily store multiple partitions on a single server. Our hash function needs to understand this logical partitioning scheme so that it can map multiple logical partitions on one physical server. Since we will store an unlimited history of messages, we can start with a big number of logical partitions, which will be mapped to fewer physical servers, and as our storage demand increases, we can add more physical servers to distribute our logical partitions. Partitioning based on MessageID: If we store different messages of a user on separate database shards, fetching a range of messages of a chat would be very slow, so we should not adopt this scheme.","title":"6. Data partitioning"},{"location":"DesigningFacebookMessenger/#7-cache","text":"We can cache a few recent messages (say last 15) in a few recent conversations that are visible in a user\u2019s viewport (say last 5). Since we decided to store all of the user\u2019s messages on one shard, the cache for a user should entirely reside on one machine too.","title":"7. Cache"},{"location":"DesigningFacebookMessenger/#8-load-balancing","text":"We will need a load balancer in front of our chat servers; that can map each UserID to a server that holds the connection for the user and then direct the request to that server. Similarly, we would need a load balancer for our cache servers.","title":"8. Load balancing"},{"location":"DesigningFacebookMessenger/#9-fault-tolerance-and-replication","text":"What will happen when a chat server fails? Our chat servers are holding connections with the users. If a server goes down, should we devise a mechanism to transfer those connections to some other server? It\u2019s extremely hard to failover TCP connections to other servers; an easier approach can be to have clients automatically reconnect if the connection is lost. Should we store multiple copies of user messages? We cannot have only one copy of the user\u2019s data, because if the server holding the data crashes or is down permanently, we don\u2019t have any mechanism to recover that data. For this, either we have to store multiple copies of the data on different servers or use techniques like Reed-Solomon encoding to distribute and replicate it.","title":"9. Fault tolerance and Replication"},{"location":"DesigningFacebookMessenger/#10-extended-requirements","text":"a. Group chat We can have separate group-chat objects in our system that can be stored on the chat servers. A group-chat object is identified by GroupChatID and will also maintain a list of people who are part of that chat. Our load balancer can direct each group chat message based on GroupChatID and the server handling that group chat can iterate through all the users of the chat to find the server handling the connection of each user to deliver the message. In databases, we can store all the group chats in a separate table partitioned based on GroupChatID. b. Push notifications In our current design, users can only send messages to active users and if the receiving user is offline, we send a failure to the sending user. Push notifications will enable our system to send messages to offline users. For Push notifications, each user can opt-in from their device (or a web browser) to get notifications whenever there is a new message or event. Each manufacturer maintains a set of servers that handles pushing these notifications to the user. To have push notifications in our system, we would need to set up a Notification server, which will take the messages for offline users and send them to the manufacture\u2019s push notification server, which will then send them to the user\u2019s device.","title":"10. Extended Requirements"},{"location":"DesigningFacebookNewsfeed/","text":"Designing Facebook\u2019s Newsfeed Let's design Facebook's Newsfeed, which would contain posts, photos, videos, and status updates from all the people and pages a user follows. Similar Services: Twitter Newsfeed, Instagram Newsfeed, Quora Newsfeed Difficulty Level: Hard 1. What is Facebook\u2019s newsfeed? A Newsfeed is the constantly updating list of stories in the middle of Facebook\u2019s homepage. It includes status updates, photos, videos, links, app activity, and \u2018likes\u2019 from people, pages, and groups that a user follows on Facebook. In other words, it is a compilation of a complete scrollable version of your friends\u2019 and your life story from photos, videos, locations, status updates, and other activities. For any social media site you design - Twitter, Instagram, or Facebook - you will need some newsfeed system to display updates from friends and followers. 2. Requirements and Goals of the System Let\u2019s design a newsfeed for Facebook with the following requirements: Functional requirements: Newsfeed will be generated based on the posts from the people, pages, and groups that a user follows. A user may have many friends and follow a large number of pages/groups. Feeds may contain images, videos, or just text. Our service should support appending new posts as they arrive to the newsfeed for all active users. Non-functional requirements: Our system should be able to generate any user\u2019s newsfeed in real-time - maximum latency seen by the end user would be 2s. A post shouldn\u2019t take more than 5s to make it to a user\u2019s feed assuming a new newsfeed request comes in. 3. Capacity Estimation and Constraints Let\u2019s assume on average a user has 300 friends and follows 200 pages. Traffic estimates: Let\u2019s assume 300M daily active users with each user fetching their timeline an average of five times a day. This will result in 1.5B newsfeed requests per day or approximately 17,500 requests per second. Storage estimates: On average, let\u2019s assume we need to have around 500 posts in every user\u2019s feed that we want to keep in memory for a quick fetch. Let\u2019s also assume that on average each post would be 1KB in size. This would mean that we need to store roughly 500KB of data per user. To store all this data for all the active users we would need 150TB of memory. If a server can hold 100GB we would need around 1500 machines to keep the top 500 posts in memory for all active users. 4. System APIs We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the API for getting the newsfeed: getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replies) Parameters: api_dev_key (string): The API developer key of a registered can be used to, among other things, throttle users based on their allocated quota. user_id (number): The ID of the user for whom the system will generate the newsfeed. since_id (number): Optional; returns results with an ID higher than (that is, more recent than) the specified ID. count (number): Optional; specifies the number of feed items to try and retrieve up to a maximum of 200 per distinct request. max_id (number): Optional; returns results with an ID less than (that is, older than) or equal to the specified ID. exclude_replies(boolean): Optional; this parameter will prevent replies from appearing in the returned timeline. Returns: (JSON) Returns a JSON object containing a list of feed items. 5. Database Design There are three primary objects: User, Entity (e.g. page, group, etc.), and FeedItem (or Post). Here are some observations about the relationships between these entities: A User can follow other entities and can become friends with other users. Both users and entities can post FeedItems which can contain text, images, or videos. Each FeedItem will have a UserID which will point to the User who created it. For simplicity, let\u2019s assume that only users can create feed items, although, on Facebook Pages can post feed item too. Each FeedItem can optionally have an EntityID pointing to the page or the group where that post was created. If we are using a relational database, we would need to model two relations: User-Entity relation and FeedItem-Media relation. Since each user can be friends with many people and follow a lot of entities, we can store this relation in a separate table. The \u201cType\u201d column in \u201cUserFollow\u201d identifies if the entity being followed is a User or Entity. Similarly, we can have a table for FeedMedia relation. 6. High Level System Design At a high level this problem can be divided into two parts: Feed generation: Newsfeed is generated from the posts (or feed items) of users and entities (pages and groups) that a user follows. So, whenever our system receives a request to generate the feed for a user (say Jane), we will perform the following steps: Retrieve IDs of all users and entities that Jane follows. Retrieve latest, most popular and relevant posts for those IDs. These are the potential posts that we can show in Jane\u2019s newsfeed. Rank these posts based on the relevance to Jane. This represents Jane\u2019s current feed. Store this feed in the cache and return top posts (say 20) to be rendered on Jane\u2019s feed. On the front-end, when Jane reaches the end of her current feed, she can fetch the next 20 posts from the server and so on. One thing to notice here is that we generated the feed once and stored it in the cache. What about new incoming posts from people that Jane follows? If Jane is online, we should have a mechanism to rank and add those new posts to her feed. We can periodically (say every five minutes) perform the above steps to rank and add the newer posts to her feed. Jane can then be notified that there are newer items in her feed that she can fetch. Feed publishing: Whenever Jane loads her newsfeed page, she has to request and pull feed items from the server. When she reaches the end of her current feed, she can pull more data from the server. For newer items either the server can notify Jane and then she can pull, or the server can push, these new posts. We will discuss these options in detail later. At a high level, we will need following components in our Newsfeed service: 1. Web servers: To maintain a connection with the user. This connection will be used to transfer data between the user and the server. 2. Application server: To execute the workflows of storing new posts in the database servers. We will also need some application servers to retrieve and to push the newsfeed to the end user. 3. Metadata database and cache: To store the metadata about Users, Pages, and Groups. 4. Posts database and cache: To store metadata about posts and their contents. 5. Video and photo storage, and cache: Blob storage, to store all the media included in the posts. 6. Newsfeed generation service: To gather and rank all the relevant posts for a user to generate newsfeed and store in the cache. This service will also receive live updates and will add these newer feed items to any user\u2019s timeline. 7. Feed notification service: To notify the user that there are newer items available for their newsfeed. Following is the high-level architecture diagram of our system. User B and C are following User A. 7. Detailed Component Design Let\u2019s discuss different components of our system in detail. a. Feed generation Let\u2019s take the simple case of the newsfeed generation service fetching most recent posts from all the users and entities that Jane follows; the query would look like this: (SELECT FeedItemID FROM FeedItem WHERE UserID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 0(user)) ) UNION (SELECT FeedItemID FROM FeedItem WHERE EntityID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 1(entity)) ) ORDER BY CreationDate DESC LIMIT 100 Here are issues with this design for the feed generation service: Crazy slow for users with a lot of friends/follows as we have to perform sorting/merging/ranking of a huge number of posts. We generate the timeline when a user loads their page. This would be quite slow and have a high latency. For live updates, each status update will result in feed updates for all followers. This could result in high backlogs in our Newsfeed Generation Service. For live updates, the server pushing (or notifying about) newer posts to users could lead to very heavy loads, especially for people or pages that have a lot of followers. To improve the efficiency, we can pre-generate the timeline and store it in a memory. Offline generation for newsfeed: We can have dedicated servers that are continuously generating users\u2019 newsfeed and storing them in memory. So, whenever a user requests for the new posts for their feed, we can simply serve it from the pre-generated, stored location. Using this scheme, user\u2019s newsfeed is not compiled on load, but rather on a regular basis and returned to users whenever they request for it. Whenever these servers need to generate the feed for a user, they will first query to see what was the last time the feed was generated for that user. Then, new feed data would be generated from that time onwards. We can store this data in a hash table where the \u201ckey\u201d would be UserID and \u201cvalue\u201d would be a STRUCT like this: Struct { LinkedHashMap<FeedItemID, FeedItem> feedItems; DateTime lastGenerated; } We can store FeedItemIDs in a data structure similar to Linked HashMap or TreeMap, which can allow us to not only jump to any feed item but also iterate through the map easily. Whenever users want to fetch more feed items, they can send the last FeedItemID they currently see in their newsfeed, we can then jump to that FeedItemID in our hash-map and return next batch/page of feed items from there. How many feed items should we store in memory for a user\u2019s feed? Initially, we can decide to store 500 feed items per user, but this number can be adjusted later based on the usage pattern. For example, if we assume that one page of a user\u2019s feed has 20 posts and most of the users never browse more than ten pages of their feed, we can decide to store only 200 posts per user. For any user who wants to see more posts (more than what is stored in memory), we can always query backend servers. Should we generate (and keep in memory) newsfeeds for all users? There will be a lot of users that don\u2019t login frequently. Here are a few things we can do to handle this; 1) a more straightforward approach could be, to use a LRU based cache that can remove users from memory that haven\u2019t accessed their newsfeed for a long time 2) a smarter solution can figure out the login pattern of users to pre-generate their newsfeed, e.g., at what time of the day a user is active and which days of the week does a user access their newsfeed? etc. Let\u2019s now discuss some solutions to our \u201clive updates\u201d problems in the following section. b. Feed publishing The process of pushing a post to all the followers is called a fanout. By analogy, the push approach is called fanout-on-write, while the pull approach is called fanout-on-load. Let\u2019s discuss different options for publishing feed data to users. \u201cPull\u201d model or Fan-out-on-load: This method involves keeping all the recent feed data in memory so that users can pull it from the server whenever they need it. Clients can pull the feed data on a regular basis or manually whenever they need it. Possible problems with this approach are a) New data might not be shown to the users until they issue a pull request, b) It\u2019s hard to find the right pull cadence, as most of the time pull requests will result in an empty response if there is no new data, causing waste of resources. \u201cPush\u201d model or Fan-out-on-write: For a push system, once a user has published a post, we can immediately push this post to all the followers. The advantage is that when fetching feed you don\u2019t need to go through your friend\u2019s list and get feeds for each of them. It significantly reduces read operations. To efficiently handle this, users have to maintain a Long Poll request with the server for receiving the updates. A possible problem with this approach is that when a user has millions of followers (a celebrity-user) the server has to push updates to a lot of people. Hybrid: An alternate method to handle feed data could be to use a hybrid approach, i.e., to do a combination of fan-out-on-write and fan-out-on-load. Specifically, we can stop pushing posts from users with a high number of followers (a celebrity user) and only push data for those users who have a few hundred (or thousand) followers. For celebrity users, we can let the followers pull the updates. Since the push operation can be extremely costly for users who have a lot of friends or followers, by disabling fanout for them, we can save a huge number of resources. Another alternate approach could be that, once a user publishes a post, we can limit the fanout to only her online friends. Also, to get benefits from both the approaches, a combination of \u2018push to notify\u2019 and \u2018pull for serving\u2019 end users is a great way to go. Purely a push or pull model is less versatile. How many feed items can we return to the client in each request? We should have a maximum limit for the number of items a user can fetch in one request (say 20). But, we should let the client specify how many feed items they want with each request as the user may like to fetch a different number of posts depending on the device (mobile vs. desktop). Should we always notify users if there are new posts available for their newsfeed? It could be useful for users to get notified whenever new data is available. However, on mobile devices, where data usage is relatively expensive, it can consume unnecessary bandwidth. Hence, at least for mobile devices, we can choose not to push data, instead, let users \u201cPull to Refresh\u201d to get new posts. 8. Feed Ranking The most straightforward way to rank posts in a newsfeed is by the creation time of the posts, but today\u2019s ranking algorithms are doing a lot more than that to ensure \u201cimportant\u201d posts are ranked higher. The high-level idea of ranking is first to select key \u201csignals\u201d that make a post important and then to find out how to combine them to calculate a final ranking score. More specifically, we can select features that are relevant to the importance of any feed item, e.g., number of likes, comments, shares, time of the update, whether the post has images/videos, etc., and then, a score can be calculated using these features. This is generally enough for a simple ranking system. A better ranking system can significantly improve itself by constantly evaluating if we are making progress in user stickiness, retention, ads revenue, etc. 9. Data Partitioning a. Sharding posts and metadata Since we have a huge number of new posts every day and our read load is extremely high too, we need to distribute our data onto multiple machines such that we can read/write it efficiently. For sharding our databases that are storing posts and their metadata, we can have a similar design as discussed under Designing Twitter. b. Sharding feed data For feed data, which is being stored in memory, we can partition it based on UserID. We can try storing all the data of a user on one server. When storing, we can pass the UserID to our hash function that will map the user to a cache server where we will store the user\u2019s feed objects. Also, for any given user, since we don\u2019t expect to store more than 500 FeedItmeIDs, we will not run into a scenario where feed data for a user doesn\u2019t fit on a single server. To get the feed of a user, we would always have to query only one server. For future growth and replication, we must use Consistent Hashing.","title":"Designing Facebook\u2019s Newsfeed"},{"location":"DesigningFacebookNewsfeed/#designing-facebooks-newsfeed","text":"Let's design Facebook's Newsfeed, which would contain posts, photos, videos, and status updates from all the people and pages a user follows. Similar Services: Twitter Newsfeed, Instagram Newsfeed, Quora Newsfeed Difficulty Level: Hard","title":"Designing Facebook\u2019s Newsfeed"},{"location":"DesigningFacebookNewsfeed/#1-what-is-facebooks-newsfeed","text":"A Newsfeed is the constantly updating list of stories in the middle of Facebook\u2019s homepage. It includes status updates, photos, videos, links, app activity, and \u2018likes\u2019 from people, pages, and groups that a user follows on Facebook. In other words, it is a compilation of a complete scrollable version of your friends\u2019 and your life story from photos, videos, locations, status updates, and other activities. For any social media site you design - Twitter, Instagram, or Facebook - you will need some newsfeed system to display updates from friends and followers.","title":"1. What is Facebook\u2019s newsfeed?"},{"location":"DesigningFacebookNewsfeed/#2-requirements-and-goals-of-the-system","text":"Let\u2019s design a newsfeed for Facebook with the following requirements: Functional requirements: Newsfeed will be generated based on the posts from the people, pages, and groups that a user follows. A user may have many friends and follow a large number of pages/groups. Feeds may contain images, videos, or just text. Our service should support appending new posts as they arrive to the newsfeed for all active users. Non-functional requirements: Our system should be able to generate any user\u2019s newsfeed in real-time - maximum latency seen by the end user would be 2s. A post shouldn\u2019t take more than 5s to make it to a user\u2019s feed assuming a new newsfeed request comes in.","title":"2. Requirements and Goals of the System"},{"location":"DesigningFacebookNewsfeed/#3-capacity-estimation-and-constraints","text":"Let\u2019s assume on average a user has 300 friends and follows 200 pages. Traffic estimates: Let\u2019s assume 300M daily active users with each user fetching their timeline an average of five times a day. This will result in 1.5B newsfeed requests per day or approximately 17,500 requests per second. Storage estimates: On average, let\u2019s assume we need to have around 500 posts in every user\u2019s feed that we want to keep in memory for a quick fetch. Let\u2019s also assume that on average each post would be 1KB in size. This would mean that we need to store roughly 500KB of data per user. To store all this data for all the active users we would need 150TB of memory. If a server can hold 100GB we would need around 1500 machines to keep the top 500 posts in memory for all active users.","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningFacebookNewsfeed/#4-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the API for getting the newsfeed: getUserFeed(api_dev_key, user_id, since_id, count, max_id, exclude_replies) Parameters: api_dev_key (string): The API developer key of a registered can be used to, among other things, throttle users based on their allocated quota. user_id (number): The ID of the user for whom the system will generate the newsfeed. since_id (number): Optional; returns results with an ID higher than (that is, more recent than) the specified ID. count (number): Optional; specifies the number of feed items to try and retrieve up to a maximum of 200 per distinct request. max_id (number): Optional; returns results with an ID less than (that is, older than) or equal to the specified ID. exclude_replies(boolean): Optional; this parameter will prevent replies from appearing in the returned timeline. Returns: (JSON) Returns a JSON object containing a list of feed items.","title":"4. System APIs"},{"location":"DesigningFacebookNewsfeed/#5-database-design","text":"There are three primary objects: User, Entity (e.g. page, group, etc.), and FeedItem (or Post). Here are some observations about the relationships between these entities: A User can follow other entities and can become friends with other users. Both users and entities can post FeedItems which can contain text, images, or videos. Each FeedItem will have a UserID which will point to the User who created it. For simplicity, let\u2019s assume that only users can create feed items, although, on Facebook Pages can post feed item too. Each FeedItem can optionally have an EntityID pointing to the page or the group where that post was created. If we are using a relational database, we would need to model two relations: User-Entity relation and FeedItem-Media relation. Since each user can be friends with many people and follow a lot of entities, we can store this relation in a separate table. The \u201cType\u201d column in \u201cUserFollow\u201d identifies if the entity being followed is a User or Entity. Similarly, we can have a table for FeedMedia relation.","title":"5. Database Design"},{"location":"DesigningFacebookNewsfeed/#6-high-level-system-design","text":"At a high level this problem can be divided into two parts: Feed generation: Newsfeed is generated from the posts (or feed items) of users and entities (pages and groups) that a user follows. So, whenever our system receives a request to generate the feed for a user (say Jane), we will perform the following steps: Retrieve IDs of all users and entities that Jane follows. Retrieve latest, most popular and relevant posts for those IDs. These are the potential posts that we can show in Jane\u2019s newsfeed. Rank these posts based on the relevance to Jane. This represents Jane\u2019s current feed. Store this feed in the cache and return top posts (say 20) to be rendered on Jane\u2019s feed. On the front-end, when Jane reaches the end of her current feed, she can fetch the next 20 posts from the server and so on. One thing to notice here is that we generated the feed once and stored it in the cache. What about new incoming posts from people that Jane follows? If Jane is online, we should have a mechanism to rank and add those new posts to her feed. We can periodically (say every five minutes) perform the above steps to rank and add the newer posts to her feed. Jane can then be notified that there are newer items in her feed that she can fetch. Feed publishing: Whenever Jane loads her newsfeed page, she has to request and pull feed items from the server. When she reaches the end of her current feed, she can pull more data from the server. For newer items either the server can notify Jane and then she can pull, or the server can push, these new posts. We will discuss these options in detail later. At a high level, we will need following components in our Newsfeed service: 1. Web servers: To maintain a connection with the user. This connection will be used to transfer data between the user and the server. 2. Application server: To execute the workflows of storing new posts in the database servers. We will also need some application servers to retrieve and to push the newsfeed to the end user. 3. Metadata database and cache: To store the metadata about Users, Pages, and Groups. 4. Posts database and cache: To store metadata about posts and their contents. 5. Video and photo storage, and cache: Blob storage, to store all the media included in the posts. 6. Newsfeed generation service: To gather and rank all the relevant posts for a user to generate newsfeed and store in the cache. This service will also receive live updates and will add these newer feed items to any user\u2019s timeline. 7. Feed notification service: To notify the user that there are newer items available for their newsfeed. Following is the high-level architecture diagram of our system. User B and C are following User A.","title":"6. High Level System Design"},{"location":"DesigningFacebookNewsfeed/#7-detailed-component-design","text":"Let\u2019s discuss different components of our system in detail. a. Feed generation Let\u2019s take the simple case of the newsfeed generation service fetching most recent posts from all the users and entities that Jane follows; the query would look like this: (SELECT FeedItemID FROM FeedItem WHERE UserID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 0(user)) ) UNION (SELECT FeedItemID FROM FeedItem WHERE EntityID in ( SELECT EntityOrFriendID FROM UserFollow WHERE UserID = <current_user_id> and type = 1(entity)) ) ORDER BY CreationDate DESC LIMIT 100 Here are issues with this design for the feed generation service: Crazy slow for users with a lot of friends/follows as we have to perform sorting/merging/ranking of a huge number of posts. We generate the timeline when a user loads their page. This would be quite slow and have a high latency. For live updates, each status update will result in feed updates for all followers. This could result in high backlogs in our Newsfeed Generation Service. For live updates, the server pushing (or notifying about) newer posts to users could lead to very heavy loads, especially for people or pages that have a lot of followers. To improve the efficiency, we can pre-generate the timeline and store it in a memory. Offline generation for newsfeed: We can have dedicated servers that are continuously generating users\u2019 newsfeed and storing them in memory. So, whenever a user requests for the new posts for their feed, we can simply serve it from the pre-generated, stored location. Using this scheme, user\u2019s newsfeed is not compiled on load, but rather on a regular basis and returned to users whenever they request for it. Whenever these servers need to generate the feed for a user, they will first query to see what was the last time the feed was generated for that user. Then, new feed data would be generated from that time onwards. We can store this data in a hash table where the \u201ckey\u201d would be UserID and \u201cvalue\u201d would be a STRUCT like this: Struct { LinkedHashMap<FeedItemID, FeedItem> feedItems; DateTime lastGenerated; } We can store FeedItemIDs in a data structure similar to Linked HashMap or TreeMap, which can allow us to not only jump to any feed item but also iterate through the map easily. Whenever users want to fetch more feed items, they can send the last FeedItemID they currently see in their newsfeed, we can then jump to that FeedItemID in our hash-map and return next batch/page of feed items from there. How many feed items should we store in memory for a user\u2019s feed? Initially, we can decide to store 500 feed items per user, but this number can be adjusted later based on the usage pattern. For example, if we assume that one page of a user\u2019s feed has 20 posts and most of the users never browse more than ten pages of their feed, we can decide to store only 200 posts per user. For any user who wants to see more posts (more than what is stored in memory), we can always query backend servers. Should we generate (and keep in memory) newsfeeds for all users? There will be a lot of users that don\u2019t login frequently. Here are a few things we can do to handle this; 1) a more straightforward approach could be, to use a LRU based cache that can remove users from memory that haven\u2019t accessed their newsfeed for a long time 2) a smarter solution can figure out the login pattern of users to pre-generate their newsfeed, e.g., at what time of the day a user is active and which days of the week does a user access their newsfeed? etc. Let\u2019s now discuss some solutions to our \u201clive updates\u201d problems in the following section. b. Feed publishing The process of pushing a post to all the followers is called a fanout. By analogy, the push approach is called fanout-on-write, while the pull approach is called fanout-on-load. Let\u2019s discuss different options for publishing feed data to users. \u201cPull\u201d model or Fan-out-on-load: This method involves keeping all the recent feed data in memory so that users can pull it from the server whenever they need it. Clients can pull the feed data on a regular basis or manually whenever they need it. Possible problems with this approach are a) New data might not be shown to the users until they issue a pull request, b) It\u2019s hard to find the right pull cadence, as most of the time pull requests will result in an empty response if there is no new data, causing waste of resources. \u201cPush\u201d model or Fan-out-on-write: For a push system, once a user has published a post, we can immediately push this post to all the followers. The advantage is that when fetching feed you don\u2019t need to go through your friend\u2019s list and get feeds for each of them. It significantly reduces read operations. To efficiently handle this, users have to maintain a Long Poll request with the server for receiving the updates. A possible problem with this approach is that when a user has millions of followers (a celebrity-user) the server has to push updates to a lot of people. Hybrid: An alternate method to handle feed data could be to use a hybrid approach, i.e., to do a combination of fan-out-on-write and fan-out-on-load. Specifically, we can stop pushing posts from users with a high number of followers (a celebrity user) and only push data for those users who have a few hundred (or thousand) followers. For celebrity users, we can let the followers pull the updates. Since the push operation can be extremely costly for users who have a lot of friends or followers, by disabling fanout for them, we can save a huge number of resources. Another alternate approach could be that, once a user publishes a post, we can limit the fanout to only her online friends. Also, to get benefits from both the approaches, a combination of \u2018push to notify\u2019 and \u2018pull for serving\u2019 end users is a great way to go. Purely a push or pull model is less versatile. How many feed items can we return to the client in each request? We should have a maximum limit for the number of items a user can fetch in one request (say 20). But, we should let the client specify how many feed items they want with each request as the user may like to fetch a different number of posts depending on the device (mobile vs. desktop). Should we always notify users if there are new posts available for their newsfeed? It could be useful for users to get notified whenever new data is available. However, on mobile devices, where data usage is relatively expensive, it can consume unnecessary bandwidth. Hence, at least for mobile devices, we can choose not to push data, instead, let users \u201cPull to Refresh\u201d to get new posts.","title":"7. Detailed Component Design"},{"location":"DesigningFacebookNewsfeed/#8-feed-ranking","text":"The most straightforward way to rank posts in a newsfeed is by the creation time of the posts, but today\u2019s ranking algorithms are doing a lot more than that to ensure \u201cimportant\u201d posts are ranked higher. The high-level idea of ranking is first to select key \u201csignals\u201d that make a post important and then to find out how to combine them to calculate a final ranking score. More specifically, we can select features that are relevant to the importance of any feed item, e.g., number of likes, comments, shares, time of the update, whether the post has images/videos, etc., and then, a score can be calculated using these features. This is generally enough for a simple ranking system. A better ranking system can significantly improve itself by constantly evaluating if we are making progress in user stickiness, retention, ads revenue, etc.","title":"8. Feed Ranking"},{"location":"DesigningFacebookNewsfeed/#9-data-partitioning","text":"a. Sharding posts and metadata Since we have a huge number of new posts every day and our read load is extremely high too, we need to distribute our data onto multiple machines such that we can read/write it efficiently. For sharding our databases that are storing posts and their metadata, we can have a similar design as discussed under Designing Twitter. b. Sharding feed data For feed data, which is being stored in memory, we can partition it based on UserID. We can try storing all the data of a user on one server. When storing, we can pass the UserID to our hash function that will map the user to a cache server where we will store the user\u2019s feed objects. Also, for any given user, since we don\u2019t expect to store more than 500 FeedItmeIDs, we will not run into a scenario where feed data for a user doesn\u2019t fit on a single server. To get the feed of a user, we would always have to query only one server. For future growth and replication, we must use Consistent Hashing.","title":"9. Data Partitioning"},{"location":"DesigningInstagram/","text":"Designing Instagram Let's design a photo-sharing service like Instagram, where users can upload photos to share them with other users. Similar Services: Flickr, Picasa Difficulty Level: Medium 1. What is Instagram? Instagram is a social networking service which enables its users to upload and share their photos and videos with other users. Instagram users can choose to share information either publicly or privately. Anything shared publicly can be seen by any other user, whereas privately shared content can only be accessed by a specified set of people. Instagram also enables its users to share through many other social networking platforms, such as Facebook, Twitter, Flickr, and Tumblr. For the sake of this exercise, we plan to design a simpler version of Instagram, where a user can share photos and can also follow other users. The \u2018News Feed\u2019 for each user will consist of top photos of all the people the user follows. 2. Requirements and Goals of the System We\u2019ll focus on the following set of requirements while designing the Instagram: Functional Requirements Users should be able to upload/download/view photos. Users can perform searches based on photo/video titles. Users can follow other users. The system should be able to generate and display a user\u2019s News Feed consisting of top photos from all the people the user follows. Non-functional Requirements Our service needs to be highly available. The acceptable latency of the system is 200ms for News Feed generation. Consistency can take a hit (in the interest of availability), if a user doesn\u2019t see a photo for a while; it should be fine. The system should be highly reliable; any uploaded photo or video should never be lost. Not in scope: Adding tags to photos, searching photos on tags, commenting on photos, tagging users to photos, who to follow, etc. 3. Some Design Considerations The system would be read-heavy, so we will focus on building a system that can retrieve photos quickly. Practically, users can upload as many photos as they like. Efficient management of storage should be a crucial factor while designing this system. Low latency is expected while viewing photos. Data should be 100% reliable. If a user uploads a photo, the system will guarantee that it will never be lost. 4. Capacity Estimation and Constraints Let\u2019s assume we have 500M total users, with 1M daily active users. 2M new photos every day, 23 new photos every second. Average photo file size => 200KB Total space required for 1 day of photos 2M * 200KB => 400 GB Total space required for 10 years: 400GB * 365 (days a year) * 10 (years) ~= 1425TB 5. High Level System Design At a high-level, we need to support two scenarios, one to upload photos and the other to view/search photos. Our service would need some object storage servers to store photos and also some database servers to store metadata information about the photos.","title":"Designing Instagram"},{"location":"DesigningInstagram/#designing-instagram","text":"Let's design a photo-sharing service like Instagram, where users can upload photos to share them with other users. Similar Services: Flickr, Picasa Difficulty Level: Medium","title":"Designing Instagram"},{"location":"DesigningInstagram/#1-what-is-instagram","text":"Instagram is a social networking service which enables its users to upload and share their photos and videos with other users. Instagram users can choose to share information either publicly or privately. Anything shared publicly can be seen by any other user, whereas privately shared content can only be accessed by a specified set of people. Instagram also enables its users to share through many other social networking platforms, such as Facebook, Twitter, Flickr, and Tumblr. For the sake of this exercise, we plan to design a simpler version of Instagram, where a user can share photos and can also follow other users. The \u2018News Feed\u2019 for each user will consist of top photos of all the people the user follows.","title":"1. What is Instagram?"},{"location":"DesigningInstagram/#2-requirements-and-goals-of-the-system","text":"We\u2019ll focus on the following set of requirements while designing the Instagram:","title":"2. Requirements and Goals of the System"},{"location":"DesigningInstagram/#functional-requirements","text":"Users should be able to upload/download/view photos. Users can perform searches based on photo/video titles. Users can follow other users. The system should be able to generate and display a user\u2019s News Feed consisting of top photos from all the people the user follows.","title":"Functional Requirements"},{"location":"DesigningInstagram/#non-functional-requirements","text":"Our service needs to be highly available. The acceptable latency of the system is 200ms for News Feed generation. Consistency can take a hit (in the interest of availability), if a user doesn\u2019t see a photo for a while; it should be fine. The system should be highly reliable; any uploaded photo or video should never be lost. Not in scope: Adding tags to photos, searching photos on tags, commenting on photos, tagging users to photos, who to follow, etc.","title":"Non-functional Requirements"},{"location":"DesigningInstagram/#3-some-design-considerations","text":"The system would be read-heavy, so we will focus on building a system that can retrieve photos quickly. Practically, users can upload as many photos as they like. Efficient management of storage should be a crucial factor while designing this system. Low latency is expected while viewing photos. Data should be 100% reliable. If a user uploads a photo, the system will guarantee that it will never be lost.","title":"3. Some Design Considerations"},{"location":"DesigningInstagram/#4-capacity-estimation-and-constraints","text":"Let\u2019s assume we have 500M total users, with 1M daily active users. 2M new photos every day, 23 new photos every second. Average photo file size => 200KB Total space required for 1 day of photos 2M * 200KB => 400 GB Total space required for 10 years: 400GB * 365 (days a year) * 10 (years) ~= 1425TB","title":"4. Capacity Estimation and Constraints"},{"location":"DesigningInstagram/#5-high-level-system-design","text":"At a high-level, we need to support two scenarios, one to upload photos and the other to view/search photos. Our service would need some object storage servers to store photos and also some database servers to store metadata information about the photos.","title":"5. High Level System Design"},{"location":"DesigningPastebin/","text":"Designing Pastebin Let's design a Pastebin like web service, where users can store plain text. Users of the service will enter a piece of text and get a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy 1. What is Pastebin? Pastebin like services enable users to store plain text or images over the network (typically the Internet) and generate unique URLs to access the uploaded data. Such services are also used to share data over the network quickly, as users would just need to pass the URL to let other users see it. If you haven\u2019t used pastebin.com before, please try creating a new \u2018Paste\u2019 there and spend some time going through the different options their service offers. This will help you a lot in understanding this chapter. 2. Requirements and Goals of the System Our Pastebin service should meet the following requirements: Functional Requirements: Users should be able to upload or \u201cpaste\u201d their data and get a unique URL to access it. Users will only be able to upload text. Data and links will expire after a specific timespan automatically; users should also be able to specify 4. expiration time. Users should optionally be able to pick a custom alias for their paste. Non-Functional Requirements: The system should be highly reliable, any data uploaded should not be lost. The system should be highly available. This is required because if our service is down, users will not be able to access their Pastes. Users should be able to access their Pastes in real-time with minimum latency. Paste links should not be guessable (not predictable). Extended Requirements: Analytics, e.g., how many times a paste was accessed? Our service should also be accessible through REST APIs by other services. 3. Some Design Considerations Pastebin shares some requirements with URL Shortening service, but there are some additional design considerations we should keep in mind. What should be the limit on the amount of text user can paste at a time? We can limit users not to have Pastes bigger than 10MB to stop the abuse of the service. Should we impose size limits on custom URLs? Since our service supports custom URLs, users can pick any URL that they like, but providing a custom URL is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on custom URLs, so that we have a consistent URL database. Our services will be read-heavy; there will be more read requests compared to new Pastes creation. We can assume a 5:1 ratio between read and write. Traffic estimates Pastebin services are not expected to have traffic similar to Twitter or Facebook, let\u2019s assume here that we get one million new pastes added to our system every day. This leaves us with five million reads per day. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec Storage estimates: Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs or logs. Such texts are not huge, so let\u2019s assume that each paste on average contains 10KB. At this rate, we will be storing 10GB of data per day. 1M * 10KB => 10 GB/day If we want to store this data for ten years we would need the total storage capacity of 36TB. With 1M pastes every day we will have 3.6 billion Pastes in 10 years. We need to generate and store keys to uniquely identify these pastes. If we use base64 encoding ([A-Z, a-z, 0-9, ., -]) we would need six letters strings: 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB. Bandwidth estimates: For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service. Memory estimates: We can cache some of the hot pastes that are frequently accessed. Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes Since we have 5M read requests per day, to cache 20% of these requests, we would need: 0.2 * 5M * 10KB ~= 10 GB 5. System APIs We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs to create/retrieve/delete Pastes: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: 1. api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. 2. paste_data (string): Textual data of the paste. 3. custom_url (string): Optional custom URL. 4. user_name (string): Optional user name to be used to generate URL. 5. paste_name (string): Optional name of the paste 6. expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion returns the URL through which the paste can be accessed, otherwise, it will return an error code. Similarly, we can have retrieve and delete Paste APIs: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019. 6. Database Design A few observations about the nature of the data we are storing: We need to store billions of records. Each metadata object we are storing would be small (less than 1KB). Each paste object we are storing can be of medium size (it can be a few MB). There are no relationships between records, except if we want to store which user created what Paste. Our service is read-heavy. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. Here, \u2018URlHash\u2019 is the URL equivalent of the TinyURL and \u2018ContentKey\u2019 is a reference to an external object storing the contents of the paste; we\u2019ll discuss the external storage of the paste contents later in the chapter. 7. High Level Design At a high level, we need an application layer that will serve all the read and write requests. Application layer will talk to a storage layer to store and retrieve data. We can segregate our storage layer with one database storing metadata related to each paste, users, etc., while the other storing the paste contents in some object storage (like Amazon S3). This division of data will also allow us to scale them individually. 8. Component Design a. Application layer Our application layer will process all incoming and outgoing requests. The application servers will be talking to the backend data store components to serve the requests. How to handle a write request? Upon receiving a write request, our application server will generate a six-letter random string, which would serve as the key of the paste (if the user has not provided a custom key). The application server will then store the contents of the paste and the generated key in the database. After the successful insertion, the server can return the key to the user. One possible problem here could be that the insertion fails because of a duplicate key. Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don\u2019t see failure due to the duplicate key. We should return an error to the user if the custom key they have provided is already present in our database. Another solution of the above problem could be to run a standalone Key Generation Service (KGS) that generates random six letters strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to store a new paste, we will just take one of the already generated keys and use it. This approach will make things quite simple and fast since we will not be worrying about duplications or collisions. KGS will make sure all the keys inserted in key-DB are unique. KGS can use two tables to store keys, one for keys that are not used yet and one for all the used keys. As soon as KGS gives some keys to an application server, it can move these to the used keys table. KGS can always keep some keys in memory so that whenever a server needs them, it can quickly provide them. As soon as KGS loads some keys in memory, it can move them to the used keys table, this way we can make sure each server gets unique keys. If KGS dies before using all the keys loaded in memory, we will be wasting those keys. We can ignore these keys given that we have a huge number of them. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS and whenever the primary server dies it can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This could be acceptable since we have 68B unique six letters keys, which are a lot more than we require. How does it handle a paste read request? Upon receiving a read paste request, the application service layer contacts the datastore. The datastore searches for the key, and if it is found, returns the paste\u2019s contents. Otherwise, an error code is returned. b. Datastore layer We can divide our datastore layer into two: Metadata database: We can use a relational database like MySQL or a Distributed Key-Value store like Dynamo or Cassandra. Object storage: We can store our contents in an Object Storage like Amazon\u2019s S3. Whenever we feel like hitting our full capacity on content storage, we can easily increase it by adding more servers. 9. Purging or DB Cleanup Please see Designing a URL Shortening service. 10. Data Partitioning and Replication Please see Designing a URL Shortening service. 11. Cache and Load Balancer Please see Designing a URL Shortening service. 12. Security and Permissions Please see Designing a URL Shortening service.","title":"Designing Pastebin"},{"location":"DesigningPastebin/#designing-pastebin","text":"Let's design a Pastebin like web service, where users can store plain text. Users of the service will enter a piece of text and get a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy","title":"Designing Pastebin"},{"location":"DesigningPastebin/#1-what-is-pastebin","text":"Pastebin like services enable users to store plain text or images over the network (typically the Internet) and generate unique URLs to access the uploaded data. Such services are also used to share data over the network quickly, as users would just need to pass the URL to let other users see it. If you haven\u2019t used pastebin.com before, please try creating a new \u2018Paste\u2019 there and spend some time going through the different options their service offers. This will help you a lot in understanding this chapter.","title":"1. What is Pastebin?"},{"location":"DesigningPastebin/#2-requirements-and-goals-of-the-system","text":"Our Pastebin service should meet the following requirements:","title":"2. Requirements and Goals of the System"},{"location":"DesigningPastebin/#functional-requirements","text":"Users should be able to upload or \u201cpaste\u201d their data and get a unique URL to access it. Users will only be able to upload text. Data and links will expire after a specific timespan automatically; users should also be able to specify 4. expiration time. Users should optionally be able to pick a custom alias for their paste.","title":"Functional Requirements:"},{"location":"DesigningPastebin/#non-functional-requirements","text":"The system should be highly reliable, any data uploaded should not be lost. The system should be highly available. This is required because if our service is down, users will not be able to access their Pastes. Users should be able to access their Pastes in real-time with minimum latency. Paste links should not be guessable (not predictable).","title":"Non-Functional Requirements:"},{"location":"DesigningPastebin/#extended-requirements","text":"Analytics, e.g., how many times a paste was accessed? Our service should also be accessible through REST APIs by other services.","title":"Extended Requirements:"},{"location":"DesigningPastebin/#3-some-design-considerations","text":"Pastebin shares some requirements with URL Shortening service, but there are some additional design considerations we should keep in mind. What should be the limit on the amount of text user can paste at a time? We can limit users not to have Pastes bigger than 10MB to stop the abuse of the service. Should we impose size limits on custom URLs? Since our service supports custom URLs, users can pick any URL that they like, but providing a custom URL is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on custom URLs, so that we have a consistent URL database. Our services will be read-heavy; there will be more read requests compared to new Pastes creation. We can assume a 5:1 ratio between read and write.","title":"3. Some Design Considerations"},{"location":"DesigningPastebin/#traffic-estimates","text":"Pastebin services are not expected to have traffic similar to Twitter or Facebook, let\u2019s assume here that we get one million new pastes added to our system every day. This leaves us with five million reads per day. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec","title":"Traffic estimates"},{"location":"DesigningPastebin/#storage-estimates","text":"Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs or logs. Such texts are not huge, so let\u2019s assume that each paste on average contains 10KB. At this rate, we will be storing 10GB of data per day. 1M * 10KB => 10 GB/day If we want to store this data for ten years we would need the total storage capacity of 36TB. With 1M pastes every day we will have 3.6 billion Pastes in 10 years. We need to generate and store keys to uniquely identify these pastes. If we use base64 encoding ([A-Z, a-z, 0-9, ., -]) we would need six letters strings: 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB.","title":"Storage estimates:"},{"location":"DesigningPastebin/#bandwidth-estimates","text":"For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service.","title":"Bandwidth estimates:"},{"location":"DesigningPastebin/#memory-estimates","text":"We can cache some of the hot pastes that are frequently accessed. Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes Since we have 5M read requests per day, to cache 20% of these requests, we would need: 0.2 * 5M * 10KB ~= 10 GB","title":"Memory estimates:"},{"location":"DesigningPastebin/#5-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs to create/retrieve/delete Pastes: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: 1. api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. 2. paste_data (string): Textual data of the paste. 3. custom_url (string): Optional custom URL. 4. user_name (string): Optional user name to be used to generate URL. 5. paste_name (string): Optional name of the paste 6. expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion returns the URL through which the paste can be accessed, otherwise, it will return an error code. Similarly, we can have retrieve and delete Paste APIs: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019.","title":"5. System APIs"},{"location":"DesigningPastebin/#6-database-design","text":"A few observations about the nature of the data we are storing: We need to store billions of records. Each metadata object we are storing would be small (less than 1KB). Each paste object we are storing can be of medium size (it can be a few MB). There are no relationships between records, except if we want to store which user created what Paste. Our service is read-heavy. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. Here, \u2018URlHash\u2019 is the URL equivalent of the TinyURL and \u2018ContentKey\u2019 is a reference to an external object storing the contents of the paste; we\u2019ll discuss the external storage of the paste contents later in the chapter.","title":"6. Database Design"},{"location":"DesigningPastebin/#7-high-level-design","text":"At a high level, we need an application layer that will serve all the read and write requests. Application layer will talk to a storage layer to store and retrieve data. We can segregate our storage layer with one database storing metadata related to each paste, users, etc., while the other storing the paste contents in some object storage (like Amazon S3). This division of data will also allow us to scale them individually.","title":"7. High Level Design"},{"location":"DesigningPastebin/#8-component-design","text":"","title":"8. Component Design"},{"location":"DesigningPastebin/#a-application-layer","text":"Our application layer will process all incoming and outgoing requests. The application servers will be talking to the backend data store components to serve the requests. How to handle a write request? Upon receiving a write request, our application server will generate a six-letter random string, which would serve as the key of the paste (if the user has not provided a custom key). The application server will then store the contents of the paste and the generated key in the database. After the successful insertion, the server can return the key to the user. One possible problem here could be that the insertion fails because of a duplicate key. Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don\u2019t see failure due to the duplicate key. We should return an error to the user if the custom key they have provided is already present in our database. Another solution of the above problem could be to run a standalone Key Generation Service (KGS) that generates random six letters strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to store a new paste, we will just take one of the already generated keys and use it. This approach will make things quite simple and fast since we will not be worrying about duplications or collisions. KGS will make sure all the keys inserted in key-DB are unique. KGS can use two tables to store keys, one for keys that are not used yet and one for all the used keys. As soon as KGS gives some keys to an application server, it can move these to the used keys table. KGS can always keep some keys in memory so that whenever a server needs them, it can quickly provide them. As soon as KGS loads some keys in memory, it can move them to the used keys table, this way we can make sure each server gets unique keys. If KGS dies before using all the keys loaded in memory, we will be wasting those keys. We can ignore these keys given that we have a huge number of them. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS and whenever the primary server dies it can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This could be acceptable since we have 68B unique six letters keys, which are a lot more than we require. How does it handle a paste read request? Upon receiving a read paste request, the application service layer contacts the datastore. The datastore searches for the key, and if it is found, returns the paste\u2019s contents. Otherwise, an error code is returned.","title":"a. Application layer"},{"location":"DesigningPastebin/#b-datastore-layer","text":"We can divide our datastore layer into two: Metadata database: We can use a relational database like MySQL or a Distributed Key-Value store like Dynamo or Cassandra. Object storage: We can store our contents in an Object Storage like Amazon\u2019s S3. Whenever we feel like hitting our full capacity on content storage, we can easily increase it by adding more servers.","title":"b. Datastore layer"},{"location":"DesigningPastebin/#9-purging-or-db-cleanup","text":"Please see Designing a URL Shortening service.","title":"9. Purging or DB Cleanup"},{"location":"DesigningPastebin/#10-data-partitioning-and-replication","text":"Please see Designing a URL Shortening service.","title":"10. Data Partitioning and Replication"},{"location":"DesigningPastebin/#11-cache-and-load-balancer","text":"Please see Designing a URL Shortening service.","title":"11. Cache and Load Balancer"},{"location":"DesigningPastebin/#12-security-and-permissions","text":"Please see Designing a URL Shortening service.","title":"12. Security and Permissions"},{"location":"DesigningTwitter/","text":"Designing Twitter Let's design a Twitter-like social networking service. Users of the service will be able to post tweets, follow other people, and favorite tweets. Difficulty Level: Medium 1. What is Twitter? Twitter is an online social networking service where users post and read short 140-character messages called \"tweets.\" Registered users can post and read tweets, but those who are not registered can only read them. Users access Twitter through their website interface, SMS, or mobile app. 2. Requirements and Goals of the System We will be designing a simpler version of Twitter with the following requirements: Functional Requirements Users should be able to post new tweets. A user should be able to follow other users. Users should be able to mark tweets as favorites. The service should be able to create and display a user\u2019s timeline consisting of top tweets from all the people the user follows. Tweets can contain photos and videos. Non-functional Requirements Our service needs to be highly available. Acceptable latency of the system is 200ms for timeline generation. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a tweet for a while, it should be fine. Extended Requirements Searching for tweets. Replying to a tweet. Trending topics \u2013 current hot topics/searches. Tagging other users. Tweet Notification. Who to follow? Suggestions? Moments. 3. Capacity Estimation and Constraints Let\u2019s assume we have one billion total users with 200 million daily active users (DAU). Also assume we have 100 million new tweets every day and on average each user follows 200 people. How many favorites per day? If, on average, each user favorites five tweets per day we will have: 200M users * 5 favorites => 1B favorites How many total tweet-views will our system generate? Let\u2019s assume on average a user visits their timeline two times a day and visits five other people\u2019s pages. On each page if a user sees 20 tweets, then our system will generate 28B/day total tweet-views: 200M DAU * ((2 + 5) * 20 tweets) => 28B/day Storage Estimates Let\u2019s say each tweet has 140 characters and we need two bytes to store a character without compression. Let\u2019s assume we need 30 bytes to store metadata with each tweet (like ID, timestamp, user ID, etc.). Total storage we would need: 100M * (280 + 30) bytes => 30GB/day What would our storage needs be for five years? How much storage we would need for users\u2019 data, follows, favorites? We will leave this for the exercise. Not all tweets will have media, let\u2019s assume that on average every fifth tweet has a photo and every tenth has a video. Let\u2019s also assume on average a photo is 200KB and a video is 2MB. This will lead us to have 24TB of new media every day. (100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day Bandwidth Estimates Since total ingress is 24TB per day, this would translate into 290MB/sec. Remember that we have 28B tweet views per day. We must show the photo of every tweet (if it has a photo), but let\u2019s assume that the users watch every 3rd video they see in their timeline. So, total egress will be: (28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s Total ~= 35GB/s 4. System APIs \ud83d\udca1 Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system. We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definition of the API for posting a new tweet: tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. tweet_data (string): The text of the tweet, typically up to 140 characters. tweet_location (string): Optional location (longitude, latitude) this Tweet refers to. user_location (string): Optional location (longitude, latitude) of the user adding the tweet. media_ids (number[]): Optional list of media_ids to be associated with the Tweet. (all the media photo, video, etc. need to be uploaded separately). Returns: (string) A successful post will return the URL to access that tweet. Otherwise, an appropriate HTTP error is returned. 5. High Level System Design We need a system that can efficiently store all the new tweets, 100M/86400s => 1150 tweets per second and read 28B/86400s => 325K tweets per second. It is clear from the requirements that this will be a read-heavy system. At a high level, we need multiple application servers to serve all these requests with load balancers in front of them for traffic distributions. On the backend, we need an efficient database that can store all the new tweets and can support a huge number of reads. We also need some file storage to store photos and videos. Although our expected daily write load is 100 million and read load is 28 billion tweets. This means on average our system will receive around 1160 new tweets and 325K read requests per second. This traffic will be distributed unevenly throughout the day, though, at peak time we should expect at least a few thousand write requests and around 1M read requests per second. We should keep this in mind while designing the architecture of our system. 6. Database Schema We need to store data about users, their tweets, their favorite tweets, and people they follow. For choosing between SQL and NoSQL databases to store the above schema, please see \u2018Database schema\u2019 under Designing Instagram. 7. Data Sharding Since we have a huge number of new tweets every day and our read load is extremely high too, we need to distribute our data onto multiple machines such that we can read/write it efficiently. We have many options to shard our data; let\u2019s go through them one by one: Sharding based on UserID: We can try storing all the data of a user on one server. While storing, we can pass the UserID to our hash function that will map the user to a database server where we will store all of the user\u2019s tweets, favorites, follows, etc. While querying for tweets/follows/favorites of a user, we can ask our hash function where can we find the data of a user and then read it from there. This approach has a couple of issues: What if a user becomes hot? There could be a lot of queries on the server holding the user. This high load will affect the performance of our service. Over time some users can end up storing a lot of tweets or having a lot of follows compared to others. Maintaining a uniform distribution of growing user data is quite difficult. To recover from these situations either we have to repartition/redistribute our data or use consistent hashing. Sharding based on TweetID: Our hash function will map each TweetID to a random server where we will store that Tweet. To search for tweets, we have to query all servers, and each server will return a set of tweets. A centralized server will aggregate these results to return them to the user. Let\u2019s look into timeline generation example; here are the number of steps our system has to perform to generate a user\u2019s timeline: Our application (app) server will find all the people the user follows. App server will send the query to all database servers to find tweets from these people. Each database server will find the tweets for each user, sort them by recency and return the top tweets. App server will merge all the results and sort them again to return the top results to the user. This approach solves the problem of hot users, but, in contrast to sharding by UserID, we have to query all database partitions to find tweets of a user, which can result in higher latencies. We can further improve our performance by introducing cache to store hot tweets in front of the database servers. Sharding based on Tweet creation time: Storing tweets based on creation time will give us the advantage of fetching all the top tweets quickly and we only have to query a very small set of servers. The problem here is that the traffic load will not be distributed, e.g., while writing, all new tweets will be going to one server and the remaining servers will be sitting idle. Similarly, while reading, the server holding the latest data will have a very high load as compared to servers holding old data. What if we can combine sharding by TweetID and Tweet creation time? If we don\u2019t store tweet creation time separately and use TweetID to reflect that, we can get benefits of both the approaches. This way it will be quite quick to find the latest Tweets. For this, we must make each TweetID universally unique in our system and each TweetID should contain a timestamp too. We can use epoch time for this. Let\u2019s say our TweetID will have two parts: the first part will be representing epoch seconds and the second part will be an auto-incrementing sequence. So, to make a new TweetID, we can take the current epoch time and append an auto-incrementing number to it. We can figure out the shard number from this TweetID and store it there. What could be the size of our TweetID? Let\u2019s say our epoch time starts today, how many bits we would need to store the number of seconds for the next 50 years? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6B We would need 31 bits to store this number. Since on average we are expecting 1150 new tweets per second, we can allocate 17 bits to store auto incremented sequence; this will make our TweetID 48 bits long. So, every second we can store (2^17 => 130K) new tweets. We can reset our auto incrementing sequence every second. For fault tolerance and better performance, we can have two database servers to generate auto-incrementing keys for us, one generating even numbered keys and the other generating odd numbered keys. If we assume our current epoch seconds are \u201c1483228800,\u201d our TweetID will look like this: 1483228800 000001 1483228800 000002 1483228800 000003 1483228800 000004 \u2026 If we make our TweetID 64bits (8 bytes) long, we can easily store tweets for the next 100 years and also store them for mili-seconds granularity. In the above approach, we still have to query all the servers for timeline generation, but our reads (and writes) will be substantially quicker. Since we don\u2019t have any secondary index (on creation time) this will reduce our write latency. While reading, we don\u2019t need to filter on creation-time as our primary key has epoch time included in it. 8. Cache We can introduce a cache for database servers to cache hot tweets and users. We can use an off-the-shelf solution like Memcache that can store the whole tweet objects. Application servers, before hitting database, can quickly check if the cache has desired tweets. Based on clients\u2019 usage patterns we can determine how many cache servers we need. Which cache replacement policy would best fit our needs? When the cache is full and we want to replace a tweet with a newer/hotter tweet, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently viewed tweet first. How can we have a more intelligent cache? If we go with 80-20 rule, that is 20% of tweets generating 80% of read traffic which means that certain tweets are so popular that a majority of people read them. This dictates that we can try to cache 20% of daily read volume from each shard. What if we cache the latest data? Our service can benefit from this approach. Let\u2019s say if 80% of our users see tweets from the past three days only; we can try to cache all the tweets from the past three days. Let\u2019s say we have dedicated cache servers that cache all the tweets from all the users from the past three days. As estimated above, we are getting 100 million new tweets or 30GB of new data every day (without photos and videos). If we want to store all the tweets from last three days, we will need less than 100GB of memory. This data can easily fit into one server, but we should replicate it onto multiple servers to distribute all the read traffic to reduce the load on cache servers. So whenever we are generating a user\u2019s timeline, we can ask the cache servers if they have all the recent tweets for that user. If yes, we can simply return all the data from the cache. If we don\u2019t have enough tweets in the cache, we have to query the backend server to fetch that data. On a similar design, we can try caching photos and videos from the last three days. Our cache would be like a hash table where \u2018key\u2019 would be \u2018OwnerID\u2019 and \u2018value\u2019 would be a doubly linked list containing all the tweets from that user in the past three days. Since we want to retrieve the most recent data first, we can always insert new tweets at the head of the linked list, which means all the older tweets will be near the tail of the linked list. Therefore, we can remove tweets from the tail to make space for newer tweets. 9. Timeline Generation For a detailed discussion about timeline generation, take a look at Designing Facebook\u2019s Newsfeed. 10. Replication and Fault Tolerance Since our system is read-heavy, we can have multiple secondary database servers for each DB partition. Secondary servers will be used for read traffic only. All writes will first go to the primary server and then will be replicated to secondary servers. This scheme will also give us fault tolerance, since whenever the primary server goes down we can failover to a secondary server. 11. Load Balancing We can add Load balancing layer at three places in our system 1) Between Clients and Application servers 2) Between Application servers and database replication servers and 3) Between Aggregation servers and Cache server. Initially, a simple Round Robin approach can be adopted; that distributes incoming requests equally among servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is that it won\u2019t take servers load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that. 12. Monitoring Having the ability to monitor our systems is crucial. We should constantly collect data to get an instant insight into how our system is doing. We can collect following metrics/counters to get an understanding of the performance of our service: New tweets per day/second, what is the daily peak? Timeline delivery stats, how many tweets per day/second our service is delivering. Average latency that is seen by the user to refresh timeline. By monitoring these counters, we will realize if we need more replication, load balancing, or caching. 13. Extended Requirements How do we serve feeds? Get all the latest tweets from the people someone follows and merge/sort them by time. Use pagination to fetch/show tweets. Only fetch top N tweets from all the people someone follows. This N will depend on the client\u2019s Viewport, since on a mobile we show fewer tweets compared to a Web client. We can also cache next top tweets to speed things up. Alternately, we can pre-generate the feed to improve efficiency; for details please see \u2018Ranking and timeline generation\u2019 under Designing Instagram. Retweet: With each Tweet object in the database, we can store the ID of the original Tweet and not store any contents on this retweet object. Trending Topics: We can cache most frequently occurring hashtags or search queries in the last N seconds and keep updating them after every M seconds. We can rank trending topics based on the frequency of tweets or search queries or retweets or likes. We can give more weight to topics which are shown to more people. Who to follow? How to give suggestions? This feature will improve user engagement. We can suggest friends of people someone follows. We can go two or three levels down to find famous people for the suggestions. We can give preference to people with more followers. As only a few suggestions can be made at any time, use Machine Learning (ML) to shuffle and re-prioritize. ML signals could include people with recently increased follow-ship, common followers if the other person is following this user, common location or interests, etc. Moments: Get top news for different websites for past 1 or 2 hours, figure out related tweets, prioritize them, categorize them (news, support, financial, entertainment, etc.) using ML \u2013 supervised learning or Clustering. Then we can show these articles as trending topics in Moments. Search: s Search involves Indexing, Ranking, and Retrieval of tweets. A similar solution is discussed in our next problem Design Twitter Search.","title":"Designing Twitter"},{"location":"DesigningTwitter/#designing-twitter","text":"Let's design a Twitter-like social networking service. Users of the service will be able to post tweets, follow other people, and favorite tweets. Difficulty Level: Medium","title":"Designing Twitter"},{"location":"DesigningTwitter/#1-what-is-twitter","text":"Twitter is an online social networking service where users post and read short 140-character messages called \"tweets.\" Registered users can post and read tweets, but those who are not registered can only read them. Users access Twitter through their website interface, SMS, or mobile app.","title":"1. What is Twitter?"},{"location":"DesigningTwitter/#2-requirements-and-goals-of-the-system","text":"We will be designing a simpler version of Twitter with the following requirements: Functional Requirements Users should be able to post new tweets. A user should be able to follow other users. Users should be able to mark tweets as favorites. The service should be able to create and display a user\u2019s timeline consisting of top tweets from all the people the user follows. Tweets can contain photos and videos. Non-functional Requirements Our service needs to be highly available. Acceptable latency of the system is 200ms for timeline generation. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a tweet for a while, it should be fine. Extended Requirements Searching for tweets. Replying to a tweet. Trending topics \u2013 current hot topics/searches. Tagging other users. Tweet Notification. Who to follow? Suggestions? Moments.","title":"2. Requirements and Goals of the System"},{"location":"DesigningTwitter/#3-capacity-estimation-and-constraints","text":"Let\u2019s assume we have one billion total users with 200 million daily active users (DAU). Also assume we have 100 million new tweets every day and on average each user follows 200 people. How many favorites per day? If, on average, each user favorites five tweets per day we will have: 200M users * 5 favorites => 1B favorites How many total tweet-views will our system generate? Let\u2019s assume on average a user visits their timeline two times a day and visits five other people\u2019s pages. On each page if a user sees 20 tweets, then our system will generate 28B/day total tweet-views: 200M DAU * ((2 + 5) * 20 tweets) => 28B/day Storage Estimates Let\u2019s say each tweet has 140 characters and we need two bytes to store a character without compression. Let\u2019s assume we need 30 bytes to store metadata with each tweet (like ID, timestamp, user ID, etc.). Total storage we would need: 100M * (280 + 30) bytes => 30GB/day What would our storage needs be for five years? How much storage we would need for users\u2019 data, follows, favorites? We will leave this for the exercise. Not all tweets will have media, let\u2019s assume that on average every fifth tweet has a photo and every tenth has a video. Let\u2019s also assume on average a photo is 200KB and a video is 2MB. This will lead us to have 24TB of new media every day. (100M/5 photos * 200KB) + (100M/10 videos * 2MB) ~= 24TB/day Bandwidth Estimates Since total ingress is 24TB per day, this would translate into 290MB/sec. Remember that we have 28B tweet views per day. We must show the photo of every tweet (if it has a photo), but let\u2019s assume that the users watch every 3rd video they see in their timeline. So, total egress will be: (28B * 280 bytes) / 86400s of text => 93MB/s + (28B/5 * 200KB ) / 86400s of photos => 13GB/S + (28B/10/3 * 2MB ) / 86400s of Videos => 22GB/s Total ~= 35GB/s","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningTwitter/#4-system-apis","text":"\ud83d\udca1 Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system. We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definition of the API for posting a new tweet: tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. tweet_data (string): The text of the tweet, typically up to 140 characters. tweet_location (string): Optional location (longitude, latitude) this Tweet refers to. user_location (string): Optional location (longitude, latitude) of the user adding the tweet. media_ids (number[]): Optional list of media_ids to be associated with the Tweet. (all the media photo, video, etc. need to be uploaded separately). Returns: (string) A successful post will return the URL to access that tweet. Otherwise, an appropriate HTTP error is returned.","title":"4. System APIs"},{"location":"DesigningTwitter/#5-high-level-system-design","text":"We need a system that can efficiently store all the new tweets, 100M/86400s => 1150 tweets per second and read 28B/86400s => 325K tweets per second. It is clear from the requirements that this will be a read-heavy system. At a high level, we need multiple application servers to serve all these requests with load balancers in front of them for traffic distributions. On the backend, we need an efficient database that can store all the new tweets and can support a huge number of reads. We also need some file storage to store photos and videos. Although our expected daily write load is 100 million and read load is 28 billion tweets. This means on average our system will receive around 1160 new tweets and 325K read requests per second. This traffic will be distributed unevenly throughout the day, though, at peak time we should expect at least a few thousand write requests and around 1M read requests per second. We should keep this in mind while designing the architecture of our system.","title":"5. High Level System Design"},{"location":"DesigningTwitter/#6-database-schema","text":"We need to store data about users, their tweets, their favorite tweets, and people they follow. For choosing between SQL and NoSQL databases to store the above schema, please see \u2018Database schema\u2019 under Designing Instagram.","title":"6. Database Schema"},{"location":"DesigningTwitter/#7-data-sharding","text":"Since we have a huge number of new tweets every day and our read load is extremely high too, we need to distribute our data onto multiple machines such that we can read/write it efficiently. We have many options to shard our data; let\u2019s go through them one by one: Sharding based on UserID: We can try storing all the data of a user on one server. While storing, we can pass the UserID to our hash function that will map the user to a database server where we will store all of the user\u2019s tweets, favorites, follows, etc. While querying for tweets/follows/favorites of a user, we can ask our hash function where can we find the data of a user and then read it from there. This approach has a couple of issues: What if a user becomes hot? There could be a lot of queries on the server holding the user. This high load will affect the performance of our service. Over time some users can end up storing a lot of tweets or having a lot of follows compared to others. Maintaining a uniform distribution of growing user data is quite difficult. To recover from these situations either we have to repartition/redistribute our data or use consistent hashing. Sharding based on TweetID: Our hash function will map each TweetID to a random server where we will store that Tweet. To search for tweets, we have to query all servers, and each server will return a set of tweets. A centralized server will aggregate these results to return them to the user. Let\u2019s look into timeline generation example; here are the number of steps our system has to perform to generate a user\u2019s timeline: Our application (app) server will find all the people the user follows. App server will send the query to all database servers to find tweets from these people. Each database server will find the tweets for each user, sort them by recency and return the top tweets. App server will merge all the results and sort them again to return the top results to the user. This approach solves the problem of hot users, but, in contrast to sharding by UserID, we have to query all database partitions to find tweets of a user, which can result in higher latencies. We can further improve our performance by introducing cache to store hot tweets in front of the database servers. Sharding based on Tweet creation time: Storing tweets based on creation time will give us the advantage of fetching all the top tweets quickly and we only have to query a very small set of servers. The problem here is that the traffic load will not be distributed, e.g., while writing, all new tweets will be going to one server and the remaining servers will be sitting idle. Similarly, while reading, the server holding the latest data will have a very high load as compared to servers holding old data. What if we can combine sharding by TweetID and Tweet creation time? If we don\u2019t store tweet creation time separately and use TweetID to reflect that, we can get benefits of both the approaches. This way it will be quite quick to find the latest Tweets. For this, we must make each TweetID universally unique in our system and each TweetID should contain a timestamp too. We can use epoch time for this. Let\u2019s say our TweetID will have two parts: the first part will be representing epoch seconds and the second part will be an auto-incrementing sequence. So, to make a new TweetID, we can take the current epoch time and append an auto-incrementing number to it. We can figure out the shard number from this TweetID and store it there. What could be the size of our TweetID? Let\u2019s say our epoch time starts today, how many bits we would need to store the number of seconds for the next 50 years? 86400 sec/day * 365 (days a year) * 50 (years) => 1.6B We would need 31 bits to store this number. Since on average we are expecting 1150 new tweets per second, we can allocate 17 bits to store auto incremented sequence; this will make our TweetID 48 bits long. So, every second we can store (2^17 => 130K) new tweets. We can reset our auto incrementing sequence every second. For fault tolerance and better performance, we can have two database servers to generate auto-incrementing keys for us, one generating even numbered keys and the other generating odd numbered keys. If we assume our current epoch seconds are \u201c1483228800,\u201d our TweetID will look like this: 1483228800 000001 1483228800 000002 1483228800 000003 1483228800 000004 \u2026 If we make our TweetID 64bits (8 bytes) long, we can easily store tweets for the next 100 years and also store them for mili-seconds granularity. In the above approach, we still have to query all the servers for timeline generation, but our reads (and writes) will be substantially quicker. Since we don\u2019t have any secondary index (on creation time) this will reduce our write latency. While reading, we don\u2019t need to filter on creation-time as our primary key has epoch time included in it.","title":"7. Data Sharding"},{"location":"DesigningTwitter/#8-cache","text":"We can introduce a cache for database servers to cache hot tweets and users. We can use an off-the-shelf solution like Memcache that can store the whole tweet objects. Application servers, before hitting database, can quickly check if the cache has desired tweets. Based on clients\u2019 usage patterns we can determine how many cache servers we need. Which cache replacement policy would best fit our needs? When the cache is full and we want to replace a tweet with a newer/hotter tweet, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently viewed tweet first. How can we have a more intelligent cache? If we go with 80-20 rule, that is 20% of tweets generating 80% of read traffic which means that certain tweets are so popular that a majority of people read them. This dictates that we can try to cache 20% of daily read volume from each shard. What if we cache the latest data? Our service can benefit from this approach. Let\u2019s say if 80% of our users see tweets from the past three days only; we can try to cache all the tweets from the past three days. Let\u2019s say we have dedicated cache servers that cache all the tweets from all the users from the past three days. As estimated above, we are getting 100 million new tweets or 30GB of new data every day (without photos and videos). If we want to store all the tweets from last three days, we will need less than 100GB of memory. This data can easily fit into one server, but we should replicate it onto multiple servers to distribute all the read traffic to reduce the load on cache servers. So whenever we are generating a user\u2019s timeline, we can ask the cache servers if they have all the recent tweets for that user. If yes, we can simply return all the data from the cache. If we don\u2019t have enough tweets in the cache, we have to query the backend server to fetch that data. On a similar design, we can try caching photos and videos from the last three days. Our cache would be like a hash table where \u2018key\u2019 would be \u2018OwnerID\u2019 and \u2018value\u2019 would be a doubly linked list containing all the tweets from that user in the past three days. Since we want to retrieve the most recent data first, we can always insert new tweets at the head of the linked list, which means all the older tweets will be near the tail of the linked list. Therefore, we can remove tweets from the tail to make space for newer tweets.","title":"8. Cache"},{"location":"DesigningTwitter/#9-timeline-generation","text":"For a detailed discussion about timeline generation, take a look at Designing Facebook\u2019s Newsfeed.","title":"9. Timeline Generation"},{"location":"DesigningTwitter/#10-replication-and-fault-tolerance","text":"Since our system is read-heavy, we can have multiple secondary database servers for each DB partition. Secondary servers will be used for read traffic only. All writes will first go to the primary server and then will be replicated to secondary servers. This scheme will also give us fault tolerance, since whenever the primary server goes down we can failover to a secondary server.","title":"10. Replication and Fault Tolerance"},{"location":"DesigningTwitter/#11-load-balancing","text":"We can add Load balancing layer at three places in our system 1) Between Clients and Application servers 2) Between Application servers and database replication servers and 3) Between Aggregation servers and Cache server. Initially, a simple Round Robin approach can be adopted; that distributes incoming requests equally among servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is that it won\u2019t take servers load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.","title":"11. Load Balancing"},{"location":"DesigningTwitter/#12-monitoring","text":"Having the ability to monitor our systems is crucial. We should constantly collect data to get an instant insight into how our system is doing. We can collect following metrics/counters to get an understanding of the performance of our service: New tweets per day/second, what is the daily peak? Timeline delivery stats, how many tweets per day/second our service is delivering. Average latency that is seen by the user to refresh timeline. By monitoring these counters, we will realize if we need more replication, load balancing, or caching.","title":"12. Monitoring"},{"location":"DesigningTwitter/#13-extended-requirements","text":"How do we serve feeds? Get all the latest tweets from the people someone follows and merge/sort them by time. Use pagination to fetch/show tweets. Only fetch top N tweets from all the people someone follows. This N will depend on the client\u2019s Viewport, since on a mobile we show fewer tweets compared to a Web client. We can also cache next top tweets to speed things up. Alternately, we can pre-generate the feed to improve efficiency; for details please see \u2018Ranking and timeline generation\u2019 under Designing Instagram. Retweet: With each Tweet object in the database, we can store the ID of the original Tweet and not store any contents on this retweet object. Trending Topics: We can cache most frequently occurring hashtags or search queries in the last N seconds and keep updating them after every M seconds. We can rank trending topics based on the frequency of tweets or search queries or retweets or likes. We can give more weight to topics which are shown to more people. Who to follow? How to give suggestions? This feature will improve user engagement. We can suggest friends of people someone follows. We can go two or three levels down to find famous people for the suggestions. We can give preference to people with more followers. As only a few suggestions can be made at any time, use Machine Learning (ML) to shuffle and re-prioritize. ML signals could include people with recently increased follow-ship, common followers if the other person is following this user, common location or interests, etc. Moments: Get top news for different websites for past 1 or 2 hours, figure out related tweets, prioritize them, categorize them (news, support, financial, entertainment, etc.) using ML \u2013 supervised learning or Clustering. Then we can show these articles as trending topics in Moments. Search: s Search involves Indexing, Ranking, and Retrieval of tweets. A similar solution is discussed in our next problem Design Twitter Search.","title":"13. Extended Requirements"},{"location":"DesigningTwitterSearch/","text":"Designing Twitter Search Twitter is one of the largest social networking service where users can share photos, news, and text-based messages. In this chapter, we will design a service that can store and search user tweets. Similar Problems: Tweet search. Difficulty Level: Medium 1. What is Twitter Search? Twitter users can update their status whenever they like. Each status (called tweet) consists of plain text and our goal is to design a system that allows searching over all the user tweets. 2. Requirements and Goals of the System Let\u2019s assume Twitter has 1.5 billion total users with 800 million daily active users. On average Twitter gets 400 million tweets every day. The average size of a tweet is 300 bytes. Let\u2019s assume there will be 500M searches every day. The search query will consist of multiple words combined with AND/OR. We need to design a system that can efficiently store and query tweets. 3. Capacity Estimation and Constraints Storage Capacity: Since we have 400 million new tweets every day and each tweet on average is 300 bytes, the total storage we need, will be: 400M * 300 => 120GB/day Total storage per second: 120GB / 24hours / 3600sec ~= 1.38MB/second 4. System APIs We can have SOAP or REST APIs to expose the functionality of our service; following could be the definition of the search API: search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. maximum_results_to_return (number): Number of tweets to return. sort (number): Optional sort mode: Latest first (0 - default), Best matched (1), Most liked (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of tweets matching the search query. Each result entry can have the user ID & name, tweet text, tweet ID, creation time, number of likes, etc. 5. High Level Design At the high level, we need to store all the statues in a database and also build an index that can keep track of which word appears in which tweet. This index will help us quickly find tweets that users are trying to search. 6. Detailed Component Design 1. Storage: We need to store 120GB of new data every day. Given this huge amount of data, we need to come up with a data partitioning scheme that will be efficiently distributing the data onto multiple servers. If we plan for next five years, we will need the following storage: 120GB * 365days * 5years ~= 200TB If we never want to be more than 80% full at any time, we approximately will need 250TB of total storage. Let\u2019s assume that we want to keep an extra copy of all tweets for fault tolerance; then, our total storage requirement will be 500TB. If we assume a modern server can store up to 4TB of data, we would need 125 such servers to hold all of the required data for the next five years. Let\u2019s start with a simplistic design where we store the tweets in a MySQL database. We can assume that we store the tweets in a table having two columns, TweetID and TweetText. Let\u2019s assume we partition our data based on TweetID. If our TweetIDs are unique system-wide, we can define a hash function that can map a TweetID to a storage server where we can store that tweet object. How can we create system-wide unique TweetIDs? If we are getting 400M new tweets each day, then how many tweet objects we can expect in five years? 400M * 365 days * 5 years => 730 billion This means we would need a five bytes number to identify TweetIDs uniquely. Let\u2019s assume we have a service that can generate a unique TweetID whenever we need to store an object (The TweetID discussed here will be similar to TweetID discussed in Designing Twitter). We can feed the TweetID to our hash function to find the storage server and store our tweet object there. 2. Index: What should our index look like? Since our tweet queries will consist of words, let\u2019s build the index that can tell us which word comes in which tweet object. Let\u2019s first estimate how big our index will be. If we want to build an index for all the English words and some famous nouns like people names, city names, etc., and if we assume that we have around 300K English words and 200K nouns, then we will have 500k total words in our index. Let\u2019s assume that the average length of a word is five characters. If we are keeping our index in memory, we need 2.5MB of memory to store all the words: 500K * 5 => 2.5 MB Let\u2019s assume that we want to keep the index in memory for all the tweets from only past two years. Since we will be getting 730B tweets in 5 years, this will give us 292B tweets in two years. Given that each TweetID will be 5 bytes, how much memory will we need to store all the TweetIDs? 292B * 5 => 1460 GB So our index would be like a big distributed hash table, where \u2018key\u2019 would be the word and \u2018value\u2019 will be a list of TweetIDs of all those tweets which contain that word. Assuming on average we have 40 words in each tweet and since we will not be indexing prepositions and other small words like \u2018the\u2019, \u2018an\u2019, \u2018and\u2019 etc., let\u2019s assume we will have around 15 words in each tweet that need to be indexed. This means each TweetID will be stored 15 times in our index. So total memory we will need to store our index: (1460 * 15) + 2.5MB ~= 21 TB Assuming a high-end server has 144GB of memory, we would need 152 such servers to hold our index. We can partition our data based on two criteria: Sharding based on Words: While building our index, we will iterate through all the words of a tweet and calculate the hash of each word to find the server where it would be indexed. To find all tweets containing a specific word we have to query only the server which contains this word. We have a couple of issues with this approach: What if a word becomes hot? Then there will be a lot of queries on the server holding that word. This high load will affect the performance of our service. Over time, some words can end up storing a lot of TweetIDs compared to others, therefore, maintaining a uniform distribution of words while tweets are growing is quite tricky. To recover from these situations we either have to repartition our data or use Consistent Hashing. Sharding based on the tweet object: While storing, we will pass the TweetID to our hash function to find the server and index all the words of the tweet on that server. While querying for a particular word, we have to query all the servers, and each server will return a set of TweetIDs. A centralized server will aggregate these results to return them to the user. 7. Fault Tolerance What will happen when an index server dies? We can have a secondary replica of each server and if the primary server dies it can take control after the failover. Both primary and secondary servers will have the same copy of the index. What if both primary and secondary servers die at the same time? We have to allocate a new server and rebuild the same index on it. How can we do that? We don\u2019t know what words/tweets were kept on this server. If we were using \u2018Sharding based on the tweet object\u2019, the brute-force solution would be to iterate through the whole database and filter TweetIDs using our hash function to figure out all the required tweets that would be stored on this server. This would be inefficient and also during the time when the server was being rebuilt we would not be able to serve any query from it, thus missing some tweets that should have been seen by the user. How can we efficiently retrieve a mapping between tweets and the index server? We have to build a reverse index that will map all the TweetID to their index server. Our Index-Builder server can hold this information. We will need to build a Hashtable where the \u2018key\u2019 will be the index server number and the \u2018value\u2019 will be a HashSet containing all the TweetIDs being kept at that index server. Notice that we are keeping all the TweetIDs in a HashSet; this will enable us to add/remove tweets from our index quickly. So now, whenever an index server has to rebuild itself, it can simply ask the Index-Builder server for all the tweets it needs to store and then fetch those tweets to build the index. This approach will surely be fast. We should also have a replica of the Index-Builder server for fault tolerance. 8. Cache To deal with hot tweets we can introduce a cache in front of our database. We can use Memcached, which can store all such hot tweets in memory. Application servers, before hitting the backend database, can quickly check if the cache has that tweet. Based on clients\u2019 usage patterns, we can adjust how many cache servers we need. For cache eviction policy, Least Recently Used (LRU) seems suitable for our system. 9. Load Balancing We can add a load balancing layer at two places in our system 1) Between Clients and Application servers and 2) Between Application servers and Backend server. Initially, a simple Round Robin approach can be adopted; that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is LB will take dead servers out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is it won\u2019t take server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries the backend server about their load and adjust traffic based on that. 10. Ranking How about if we want to rank the search results by social graph distance, popularity, relevance, etc? Let\u2019s assume we want to rank tweets by popularity, like how many likes or comments a tweet is getting, etc. In such a case, our ranking algorithm can calculate a \u2018popularity number\u2019 (based on the number of likes, etc.) and store it with the index. Each partition can sort the results based on this popularity number before returning results to the aggregator server. The aggregator server combines all these results, sorts them based on the popularity number, and sends the top results to the user.","title":"Designing Twitter Search"},{"location":"DesigningTwitterSearch/#designing-twitter-search","text":"Twitter is one of the largest social networking service where users can share photos, news, and text-based messages. In this chapter, we will design a service that can store and search user tweets. Similar Problems: Tweet search. Difficulty Level: Medium","title":"Designing Twitter Search"},{"location":"DesigningTwitterSearch/#1-what-is-twitter-search","text":"Twitter users can update their status whenever they like. Each status (called tweet) consists of plain text and our goal is to design a system that allows searching over all the user tweets.","title":"1. What is Twitter Search?"},{"location":"DesigningTwitterSearch/#2-requirements-and-goals-of-the-system","text":"Let\u2019s assume Twitter has 1.5 billion total users with 800 million daily active users. On average Twitter gets 400 million tweets every day. The average size of a tweet is 300 bytes. Let\u2019s assume there will be 500M searches every day. The search query will consist of multiple words combined with AND/OR. We need to design a system that can efficiently store and query tweets.","title":"2. Requirements and Goals of the System"},{"location":"DesigningTwitterSearch/#3-capacity-estimation-and-constraints","text":"Storage Capacity: Since we have 400 million new tweets every day and each tweet on average is 300 bytes, the total storage we need, will be: 400M * 300 => 120GB/day Total storage per second: 120GB / 24hours / 3600sec ~= 1.38MB/second","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningTwitterSearch/#4-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service; following could be the definition of the search API: search(api_dev_key, search_terms, maximum_results_to_return, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. maximum_results_to_return (number): Number of tweets to return. sort (number): Optional sort mode: Latest first (0 - default), Best matched (1), Most liked (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of tweets matching the search query. Each result entry can have the user ID & name, tweet text, tweet ID, creation time, number of likes, etc.","title":"4. System APIs"},{"location":"DesigningTwitterSearch/#5-high-level-design","text":"At the high level, we need to store all the statues in a database and also build an index that can keep track of which word appears in which tweet. This index will help us quickly find tweets that users are trying to search.","title":"5. High Level Design"},{"location":"DesigningTwitterSearch/#6-detailed-component-design","text":"1. Storage: We need to store 120GB of new data every day. Given this huge amount of data, we need to come up with a data partitioning scheme that will be efficiently distributing the data onto multiple servers. If we plan for next five years, we will need the following storage: 120GB * 365days * 5years ~= 200TB If we never want to be more than 80% full at any time, we approximately will need 250TB of total storage. Let\u2019s assume that we want to keep an extra copy of all tweets for fault tolerance; then, our total storage requirement will be 500TB. If we assume a modern server can store up to 4TB of data, we would need 125 such servers to hold all of the required data for the next five years. Let\u2019s start with a simplistic design where we store the tweets in a MySQL database. We can assume that we store the tweets in a table having two columns, TweetID and TweetText. Let\u2019s assume we partition our data based on TweetID. If our TweetIDs are unique system-wide, we can define a hash function that can map a TweetID to a storage server where we can store that tweet object. How can we create system-wide unique TweetIDs? If we are getting 400M new tweets each day, then how many tweet objects we can expect in five years? 400M * 365 days * 5 years => 730 billion This means we would need a five bytes number to identify TweetIDs uniquely. Let\u2019s assume we have a service that can generate a unique TweetID whenever we need to store an object (The TweetID discussed here will be similar to TweetID discussed in Designing Twitter). We can feed the TweetID to our hash function to find the storage server and store our tweet object there. 2. Index: What should our index look like? Since our tweet queries will consist of words, let\u2019s build the index that can tell us which word comes in which tweet object. Let\u2019s first estimate how big our index will be. If we want to build an index for all the English words and some famous nouns like people names, city names, etc., and if we assume that we have around 300K English words and 200K nouns, then we will have 500k total words in our index. Let\u2019s assume that the average length of a word is five characters. If we are keeping our index in memory, we need 2.5MB of memory to store all the words: 500K * 5 => 2.5 MB Let\u2019s assume that we want to keep the index in memory for all the tweets from only past two years. Since we will be getting 730B tweets in 5 years, this will give us 292B tweets in two years. Given that each TweetID will be 5 bytes, how much memory will we need to store all the TweetIDs? 292B * 5 => 1460 GB So our index would be like a big distributed hash table, where \u2018key\u2019 would be the word and \u2018value\u2019 will be a list of TweetIDs of all those tweets which contain that word. Assuming on average we have 40 words in each tweet and since we will not be indexing prepositions and other small words like \u2018the\u2019, \u2018an\u2019, \u2018and\u2019 etc., let\u2019s assume we will have around 15 words in each tweet that need to be indexed. This means each TweetID will be stored 15 times in our index. So total memory we will need to store our index: (1460 * 15) + 2.5MB ~= 21 TB Assuming a high-end server has 144GB of memory, we would need 152 such servers to hold our index. We can partition our data based on two criteria: Sharding based on Words: While building our index, we will iterate through all the words of a tweet and calculate the hash of each word to find the server where it would be indexed. To find all tweets containing a specific word we have to query only the server which contains this word. We have a couple of issues with this approach: What if a word becomes hot? Then there will be a lot of queries on the server holding that word. This high load will affect the performance of our service. Over time, some words can end up storing a lot of TweetIDs compared to others, therefore, maintaining a uniform distribution of words while tweets are growing is quite tricky. To recover from these situations we either have to repartition our data or use Consistent Hashing. Sharding based on the tweet object: While storing, we will pass the TweetID to our hash function to find the server and index all the words of the tweet on that server. While querying for a particular word, we have to query all the servers, and each server will return a set of TweetIDs. A centralized server will aggregate these results to return them to the user.","title":"6. Detailed Component Design"},{"location":"DesigningTwitterSearch/#7-fault-tolerance","text":"What will happen when an index server dies? We can have a secondary replica of each server and if the primary server dies it can take control after the failover. Both primary and secondary servers will have the same copy of the index. What if both primary and secondary servers die at the same time? We have to allocate a new server and rebuild the same index on it. How can we do that? We don\u2019t know what words/tweets were kept on this server. If we were using \u2018Sharding based on the tweet object\u2019, the brute-force solution would be to iterate through the whole database and filter TweetIDs using our hash function to figure out all the required tweets that would be stored on this server. This would be inefficient and also during the time when the server was being rebuilt we would not be able to serve any query from it, thus missing some tweets that should have been seen by the user. How can we efficiently retrieve a mapping between tweets and the index server? We have to build a reverse index that will map all the TweetID to their index server. Our Index-Builder server can hold this information. We will need to build a Hashtable where the \u2018key\u2019 will be the index server number and the \u2018value\u2019 will be a HashSet containing all the TweetIDs being kept at that index server. Notice that we are keeping all the TweetIDs in a HashSet; this will enable us to add/remove tweets from our index quickly. So now, whenever an index server has to rebuild itself, it can simply ask the Index-Builder server for all the tweets it needs to store and then fetch those tweets to build the index. This approach will surely be fast. We should also have a replica of the Index-Builder server for fault tolerance.","title":"7. Fault Tolerance"},{"location":"DesigningTwitterSearch/#8-cache","text":"","title":"8. Cache"},{"location":"DesigningTwitterSearch/#_1","text":"To deal with hot tweets we can introduce a cache in front of our database. We can use Memcached, which can store all such hot tweets in memory. Application servers, before hitting the backend database, can quickly check if the cache has that tweet. Based on clients\u2019 usage patterns, we can adjust how many cache servers we need. For cache eviction policy, Least Recently Used (LRU) seems suitable for our system.","title":""},{"location":"DesigningTwitterSearch/#9-load-balancing","text":"","title":"9. Load Balancing"},{"location":"DesigningTwitterSearch/#_2","text":"We can add a load balancing layer at two places in our system 1) Between Clients and Application servers and 2) Between Application servers and Backend server. Initially, a simple Round Robin approach can be adopted; that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is LB will take dead servers out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is it won\u2019t take server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries the backend server about their load and adjust traffic based on that.","title":""},{"location":"DesigningTwitterSearch/#10-ranking","text":"How about if we want to rank the search results by social graph distance, popularity, relevance, etc? Let\u2019s assume we want to rank tweets by popularity, like how many likes or comments a tweet is getting, etc. In such a case, our ranking algorithm can calculate a \u2018popularity number\u2019 (based on the number of likes, etc.) and store it with the index. Each partition can sort the results based on this popularity number before returning results to the aggregator server. The aggregator server combines all these results, sorts them based on the popularity number, and sends the top results to the user.","title":"10. Ranking"},{"location":"DesigningTypeaheadSuggestion/","text":"Designing Typeahead Suggestion Let's design a real-time suggestion service, which will recommend terms to users as they enter text for searching. Similar Services: Auto-suggestions, Typeahead search Difficulty: Medium 1. What is Typeahead Suggestion? Typeahead suggestions enable users to search for known and frequently searched terms. As the user types into the search box, it tries to predict the query based on the characters the user has entered and gives a list of suggestions to complete the query. Typeahead suggestions help the user to articulate their search queries better. It\u2019s not about speeding up the search process but rather about guiding the users and lending them a helping hand in constructing their search query. 2. Requirements and Goals of the System Functional Requirements: As the user types in their query, our service should suggest top 10 terms starting with whatever the user has typed. Non-function Requirements: The suggestions should appear in real-time. The user should be able to see the suggestions within 200ms. 3. Basic System Design and Algorithm The problem we are solving is that we have a lot of \u2018strings\u2019 that we need to store in such a way that users can search with any prefix. Our service will suggest next terms that will match the given prefix. For example, if our database contains the following terms: cap, cat, captain, or capital and the user has typed in \u2018cap\u2019, our system should suggest \u2018cap\u2019, \u2018captain\u2019 and \u2018capital\u2019. Since we\u2019ve got to serve a lot of queries with minimum latency, we need to come up with a scheme that can efficiently store our data such that it can be queried quickly. We can\u2019t depend upon some database for this; we need to store our index in memory in a highly efficient data structure. One of the most appropriate data structures that can serve our purpose is the Trie (pronounced \u201ctry\u201d). A trie is a tree-like data structure used to store phrases where each node stores a character of the phrase in a sequential manner. For example, if we need to store \u2018cap, cat, caption, captain, capital\u2019 in the trie, it would look like: Now if the user has typed \u2018cap\u2019, our service can traverse the trie to go to the node \u2018P\u2019 to find all the terms that start with this prefix (e.g., cap-tion, cap-ital etc). We can merge nodes that have only one branch to save storage space. The above trie can be stored like this: Should we have case insensitive trie? For simplicity and search use-case, let\u2019s assume our data is case insensitive. How to find top suggestion? Now that we can find all the terms for a given prefix, how can we find the top 10 terms for the given prefix? One simple solution could be to store the count of searches that terminated at each node, e.g., if users have searched about \u2018CAPTAIN\u2019 100 times and \u2018CAPTION\u2019 500 times, we can store this number with the last character of the phrase. Now if the user types \u2018CAP\u2019 we know the top most searched word under the prefix \u2018CAP\u2019 is \u2018CAPTION\u2019. So, to find the top suggestions for a given prefix, we can traverse the sub-tree under it. Given a prefix, how much time will it take to traverse its sub-tree? Given the amount of data we need to index, we should expect a huge tree. Even traversing a sub-tree would take really long, e.g., the phrase \u2018system design interview questions\u2019 is 30 levels deep. Since we have very strict latency requirements we do need to improve the efficiency of our solution. Can we store top suggestions with each node? This can surely speed up our searches but will require a lot of extra storage. We can store top 10 suggestions at each node that we can return to the user. We have to bear the big increase in our storage capacity to achieve the required efficiency. We can optimize our storage by storing only references of the terminal nodes rather than storing the entire phrase. To find the suggested terms we need to traverse back using the parent reference from the terminal node. We will also need to store the frequency with each reference to keep track of top suggestions. How would we build this trie? We can efficiently build our trie bottom up. Each parent node will recursively call all the child nodes to calculate their top suggestions and their counts. Parent nodes will combine top suggestions from all of their children to determine their top suggestions. How to update the trie? Assuming five billion searches every day, which would give us approximately 60K queries per second. If we try to update our trie for every query it\u2019ll be extremely resource intensive and this can hamper our read requests, too. One solution to handle this could be to update our trie offline after a certain interval. As the new queries come in we can log them and also track their frequencies. Either we can log every query or do sampling and log every 1000th query. For example, if we don\u2019t want to show a term which is searched for less than 1000 times, it\u2019s safe to log every 1000th searched term. We can have a Map-Reduce (MR) set-up to process all the logging data periodically say every hour. These MR jobs will calculate frequencies of all searched terms in the past hour. We can then update our trie with this new data. We can take the current snapshot of the trie and update it with all the new terms and their frequencies. We should do this offline as we don\u2019t want our read queries to be blocked by update trie requests. We can have two options: We can make a copy of the trie on each server to update it offline. Once done we can switch to start using it and discard the old one. Another option is we can have a master-slave configuration for each trie server. We can update slave while the master is serving traffic. Once the update is complete, we can make the slave our new master. We can later update our old master, which can then start serving traffic, too. How can we update the frequencies of typeahead suggestions? Since we are storing frequencies of our typeahead suggestions with each node, we need to update them too! We can update only differences in frequencies rather than recounting all search terms from scratch. If we\u2019re keeping count of all the terms searched in last 10 days, we\u2019ll need to subtract the counts from the time period no longer included and add the counts for the new time period being included. We can add and subtract frequencies based on Exponential Moving Average (EMA) of each term. In EMA, we give more weight to the latest data. It\u2019s also known as the exponentially weighted moving average. After inserting a new term in the trie, we\u2019ll go to the terminal node of the phrase and increase its frequency. Since we\u2019re storing the top 10 queries in each node, it is possible that this particular search term jumped into the top 10 queries of a few other nodes. So, we need to update the top 10 queries of those nodes then. We have to traverse back from the node to all the way up to the root. For every parent, we check if the current query is part of the top 10. If so, we update the corresponding frequency. If not, we check if the current query\u2019s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency. How can we remove a term from the trie? Let\u2019s say we have to remove a term from the trie because of some legal issue or hate or piracy etc. We can completely remove such terms from the trie when the regular update happens, meanwhile, we can add a filtering layer on each server which will remove any such term before sending them to users. What could be different ranking criteria for suggestions? In addition to a simple count, for terms ranking, we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc. 4. Permanent Storage of the Trie How to store trie in a file so that we can rebuild our trie easily - this will be needed when a machine restarts? We can take a snapshot of our trie periodically and store it in a file. This will enable us to rebuild a trie if the server goes down. To store, we can start with the root node and save the trie level-by-level. With each node, we can store what character it contains and how many children it has. Right after each node, we should put all of its children. Let\u2019s assume we have the following trie: If we store this trie in a file with the above-mentioned scheme, we will have: \u201cC2,A2,R1,T,P,O1,D\u201d. From this, we can easily rebuild our trie. If you\u2019ve noticed, we are not storing top suggestions and their counts with each node. It is hard to store this information; as our trie is being stored top down, we don\u2019t have child nodes created before the parent, so there is no easy way to store their references. For this, we have to recalculate all the top terms with counts. This can be done while we are building the trie. Each node will calculate its top suggestions and pass it to its parent. Each parent node will merge results from all of its children to figure out its top suggestions. 5. Scale Estimation If we are building a service that has the same scale as that of Google we can expect 5 billion searches every day, which would give us approximately 60K queries per second. Since there will be a lot of duplicates in 5 billion queries, we can assume that only 20% of these will be unique. If we only want to index the top 50% of the search terms, we can get rid of a lot of less frequently searched queries. Let\u2019s assume we will have 100 million unique terms for which we want to build an index. Storage Estimation: If on the average each query consists of 3 words and if the average length of a word is 5 characters, this will give us 15 characters of average query size. Assuming we need 2 bytes to store a character, we will need 30 bytes to store an average query. So total storage we will need: 100 million * 30 bytes => 3 GB We can expect some growth in this data every day, but we should also be removing some terms that are not searched anymore. If we assume we have 2% new queries every day and if we are maintaining our index for the last one year, total storage we should expect: 3GB + (0.02 * 3 GB * 365 days) => 25 GB 6. Data Partition Although our index can easily fit on one server, we can still partition it in order to meet our requirements of higher efficiency and lower latencies. How can we efficiently partition our data to distribute it onto multiple servers? a. Range Based Partitioning: What if we store our phrases in separate partitions based on their first letter. So we save all the terms starting with the letter \u2018A\u2019 in one partition and those that start with the letter \u2018B\u2019 into another partition and so on. We can even combine certain less frequently occurring letters into one partition. We should come up with this partitioning scheme statically so that we can always store and search terms in a predictable manner. The main problem with this approach is that it can lead to unbalanced servers, for instance, if we decide to put all terms starting with the letter \u2018E\u2019 into one partition, but later we realize that we have too many terms that start with letter \u2018E\u2019 that we can\u2019t fit into one partition. We can see that the above problem will happen with every statically defined scheme. It is not possible to calculate if each of our partitions will fit on one server statically. b. Partition based on the maximum capacity of the server: Let\u2019s say we partition our trie based on the maximum memory capacity of the servers. We can keep storing data on a server as long as it has memory available. Whenever a sub-tree cannot fit into a server, we break our partition there to assign that range to this server and move on the next server to repeat this process. Let\u2019s say if our first trie server can store all terms from \u2018A\u2019 to \u2018AABC\u2019, which mean our next server will store from \u2018AABD\u2019 onwards. If our second server could store up to \u2018BXA\u2019, the next server will start from \u2018BXB\u2019, and so on. We can keep a hash table to quickly access this partitioning scheme: Server 1, A-AABC Server 2, AABD-BXA Server 3, BXB-CDA For querying, if the user has typed \u2018A\u2019 we have to query both server 1 and 2 to find the top suggestions. When the user has typed \u2018AA\u2019, we still have to query server 1 and 2, but when the user has typed \u2018AAA\u2019 we only need to query server 1. We can have a load balancer in front of our trie servers which can store this mapping and redirect traffic. Also, if we are querying from multiple servers, either we need to merge the results on the server side to calculate the overall top results or make our clients do that. If we prefer to do this on the server side, we need to introduce another layer of servers between load balancers and trie severs (let\u2019s call them aggregator). These servers will aggregate results from multiple trie servers and return the top results to the client. Partitioning based on the maximum capacity can still lead us to hotspots, e.g., if there are a lot of queries for terms starting with \u2018cap\u2019, the server holding it will have a high load compared to others. c. Partition based on the hash of the term: Each term will be passed to a hash function, which will generate a server number and we will store the term on that server. This will make our term distribution random and hence minimize hotspots. The disadvantage of this scheme is, to find typeahead suggestions for a term we have to ask all the servers and then aggregate the results. 7. Cache We should realize that caching the top searched terms will be extremely helpful in our service. There will be a small percentage of queries that will be responsible for most of the traffic. We can have separate cache servers in front of the trie servers holding most frequently searched terms and their typeahead suggestions. Application servers should check these cache servers before hitting the trie servers to see if they have the desired searched terms. This will save us time to traverse the tri. We can also build a simple Machine Learning (ML) model that can try to predict the engagement on each suggestion based on simple counting, personalization, or trending data, and cache these terms beforehand. 8. Replication and Load Balancer We should have replicas for our trie servers both for load balancing and also for fault tolerance. We also need a load balancer that keeps track of our data partitioning scheme and redirects traffic based on the prefixes. 9. Fault Tolerance What will happen when a trie server goes down? As discussed above we can have a master-slave configuration; if the master dies, the slave can take over after failover. Any server that comes back up, can rebuild the trie based on the last snapshot. 10. Typeahead Client We can perform the following optimizations on the client side to improve user\u2019s experience: The client should only try hitting the server if the user has not pressed any key for 50ms. If the user is constantly typing, the client can cancel the in-progress requests. Initially, the client can wait until the user enters a couple of characters. Clients can pre-fetch some data from the server to save future requests. Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused. Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn\u2019t waste time in establishing the connection. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency. 11. Personalization Users will receive some typeahead suggestions based on their historical searches, location, language, etc. We can store the personal history of each user separately on the server and also cache them on the client. The server can add these personalized terms in the final set before sending it to the user. Personalized searches should always come before others.","title":"Designing Typeahead Suggestion"},{"location":"DesigningTypeaheadSuggestion/#designing-typeahead-suggestion","text":"Let's design a real-time suggestion service, which will recommend terms to users as they enter text for searching. Similar Services: Auto-suggestions, Typeahead search Difficulty: Medium","title":"Designing Typeahead Suggestion"},{"location":"DesigningTypeaheadSuggestion/#1-what-is-typeahead-suggestion","text":"Typeahead suggestions enable users to search for known and frequently searched terms. As the user types into the search box, it tries to predict the query based on the characters the user has entered and gives a list of suggestions to complete the query. Typeahead suggestions help the user to articulate their search queries better. It\u2019s not about speeding up the search process but rather about guiding the users and lending them a helping hand in constructing their search query.","title":"1. What is Typeahead Suggestion?"},{"location":"DesigningTypeaheadSuggestion/#2-requirements-and-goals-of-the-system","text":"Functional Requirements: As the user types in their query, our service should suggest top 10 terms starting with whatever the user has typed. Non-function Requirements: The suggestions should appear in real-time. The user should be able to see the suggestions within 200ms.","title":"2. Requirements and Goals of the System"},{"location":"DesigningTypeaheadSuggestion/#3-basic-system-design-and-algorithm","text":"The problem we are solving is that we have a lot of \u2018strings\u2019 that we need to store in such a way that users can search with any prefix. Our service will suggest next terms that will match the given prefix. For example, if our database contains the following terms: cap, cat, captain, or capital and the user has typed in \u2018cap\u2019, our system should suggest \u2018cap\u2019, \u2018captain\u2019 and \u2018capital\u2019. Since we\u2019ve got to serve a lot of queries with minimum latency, we need to come up with a scheme that can efficiently store our data such that it can be queried quickly. We can\u2019t depend upon some database for this; we need to store our index in memory in a highly efficient data structure. One of the most appropriate data structures that can serve our purpose is the Trie (pronounced \u201ctry\u201d). A trie is a tree-like data structure used to store phrases where each node stores a character of the phrase in a sequential manner. For example, if we need to store \u2018cap, cat, caption, captain, capital\u2019 in the trie, it would look like: Now if the user has typed \u2018cap\u2019, our service can traverse the trie to go to the node \u2018P\u2019 to find all the terms that start with this prefix (e.g., cap-tion, cap-ital etc). We can merge nodes that have only one branch to save storage space. The above trie can be stored like this: Should we have case insensitive trie? For simplicity and search use-case, let\u2019s assume our data is case insensitive. How to find top suggestion? Now that we can find all the terms for a given prefix, how can we find the top 10 terms for the given prefix? One simple solution could be to store the count of searches that terminated at each node, e.g., if users have searched about \u2018CAPTAIN\u2019 100 times and \u2018CAPTION\u2019 500 times, we can store this number with the last character of the phrase. Now if the user types \u2018CAP\u2019 we know the top most searched word under the prefix \u2018CAP\u2019 is \u2018CAPTION\u2019. So, to find the top suggestions for a given prefix, we can traverse the sub-tree under it. Given a prefix, how much time will it take to traverse its sub-tree? Given the amount of data we need to index, we should expect a huge tree. Even traversing a sub-tree would take really long, e.g., the phrase \u2018system design interview questions\u2019 is 30 levels deep. Since we have very strict latency requirements we do need to improve the efficiency of our solution. Can we store top suggestions with each node? This can surely speed up our searches but will require a lot of extra storage. We can store top 10 suggestions at each node that we can return to the user. We have to bear the big increase in our storage capacity to achieve the required efficiency. We can optimize our storage by storing only references of the terminal nodes rather than storing the entire phrase. To find the suggested terms we need to traverse back using the parent reference from the terminal node. We will also need to store the frequency with each reference to keep track of top suggestions. How would we build this trie? We can efficiently build our trie bottom up. Each parent node will recursively call all the child nodes to calculate their top suggestions and their counts. Parent nodes will combine top suggestions from all of their children to determine their top suggestions. How to update the trie? Assuming five billion searches every day, which would give us approximately 60K queries per second. If we try to update our trie for every query it\u2019ll be extremely resource intensive and this can hamper our read requests, too. One solution to handle this could be to update our trie offline after a certain interval. As the new queries come in we can log them and also track their frequencies. Either we can log every query or do sampling and log every 1000th query. For example, if we don\u2019t want to show a term which is searched for less than 1000 times, it\u2019s safe to log every 1000th searched term. We can have a Map-Reduce (MR) set-up to process all the logging data periodically say every hour. These MR jobs will calculate frequencies of all searched terms in the past hour. We can then update our trie with this new data. We can take the current snapshot of the trie and update it with all the new terms and their frequencies. We should do this offline as we don\u2019t want our read queries to be blocked by update trie requests. We can have two options: We can make a copy of the trie on each server to update it offline. Once done we can switch to start using it and discard the old one. Another option is we can have a master-slave configuration for each trie server. We can update slave while the master is serving traffic. Once the update is complete, we can make the slave our new master. We can later update our old master, which can then start serving traffic, too. How can we update the frequencies of typeahead suggestions? Since we are storing frequencies of our typeahead suggestions with each node, we need to update them too! We can update only differences in frequencies rather than recounting all search terms from scratch. If we\u2019re keeping count of all the terms searched in last 10 days, we\u2019ll need to subtract the counts from the time period no longer included and add the counts for the new time period being included. We can add and subtract frequencies based on Exponential Moving Average (EMA) of each term. In EMA, we give more weight to the latest data. It\u2019s also known as the exponentially weighted moving average. After inserting a new term in the trie, we\u2019ll go to the terminal node of the phrase and increase its frequency. Since we\u2019re storing the top 10 queries in each node, it is possible that this particular search term jumped into the top 10 queries of a few other nodes. So, we need to update the top 10 queries of those nodes then. We have to traverse back from the node to all the way up to the root. For every parent, we check if the current query is part of the top 10. If so, we update the corresponding frequency. If not, we check if the current query\u2019s frequency is high enough to be a part of the top 10. If so, we insert this new term and remove the term with the lowest frequency. How can we remove a term from the trie? Let\u2019s say we have to remove a term from the trie because of some legal issue or hate or piracy etc. We can completely remove such terms from the trie when the regular update happens, meanwhile, we can add a filtering layer on each server which will remove any such term before sending them to users. What could be different ranking criteria for suggestions? In addition to a simple count, for terms ranking, we have to consider other factors too, e.g., freshness, user location, language, demographics, personal history etc.","title":"3. Basic System Design and Algorithm"},{"location":"DesigningTypeaheadSuggestion/#4-permanent-storage-of-the-trie","text":"How to store trie in a file so that we can rebuild our trie easily - this will be needed when a machine restarts? We can take a snapshot of our trie periodically and store it in a file. This will enable us to rebuild a trie if the server goes down. To store, we can start with the root node and save the trie level-by-level. With each node, we can store what character it contains and how many children it has. Right after each node, we should put all of its children. Let\u2019s assume we have the following trie: If we store this trie in a file with the above-mentioned scheme, we will have: \u201cC2,A2,R1,T,P,O1,D\u201d. From this, we can easily rebuild our trie. If you\u2019ve noticed, we are not storing top suggestions and their counts with each node. It is hard to store this information; as our trie is being stored top down, we don\u2019t have child nodes created before the parent, so there is no easy way to store their references. For this, we have to recalculate all the top terms with counts. This can be done while we are building the trie. Each node will calculate its top suggestions and pass it to its parent. Each parent node will merge results from all of its children to figure out its top suggestions.","title":"4. Permanent Storage of the Trie"},{"location":"DesigningTypeaheadSuggestion/#5-scale-estimation","text":"If we are building a service that has the same scale as that of Google we can expect 5 billion searches every day, which would give us approximately 60K queries per second. Since there will be a lot of duplicates in 5 billion queries, we can assume that only 20% of these will be unique. If we only want to index the top 50% of the search terms, we can get rid of a lot of less frequently searched queries. Let\u2019s assume we will have 100 million unique terms for which we want to build an index. Storage Estimation: If on the average each query consists of 3 words and if the average length of a word is 5 characters, this will give us 15 characters of average query size. Assuming we need 2 bytes to store a character, we will need 30 bytes to store an average query. So total storage we will need: 100 million * 30 bytes => 3 GB We can expect some growth in this data every day, but we should also be removing some terms that are not searched anymore. If we assume we have 2% new queries every day and if we are maintaining our index for the last one year, total storage we should expect: 3GB + (0.02 * 3 GB * 365 days) => 25 GB","title":"5. Scale Estimation"},{"location":"DesigningTypeaheadSuggestion/#6-data-partition","text":"Although our index can easily fit on one server, we can still partition it in order to meet our requirements of higher efficiency and lower latencies. How can we efficiently partition our data to distribute it onto multiple servers? a. Range Based Partitioning: What if we store our phrases in separate partitions based on their first letter. So we save all the terms starting with the letter \u2018A\u2019 in one partition and those that start with the letter \u2018B\u2019 into another partition and so on. We can even combine certain less frequently occurring letters into one partition. We should come up with this partitioning scheme statically so that we can always store and search terms in a predictable manner. The main problem with this approach is that it can lead to unbalanced servers, for instance, if we decide to put all terms starting with the letter \u2018E\u2019 into one partition, but later we realize that we have too many terms that start with letter \u2018E\u2019 that we can\u2019t fit into one partition. We can see that the above problem will happen with every statically defined scheme. It is not possible to calculate if each of our partitions will fit on one server statically. b. Partition based on the maximum capacity of the server: Let\u2019s say we partition our trie based on the maximum memory capacity of the servers. We can keep storing data on a server as long as it has memory available. Whenever a sub-tree cannot fit into a server, we break our partition there to assign that range to this server and move on the next server to repeat this process. Let\u2019s say if our first trie server can store all terms from \u2018A\u2019 to \u2018AABC\u2019, which mean our next server will store from \u2018AABD\u2019 onwards. If our second server could store up to \u2018BXA\u2019, the next server will start from \u2018BXB\u2019, and so on. We can keep a hash table to quickly access this partitioning scheme: Server 1, A-AABC Server 2, AABD-BXA Server 3, BXB-CDA For querying, if the user has typed \u2018A\u2019 we have to query both server 1 and 2 to find the top suggestions. When the user has typed \u2018AA\u2019, we still have to query server 1 and 2, but when the user has typed \u2018AAA\u2019 we only need to query server 1. We can have a load balancer in front of our trie servers which can store this mapping and redirect traffic. Also, if we are querying from multiple servers, either we need to merge the results on the server side to calculate the overall top results or make our clients do that. If we prefer to do this on the server side, we need to introduce another layer of servers between load balancers and trie severs (let\u2019s call them aggregator). These servers will aggregate results from multiple trie servers and return the top results to the client. Partitioning based on the maximum capacity can still lead us to hotspots, e.g., if there are a lot of queries for terms starting with \u2018cap\u2019, the server holding it will have a high load compared to others. c. Partition based on the hash of the term: Each term will be passed to a hash function, which will generate a server number and we will store the term on that server. This will make our term distribution random and hence minimize hotspots. The disadvantage of this scheme is, to find typeahead suggestions for a term we have to ask all the servers and then aggregate the results.","title":"6. Data Partition"},{"location":"DesigningTypeaheadSuggestion/#7-cache","text":"We should realize that caching the top searched terms will be extremely helpful in our service. There will be a small percentage of queries that will be responsible for most of the traffic. We can have separate cache servers in front of the trie servers holding most frequently searched terms and their typeahead suggestions. Application servers should check these cache servers before hitting the trie servers to see if they have the desired searched terms. This will save us time to traverse the tri. We can also build a simple Machine Learning (ML) model that can try to predict the engagement on each suggestion based on simple counting, personalization, or trending data, and cache these terms beforehand.","title":"7. Cache"},{"location":"DesigningTypeaheadSuggestion/#8-replication-and-load-balancer","text":"We should have replicas for our trie servers both for load balancing and also for fault tolerance. We also need a load balancer that keeps track of our data partitioning scheme and redirects traffic based on the prefixes.","title":"8. Replication and Load Balancer"},{"location":"DesigningTypeaheadSuggestion/#9-fault-tolerance","text":"What will happen when a trie server goes down? As discussed above we can have a master-slave configuration; if the master dies, the slave can take over after failover. Any server that comes back up, can rebuild the trie based on the last snapshot.","title":"9. Fault Tolerance"},{"location":"DesigningTypeaheadSuggestion/#10-typeahead-client","text":"We can perform the following optimizations on the client side to improve user\u2019s experience: The client should only try hitting the server if the user has not pressed any key for 50ms. If the user is constantly typing, the client can cancel the in-progress requests. Initially, the client can wait until the user enters a couple of characters. Clients can pre-fetch some data from the server to save future requests. Clients can store the recent history of suggestions locally. Recent history has a very high rate of being reused. Establishing an early connection with the server turns out to be one of the most important factors. As soon as the user opens the search engine website, the client can open a connection with the server. So when a user types in the first character, the client doesn\u2019t waste time in establishing the connection. The server can push some part of their cache to CDNs and Internet Service Providers (ISPs) for efficiency.","title":"10. Typeahead Client"},{"location":"DesigningTypeaheadSuggestion/#11-personalization","text":"Users will receive some typeahead suggestions based on their historical searches, location, language, etc. We can store the personal history of each user separately on the server and also cache them on the client. The server can add these personalized terms in the final set before sending it to the user. Personalized searches should always come before others.","title":"11. Personalization"},{"location":"DesigningURLShorteningService/","text":"Creating a TinyURL-style URL shortening service Let's make a TinyURL-style URL shortening service. Short aliases for long URLs will be provided by this service. Bit.ly, goo.gl, qlink.me, and other similar services Level of Difficulty: Easy What is the purpose of URL shortening? For long URLs, URL shortening is utilized to create shorter aliases. These shortened aliases are referred to as \"short links.\" When users click on these short links, they are forwarded to the original URL. When displayed, printed, messaged, or tweeted, short links save a lot of space. Shorter URLs are also less likely to be mistyped by users. For example, if we use TinyURL to shorten this page: https://www.jayaaemekar.io/collection/page/5668639101419520/ We would get: http://tinyurl.com/jlg8zpc The abbreviated URL is almost one-third the length of the original. The abbreviated URL is almost one-third the length of the original. URL shortening is used for a variety of purposes, including optimizing links across devices, tracking individual links to gauge audience and campaign performance, and concealing connected original URLs. Pratice Problem Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen Solution Requirements and Goals of the System \ud83d\udca1 At the start of the interview, you should always outline criteria. Ask questions to figure out the exact extent of the system that the interviewer is thinking of. The following requirements should be met by our URL shortening system: Functional Requirements: Given a URL, our service should create a unique and shorter alias for it. This is referred to as a short link. This URL should be short enough to be copied and pasted into programs without difficulty. Our service should redirect visitors to the original site when they click on a short link. Users should be allowed to choose a custom short link for their URL as an option. Links will expire after a set amount of time. The expiration time should be configurable by the user. Non-Functional Requirements: The system should have a high level of availability. This is necessary because if our service is unavailable, all URL redirections would fail. URL redirection should take place in real time with the least amount of latency possible. It should not be possible to guess the length of shortened connections (not predictable). Extended Requirements: Analytical data; for example, how many times has a redirection occurred? Our service should also be accessible through REST APIs by other services. Capacity Estimation and Constraints Our system will rely heavily on reading. In comparison to new URL shortenings, there will be a lot of redirection requests. Assume that read and write have a 100:1 ratio. Traffic estimates: If we assume 500 million new URL shortenings every month and a 100:1 read/write ratio, we can expect 50 billion redirections in the same time period: 100 * 500M => 50B What would our system's Queries Per Second (QPS) be? Per second, new URL shortenings: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let's say we keep track of every URL shortening request (and the abbreviated link that goes with it) for five years. With 500 million new URLs expected per month, the total number of objects we expect to store is 30 billion: 500 million * 5 years * 12 months = 30 billion Let's assume that each stored object is 500 bytes in size (this is just a guess\u2013we'll look at it later). We'll need a total of 15TB of storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: How much RAM will we need to keep some of the most often visited URLs if we wish to cache them? We'd like to cache this 20% of hot URLs if we follow the 80-20 rule, which states that 20% of URLs produce 80% of traffic. We'll get 1.7 billion requests each day if we have 20K requests per second: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to keep in mind is that because there will be a lot of duplicate requests (for the same URL), our actual memory consumption will be less than 170GB. Estimates at a high level: The following is a summary of our high-level estimations for our service, assuming 500 million new URLs each month and a 100:1 read:write ratio: System APIs \ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The following are possible API specifications for creating and removing URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? By consuming all URL keys in the existing design, a hostile person can put us out of business. We can restrict users based on their api_dev_key to prevent abuse. A specific amount of URL creations and redirections each time period can be configured for each api_dev_key (which may be set to a different duration per developer key). Database Design \ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. A few points to consider regarding the data we'll be storing: We'll need billions of records to store. Each item we keep is little (less than 1K). Except for storing which user created a URL, there are no linkages between records. Our service requires a lot of reading. Database Schema: We'd need two tables: one to store information about URL mappings, and another to store data about the user who created the short link. What kind of database should we use? A NoSQL store like DynamoDB, Cassandra, or Riak is a preferable choice because we expect to store billions of rows and don't need to employ associations between items. It would also be easy to scale a NoSQL database. Algorithms and Basic System Design We're trying to figure out how to make a short and unique key for a given URL. The abbreviated URL in the TinyURL example in Section 1 is \"http://tinyurl.com/jlg8zpc.\" The short key we want to produce is the final seven characters of this URL. Here, we'll look at two options: Encoding actual URL We can generate a unique hash of the supplied URL (e.g., MD5 or SHA256, etc.). After that, the hash can be decoded for display. This encoding might be base36 ([a-z,0-9]) or base62 ([A-Z, a-z, 0-9]), and we can use Base64 encoding by adding '+' and '/'. What should the length of the short key be, is a legitimate question. Is it better to have six, eight, or ten characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. The MD5 algorithm produces a 128-bit hash value when used as a hash function. We'll get a string with more than 21 characters after base64 encoding (since each base64 character encodes 6 bits of the hash value). How will we choose our key now that we only have space for 8 characters per short key? For the key, we can use the first six (or eight) letters. This could lead to key duplication; to avoid this, we can exchange certain characters or choose other characters from the encoding string. What are the different issues with our solution? The following are a couple of issues with our encoding scheme: If numerous users enter the same URL, the abbreviated URL will be the same, which is unacceptable. What if parts of the URL are URL-encoded? e.g., http://www.jayaaemekar.io/distributed.php?id=design, and http://www.jayaaemekar.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Alternative to the problems: To make each input URL unique, we can append an ascending sequence number to it and then construct a hash of it. However, we do not need to save this sequence number in the databases. An ever-increasing sequence number could be a concern with this method. Is it possible for it to overflow? Increasing the sequence number will have an effect on the service's performance. Another option is to include a user id to the input URL (which should be unique). If the user hasn't signed in yet, we'll have to prompt them to select a uniqueness key. If there is still a disagreement, we must keep creating keys until we find one that is unique. Generating keys offline We could create a separate Key Generation Service (KGS) that produces random six-letter strings and saves them in a database (let's call it key-DB). We'll just utilize one of the already-generated keys to abbreviate a URL whenever we need to. This method simplifies and expedites the process. We won't have to worry about duplications or collisions because the URL won't be encoded. KGS will ensure that all keys placed into key-DB are one-of-a-kind. Can concurrency lead to issues? Once a key has been used, it should be marked in the database to prevent it from being reused. If many servers are reading keys at the same time, we may see a situation where two or more servers attempt to read the same key from the database. What are our options for dealing with this concurrent issue? KGS allows servers to read and mark database keys. To store keys, KGS can employ two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys into the used keys table as soon as they are given to one of the servers. KGS can maintain some keys in memory at all times so that they can be rapidly provided to a server when it is needed. For simplicity, KGS can move keys to the used keys table as soon as they are loaded into memory. This guarantees that each server has its own set of keys. We will be squandering those keys if KGS dies before allocating all of the loaded keys to some server\u2013which may be acceptable given the large amount of keys we have. KGS must also ensure that the same key is not used by several servers. Before removing keys from the data structure and delivering them to a server, it must synchronize (or gain a lock on) the data structure holding the keys. How big should the key-DB be? We can construct 68.7 billion unique six-letter keys using base64 encoding. If each alpha-numeric character requires one byte, we can store all of these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Doesn't KGS represent a single point of failure? Yes, it is correct. We can solve this by having a backup copy of KGS. When the primary server fails, the standby server can generate and distribute keys in its place. Is it possible for each app server to cache some keys from the key-DB? Yes, this will undoubtedly expedite things. However, if the application server dies before all of the keys have been consumed, we will lose those keys. Because we have 68B distinct six-letter keys, this may be okay. How would we go about doing a key lookup? To acquire the whole URL, we may look up the key in our database. If it's in the database, send a \"HTTP 302 Redirect\" status to the browser, including the stored URL in the \"Location\" field. If the key isn't in our system, return the user to the homepage or deliver a \"HTTP 404 Not Found\" status. Should custom aliases be limited in size? Custom aliases are supported by our service. Users can choose any 'key' they like, but a custom alias is not required. However, imposing a size restriction on a custom alias is understandable (and frequently desirable) in order to maintain a consistent URL database. Assume that each customer key can have a maximum of 16 characters (as reflected in the above database schema). Data Partitioning and Replication We need to split our database such that it can hold information about billions of URLs in order to scale it out. We need to devise a partitioning strategy that will divide and store our data across multiple DB servers. Range Based Partitioning: Based on the initial letter of the hash key, we can store URLs in different partitions. As a result, we save all URLs that begin with the letter 'A' (and 'a') in one partition, those that begin with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. We can even merge a few characters that aren't used very often into a single database segment. We should devise a static partitioning scheme to ensure that we can always store and retrieve URLs in a consistent manner. The biggest issue with this method is that it can result in unbalanced database servers. For example, suppose we decide to put all URLs beginning with the letter 'E' into a database partition, only to discover later that we have far too many URLs beginning with the letter 'E.' Partitioning based on hashes: We take a hash of the object we're storing in this scheme. The hash is then used to determine which partition to use. In our situation, the hash of the 'key' or the short link can be used to determine the partition in which the data object is stored. Our hashing function will distribute URLs into different divisions at random (e.g., any 'key' can be mapped to a number between [1...256]), and this number will represent the partition in which we will put our object. This method can still result in overloaded partitions, which can be remedied by employing Consistent Hashing. Cache URLs that are often visited can be cached. We can utilize a commercially available solution like Memcached, which can store complete URLs along with their hashes. Before contacting backend storage, application servers can rapidly check if the needed URL is in the cache. Should we have a lot of cache memory? We can start with 20% of daily traffic and change the number of cache servers needed based on client usage patterns. To cache 20% of daily traffic, we'll require 170GB of memory, as previously calculated. We can easily fit all of the cache into one machine because a modern-day server can have 256GB of memory. Alternatively, we can store all of these popular URLs on a couple of smaller servers. Which cache eviction policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to change a link with a newer/hotter URL? For our system, LRU (Least Recently Used) can be a suitable policy. We start with the URL that has been used the least lately. To store our URLs and Hashes, we can use a Linked Hash Map or a similar data structure, which will also keep track of the URLs that have been accessed recently. We may replicate our cache servers to divide the load between them to boost efficiency even more. How do I refresh each cache replica? Our servers would hit a backend database whenever a cache miss occurred. We may update the cache and pass the new entry to all cache replicas once this happens. By adding the new entry, each copy can update its cache. If the entry already exists in a replica, it can be ignored. Load Balancer (LB) We may add a load balancing layer to our system in three places: Between the application servers and the clients The Relationship Between Application and Database Servers The Relationship Between Application and Cache Servers We might start with a simple Round Robin strategy, which evenly distributes incoming requests among backend servers. This LB is easy to set up and doesn't add any more overhead. Another advantage of this method is that if a server goes down, LB removes it from the rotation and stops transmitting traffic to it. We don't take the server load into account with Round Robin LB, which is a concern. The LB will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent LB solution can be implemented, which queries the backend server about its load on a regular basis and adjusts traffic accordingly. Database cleansing or purging Should entries be saved indefinitely or should they be deleted? What should happen to the link if it reaches the user-specified expiration time? It would put a lot of strain on our database if we decided to actively look for outdated links and remove them. Instead, we can execute a lazy cleanup and gently remove expired links. Only expired links will be erased by our service, while some expired links may exist longer but will never be returned to users. If a user attempts to access an expired link, we can erase the link and provide the user an error message. A separate Cleanup service can run on a regular basis to clear out expired links from our cache and storage. This service should be extremely light, and it should only be used when user traffic is predicted to be low. Each link can have a default expiration time (e.g.,two years). After removing an expired link, we may re-use the key by putting it back in the key-DB. Should links that haven't been seen in a certain amount of time, say six months, be removed? This could be challenging. Because storage is becoming more affordable, we can chose to store links indefinitely. Telemetry What were the user locations, how many times a short URL was used, etc.? How would we keep track of these figures? What happens when a popular URL is bombarded with a huge number of concurrent requests if it's part of a DB row that gets updated on each view? The visitor's nation, date and time of access, web page that relates to the click, browser, or platform from which the page was visited are all statistics worth keeping track of. Security and Permissions Can users build private URLs or restrict access to a URL to a specific group of users? In the database, we can store the permission level (public/private) for each URL. We can also construct a separate table to keep track of UserIDs with access to a given URL. We can return an error (HTTP 401) if a user does not have permission and attempts to access a URL. Given that we're using a NoSQL wide-column database like Cassandra to store our data, the 'Hash' (or the KGS produced 'key') would be the key for the table containing permissions. The UserIDs of those users who have authorization to see the URL will be stored in the columns.","title":"Designing a URL Shortening service"},{"location":"DesigningURLShorteningService/#creating-a-tinyurl-style-url-shortening-service","text":"Let's make a TinyURL-style URL shortening service. Short aliases for long URLs will be provided by this service. Bit.ly, goo.gl, qlink.me, and other similar services Level of Difficulty: Easy What is the purpose of URL shortening? For long URLs, URL shortening is utilized to create shorter aliases. These shortened aliases are referred to as \"short links.\" When users click on these short links, they are forwarded to the original URL. When displayed, printed, messaged, or tweeted, short links save a lot of space. Shorter URLs are also less likely to be mistyped by users. For example, if we use TinyURL to shorten this page: https://www.jayaaemekar.io/collection/page/5668639101419520/ We would get: http://tinyurl.com/jlg8zpc The abbreviated URL is almost one-third the length of the original. The abbreviated URL is almost one-third the length of the original. URL shortening is used for a variety of purposes, including optimizing links across devices, tracking individual links to gauge audience and campaign performance, and concealing connected original URLs.","title":"Creating a TinyURL-style URL shortening service"},{"location":"DesigningURLShorteningService/#pratice-problem","text":"Let's get started on the system design solution. If you run into any problems, please see the solution below. Pratice on full Screen","title":"Pratice Problem"},{"location":"DesigningURLShorteningService/#solution","text":"","title":"Solution"},{"location":"DesigningURLShorteningService/#requirements-and-goals-of-the-system","text":"\ud83d\udca1 At the start of the interview, you should always outline criteria. Ask questions to figure out the exact extent of the system that the interviewer is thinking of. The following requirements should be met by our URL shortening system: Functional Requirements: Given a URL, our service should create a unique and shorter alias for it. This is referred to as a short link. This URL should be short enough to be copied and pasted into programs without difficulty. Our service should redirect visitors to the original site when they click on a short link. Users should be allowed to choose a custom short link for their URL as an option. Links will expire after a set amount of time. The expiration time should be configurable by the user. Non-Functional Requirements: The system should have a high level of availability. This is necessary because if our service is unavailable, all URL redirections would fail. URL redirection should take place in real time with the least amount of latency possible. It should not be possible to guess the length of shortened connections (not predictable). Extended Requirements: Analytical data; for example, how many times has a redirection occurred? Our service should also be accessible through REST APIs by other services.","title":"Requirements and Goals of the System"},{"location":"DesigningURLShorteningService/#capacity-estimation-and-constraints","text":"Our system will rely heavily on reading. In comparison to new URL shortenings, there will be a lot of redirection requests. Assume that read and write have a 100:1 ratio. Traffic estimates: If we assume 500 million new URL shortenings every month and a 100:1 read/write ratio, we can expect 50 billion redirections in the same time period: 100 * 500M => 50B What would our system's Queries Per Second (QPS) be? Per second, new URL shortenings: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let's say we keep track of every URL shortening request (and the abbreviated link that goes with it) for five years. With 500 million new URLs expected per month, the total number of objects we expect to store is 30 billion: 500 million * 5 years * 12 months = 30 billion Let's assume that each stored object is 500 bytes in size (this is just a guess\u2013we'll look at it later). We'll need a total of 15TB of storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: How much RAM will we need to keep some of the most often visited URLs if we wish to cache them? We'd like to cache this 20% of hot URLs if we follow the 80-20 rule, which states that 20% of URLs produce 80% of traffic. We'll get 1.7 billion requests each day if we have 20K requests per second: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to keep in mind is that because there will be a lot of duplicate requests (for the same URL), our actual memory consumption will be less than 170GB. Estimates at a high level: The following is a summary of our high-level estimations for our service, assuming 500 million new URLs each month and a 100:1 read:write ratio:","title":"Capacity Estimation and Constraints"},{"location":"DesigningURLShorteningService/#system-apis","text":"\ud83d\udca1 It's always a good idea to establish the system APIs after we've finalized the requirements. This should express clearly what the system is intended to do. To expose the functionality of our service, we can use SOAP or REST APIs. The following are possible API specifications for creating and removing URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? By consuming all URL keys in the existing design, a hostile person can put us out of business. We can restrict users based on their api_dev_key to prevent abuse. A specific amount of URL creations and redirections each time period can be configured for each api_dev_key (which may be set to a different duration per developer key).","title":"System APIs"},{"location":"DesigningURLShorteningService/#database-design","text":"\ud83d\udca1 Defining the database schema early in the interview will aid in understanding the data flow between various components and will eventually lead to data segmentation. A few points to consider regarding the data we'll be storing: We'll need billions of records to store. Each item we keep is little (less than 1K). Except for storing which user created a URL, there are no linkages between records. Our service requires a lot of reading. Database Schema: We'd need two tables: one to store information about URL mappings, and another to store data about the user who created the short link. What kind of database should we use? A NoSQL store like DynamoDB, Cassandra, or Riak is a preferable choice because we expect to store billions of rows and don't need to employ associations between items. It would also be easy to scale a NoSQL database.","title":"Database Design"},{"location":"DesigningURLShorteningService/#algorithms-and-basic-system-design","text":"We're trying to figure out how to make a short and unique key for a given URL. The abbreviated URL in the TinyURL example in Section 1 is \"http://tinyurl.com/jlg8zpc.\" The short key we want to produce is the final seven characters of this URL. Here, we'll look at two options:","title":"Algorithms and Basic System Design"},{"location":"DesigningURLShorteningService/#encoding-actual-url","text":"We can generate a unique hash of the supplied URL (e.g., MD5 or SHA256, etc.). After that, the hash can be decoded for display. This encoding might be base36 ([a-z,0-9]) or base62 ([A-Z, a-z, 0-9]), and we can use Base64 encoding by adding '+' and '/'. What should the length of the short key be, is a legitimate question. Is it better to have six, eight, or ten characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. The MD5 algorithm produces a 128-bit hash value when used as a hash function. We'll get a string with more than 21 characters after base64 encoding (since each base64 character encodes 6 bits of the hash value). How will we choose our key now that we only have space for 8 characters per short key? For the key, we can use the first six (or eight) letters. This could lead to key duplication; to avoid this, we can exchange certain characters or choose other characters from the encoding string. What are the different issues with our solution? The following are a couple of issues with our encoding scheme: If numerous users enter the same URL, the abbreviated URL will be the same, which is unacceptable. What if parts of the URL are URL-encoded? e.g., http://www.jayaaemekar.io/distributed.php?id=design, and http://www.jayaaemekar.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Alternative to the problems: To make each input URL unique, we can append an ascending sequence number to it and then construct a hash of it. However, we do not need to save this sequence number in the databases. An ever-increasing sequence number could be a concern with this method. Is it possible for it to overflow? Increasing the sequence number will have an effect on the service's performance. Another option is to include a user id to the input URL (which should be unique). If the user hasn't signed in yet, we'll have to prompt them to select a uniqueness key. If there is still a disagreement, we must keep creating keys until we find one that is unique.","title":"Encoding actual URL"},{"location":"DesigningURLShorteningService/#generating-keys-offline","text":"We could create a separate Key Generation Service (KGS) that produces random six-letter strings and saves them in a database (let's call it key-DB). We'll just utilize one of the already-generated keys to abbreviate a URL whenever we need to. This method simplifies and expedites the process. We won't have to worry about duplications or collisions because the URL won't be encoded. KGS will ensure that all keys placed into key-DB are one-of-a-kind. Can concurrency lead to issues? Once a key has been used, it should be marked in the database to prevent it from being reused. If many servers are reading keys at the same time, we may see a situation where two or more servers attempt to read the same key from the database. What are our options for dealing with this concurrent issue? KGS allows servers to read and mark database keys. To store keys, KGS can employ two tables: one for keys that haven't been used yet, and another for all keys that have been used. KGS can move keys into the used keys table as soon as they are given to one of the servers. KGS can maintain some keys in memory at all times so that they can be rapidly provided to a server when it is needed. For simplicity, KGS can move keys to the used keys table as soon as they are loaded into memory. This guarantees that each server has its own set of keys. We will be squandering those keys if KGS dies before allocating all of the loaded keys to some server\u2013which may be acceptable given the large amount of keys we have. KGS must also ensure that the same key is not used by several servers. Before removing keys from the data structure and delivering them to a server, it must synchronize (or gain a lock on) the data structure holding the keys. How big should the key-DB be? We can construct 68.7 billion unique six-letter keys using base64 encoding. If each alpha-numeric character requires one byte, we can store all of these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Doesn't KGS represent a single point of failure? Yes, it is correct. We can solve this by having a backup copy of KGS. When the primary server fails, the standby server can generate and distribute keys in its place. Is it possible for each app server to cache some keys from the key-DB? Yes, this will undoubtedly expedite things. However, if the application server dies before all of the keys have been consumed, we will lose those keys. Because we have 68B distinct six-letter keys, this may be okay. How would we go about doing a key lookup? To acquire the whole URL, we may look up the key in our database. If it's in the database, send a \"HTTP 302 Redirect\" status to the browser, including the stored URL in the \"Location\" field. If the key isn't in our system, return the user to the homepage or deliver a \"HTTP 404 Not Found\" status. Should custom aliases be limited in size? Custom aliases are supported by our service. Users can choose any 'key' they like, but a custom alias is not required. However, imposing a size restriction on a custom alias is understandable (and frequently desirable) in order to maintain a consistent URL database. Assume that each customer key can have a maximum of 16 characters (as reflected in the above database schema).","title":"Generating keys offline"},{"location":"DesigningURLShorteningService/#data-partitioning-and-replication","text":"We need to split our database such that it can hold information about billions of URLs in order to scale it out. We need to devise a partitioning strategy that will divide and store our data across multiple DB servers. Range Based Partitioning: Based on the initial letter of the hash key, we can store URLs in different partitions. As a result, we save all URLs that begin with the letter 'A' (and 'a') in one partition, those that begin with the letter 'B' in another, and so on. Range-based partitioning is the name for this method. We can even merge a few characters that aren't used very often into a single database segment. We should devise a static partitioning scheme to ensure that we can always store and retrieve URLs in a consistent manner. The biggest issue with this method is that it can result in unbalanced database servers. For example, suppose we decide to put all URLs beginning with the letter 'E' into a database partition, only to discover later that we have far too many URLs beginning with the letter 'E.' Partitioning based on hashes: We take a hash of the object we're storing in this scheme. The hash is then used to determine which partition to use. In our situation, the hash of the 'key' or the short link can be used to determine the partition in which the data object is stored. Our hashing function will distribute URLs into different divisions at random (e.g., any 'key' can be mapped to a number between [1...256]), and this number will represent the partition in which we will put our object. This method can still result in overloaded partitions, which can be remedied by employing Consistent Hashing.","title":"Data Partitioning and Replication"},{"location":"DesigningURLShorteningService/#cache","text":"URLs that are often visited can be cached. We can utilize a commercially available solution like Memcached, which can store complete URLs along with their hashes. Before contacting backend storage, application servers can rapidly check if the needed URL is in the cache. Should we have a lot of cache memory? We can start with 20% of daily traffic and change the number of cache servers needed based on client usage patterns. To cache 20% of daily traffic, we'll require 170GB of memory, as previously calculated. We can easily fit all of the cache into one machine because a modern-day server can have 256GB of memory. Alternatively, we can store all of these popular URLs on a couple of smaller servers. Which cache eviction policy would be most appropriate for our requirements? What would we do if the cache was full and we needed to change a link with a newer/hotter URL? For our system, LRU (Least Recently Used) can be a suitable policy. We start with the URL that has been used the least lately. To store our URLs and Hashes, we can use a Linked Hash Map or a similar data structure, which will also keep track of the URLs that have been accessed recently. We may replicate our cache servers to divide the load between them to boost efficiency even more. How do I refresh each cache replica? Our servers would hit a backend database whenever a cache miss occurred. We may update the cache and pass the new entry to all cache replicas once this happens. By adding the new entry, each copy can update its cache. If the entry already exists in a replica, it can be ignored.","title":"Cache"},{"location":"DesigningURLShorteningService/#load-balancer-lb","text":"We may add a load balancing layer to our system in three places: Between the application servers and the clients The Relationship Between Application and Database Servers The Relationship Between Application and Cache Servers We might start with a simple Round Robin strategy, which evenly distributes incoming requests among backend servers. This LB is easy to set up and doesn't add any more overhead. Another advantage of this method is that if a server goes down, LB removes it from the rotation and stops transmitting traffic to it. We don't take the server load into account with Round Robin LB, which is a concern. The LB will not cease delivering new requests to a server that is overloaded or slow. To deal with this, a more intelligent LB solution can be implemented, which queries the backend server about its load on a regular basis and adjusts traffic accordingly.","title":"Load Balancer (LB)"},{"location":"DesigningURLShorteningService/#database-cleansing-or-purging","text":"Should entries be saved indefinitely or should they be deleted? What should happen to the link if it reaches the user-specified expiration time? It would put a lot of strain on our database if we decided to actively look for outdated links and remove them. Instead, we can execute a lazy cleanup and gently remove expired links. Only expired links will be erased by our service, while some expired links may exist longer but will never be returned to users. If a user attempts to access an expired link, we can erase the link and provide the user an error message. A separate Cleanup service can run on a regular basis to clear out expired links from our cache and storage. This service should be extremely light, and it should only be used when user traffic is predicted to be low. Each link can have a default expiration time (e.g.,two years). After removing an expired link, we may re-use the key by putting it back in the key-DB. Should links that haven't been seen in a certain amount of time, say six months, be removed? This could be challenging. Because storage is becoming more affordable, we can chose to store links indefinitely.","title":"Database cleansing or purging"},{"location":"DesigningURLShorteningService/#telemetry","text":"What were the user locations, how many times a short URL was used, etc.? How would we keep track of these figures? What happens when a popular URL is bombarded with a huge number of concurrent requests if it's part of a DB row that gets updated on each view? The visitor's nation, date and time of access, web page that relates to the click, browser, or platform from which the page was visited are all statistics worth keeping track of.","title":"Telemetry"},{"location":"DesigningURLShorteningService/#security-and-permissions","text":"Can users build private URLs or restrict access to a URL to a specific group of users? In the database, we can store the permission level (public/private) for each URL. We can also construct a separate table to keep track of UserIDs with access to a given URL. We can return an error (HTTP 401) if a user does not have permission and attempts to access a URL. Given that we're using a NoSQL wide-column database like Cassandra to store our data, the 'Hash' (or the KGS produced 'key') would be the key for the table containing permissions. The UserIDs of those users who have authorization to see the URL will be stored in the columns.","title":"Security and Permissions"},{"location":"DesigningUberBackend/","text":"Designing Uber backend Let's design a ride-sharing service like Uber, which connects passengers who need a ride with drivers who have a car. Similar Services: Lyft, Didi, Via, Sidecar, etc. Difficulty level: Hard Prerequisite: Designing Yelp 1. What is Uber? Uber enables its customers to book drivers for taxi rides. Uber drivers use their personal cars to drive customers around. Both customers and drivers communicate with each other through their smartphones using the Uber app. 2. Requirements and Goals of the System Let\u2019s start with building a simpler version of Uber. There are two types of users in our system: 1) Drivers 2) Customers. Drivers need to regularly notify the service about their current location and their availability to pick passengers. Passengers get to see all the nearby available drivers. Customer can request a ride; nearby drivers are notified that a customer is ready to be picked up. Once a driver and a customer accept a ride, they can constantly see each other\u2019s current location until the trip finishes. Upon reaching the destination, the driver marks the journey complete to become available for the next ride. 3. Capacity Estimation and Constraints Let\u2019s assume we have 300M customers and 1M drivers with 1M daily active customers and 500K daily active drivers. Let\u2019s assume 1M daily rides. Let\u2019s assume that all active drivers notify their current location every three seconds. Once a customer puts in a request for a ride, the system should be able to contact drivers in real-time. 4. Basic System Design and Algorithm We will take the solution discussed in Designing Yelp and modify it to make it work for the above-mentioned \u201cUber\u201d use cases. The biggest difference we have is that our QuadTree was not built keeping in mind that there would be frequent updates to it. So, we have two issues with our Dynamic Grid solution: Since all active drivers are reporting their locations every three seconds, we need to update our data structures to reflect that. If we have to update the QuadTree for every change in the driver\u2019s position, it will take a lot of time and resources. To update a driver to its new location, we must find the right grid based on the driver\u2019s previous location. If the new position does not belong to the current grid, we have to remove the driver from the current grid and move/reinsert the user to the correct grid. After this move, if the new grid reaches the maximum limit of drivers, we have to repartition it. We need to have a quick mechanism to propagate the current location of all the nearby drivers to any active customer in that area. Also, when a ride is in progress, our system needs to notify both the driver and passenger about the current location of the car. Although our QuadTree helps us find nearby drivers quickly, a fast update in the tree is not guaranteed. Do we need to modify our QuadTree every time a driver reports their location? If we don\u2019t update our QuadTree with every update from the driver, it will have some old data and will not reflect the current location of drivers correctly. If you recall, our purpose of building the QuadTree was to find nearby drivers (or places) efficiently. Since all active drivers report their location every three seconds, therefore there will be a lot more updates happening to our tree than querying for nearby drivers. So, what if we keep the latest position reported by all drivers in a hash table and update our QuadTree a little less frequently? Let\u2019s assume we guarantee that a driver\u2019s current location will be reflected in the QuadTree within 15 seconds. Meanwhile, we will maintain a hash table that will store the current location reported by drivers; let\u2019s call this DriverLocationHT. How much memory we need for DriverLocationHT? We need to store DriveID, their present and old location, in the hash table. So, we need a total of 35 bytes to store one record: DriverID (3 bytes - 1 million drivers) Old latitude (8 bytes) Old longitude (8 bytes) New latitude (8 bytes) New longitude (8 bytes) Total = 35 bytes If we have 1 million total drivers, we need the following memory (ignoring hash table overhead): 1 million * 35 bytes => 35 MB How much bandwidth will our service consume to receive location updates from all drivers? If we get DriverID and their location, it will be (3+16 => 19 bytes). If we receive this information every three seconds from 500K daily active drivers, we will be getting 9.5MB per three seconds. Do we need to distribute DriverLocationHT onto multiple servers? Although our memory and bandwidth requirements don\u2019t require this, since all this information can easily be stored on one server, but, for scalability, performance, and fault tolerance, we should distribute DriverLocationHT onto multiple servers. We can distribute based on the DriverID to make the distribution completely random. Let\u2019s call the machines holding DriverLocationHT the Driver Location server. Other than storing the driver\u2019s location, each of these servers will do two things: As soon as the server receives an update for a driver\u2019s location, they will broadcast that information to all the interested customers. The server needs to notify the respective QuadTree server to refresh the driver\u2019s location. As discussed above, this can happen every 10 seconds. How can we efficiently broadcast the driver\u2019s location to customers? We can have a Push Model where the server will push the positions to all the relevant users. We can have a dedicated Notification Service that can broadcast the current location of drivers to all the interested customers. We can build our Notification service on a publisher/subscriber model. When a customer opens the Uber app on their cell phone, they query the server to find nearby drivers. On the server side, before returning the list of drivers to the customer, we will subscribe the customer for all the updates from those drivers. We can maintain a list of customers (subscribers) interested in knowing the location of a driver and, whenever we have an update in DriverLocationHT for that driver, we can broadcast the current location of the driver to all subscribed customers. This way, our system makes sure that we always show the driver\u2019s current position to the customer. How much memory will we need to store all these subscriptions? As we have estimated above, we will have 1M daily active customers and 500K daily active drivers. On average let\u2019s assume that five customers subscribe to one driver. Let\u2019s assume we store all this information in a hash table so that we can update it efficiently. We need to store driver and customer IDs to maintain the subscriptions. Assuming we will need 3 bytes for DriverID and 8 bytes for CustomerID, we will need 21MB of memory. (500K * 3) + (500K * 5 * 8 ) ~= 21 MB How much bandwidth will we need to broadcast the driver\u2019s location to customers? For every active driver, we have five subscribers, so the total subscribers we have: 5 * 500K => 2.5M To all these customers we need to send DriverID (3 bytes) and their location (16 bytes) every second, so, we need the following bandwidth: 2.5M * 19 bytes => 47.5 MB/s How can we efficiently implement Notification service? We can either use HTTP long polling or push notifications. How will the new publishers/drivers get added for a current customer? As we have proposed above, customers will be subscribed to nearby drivers when they open the Uber app for the first time, what will happen when a new driver enters the area the customer is looking at? To add a new customer/driver subscription dynamically, we need to keep track of the area the customer is watching. This will make our solution complicated; how about if instead of pushing this information, clients pull it from the server? How about if clients pull information about nearby drivers from the server? Clients can send their current location, and the server will find all the nearby drivers from the QuadTree to return them to the client. Upon receiving this information, the client can update their screen to reflect the current positions of the drivers. Clients can query every five seconds to limit the number of round trips to the server. This solution looks simpler compared to the push model described above. Do we need to repartition a grid as soon as it reaches the maximum limit? We can have a cushion to let each grid grow a little bigger beyond the limit before we decide to partition it. Let\u2019s say our grids can grow/shrink an extra 10% before we partition/merge them. This should decrease the load for a grid partition or merge on high traffic grids. How would \u201cRequest Ride\u201d use case work? The customer will put a request for a ride. One of the Aggregator servers will take the request and asks QuadTree servers to return nearby drivers. The Aggregator server collects all the results and sorts them by ratings. The Aggregator server will send a notification to the top (say three) drivers simultaneously, whichever driver accepts the request first will be assigned the ride. The other drivers will receive a cancellation request. If none of the three drivers respond, the Aggregator will request a ride from the next three drivers from the list. Once a driver accepts a request, the customer is notified. 5. Fault Tolerance and Replication What if a Driver Location server or Notification server dies? We would need replicas of these servers, so that if the primary dies the secondary can take control. Also, we can store this data in some persistent storage like SSDs that can provide fast IOs; this will ensure that if both primary and secondary servers die we can recover the data from the persistent storage. 6. Ranking How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we return top rated drivers within a given radius? Let\u2019s assume we keep track of the overall ratings of each driver in our database and QuadTree. An aggregated number can represent this popularity in our system, e.g., how many stars does a driver get out of ten? While searching for the top 10 drivers within a given radius, we can ask each partition of the QuadTree to return the top 10 drivers with a maximum rating. The aggregator server can then determine the top 10 drivers among all the drivers returned by different partitions. 7. Advanced Issues How will we handle clients on slow and disconnecting networks? What if a client gets disconnected when they are a part of a ride? How will we handle billing in such a scenario? How about if clients pull all the information, compared to servers always pushing it?","title":"Designing Uber backend"},{"location":"DesigningUberBackend/#designing-uber-backend","text":"Let's design a ride-sharing service like Uber, which connects passengers who need a ride with drivers who have a car. Similar Services: Lyft, Didi, Via, Sidecar, etc. Difficulty level: Hard Prerequisite: Designing Yelp","title":"Designing Uber backend"},{"location":"DesigningUberBackend/#1-what-is-uber","text":"Uber enables its customers to book drivers for taxi rides. Uber drivers use their personal cars to drive customers around. Both customers and drivers communicate with each other through their smartphones using the Uber app.","title":"1. What is Uber?"},{"location":"DesigningUberBackend/#2-requirements-and-goals-of-the-system","text":"Let\u2019s start with building a simpler version of Uber. There are two types of users in our system: 1) Drivers 2) Customers. Drivers need to regularly notify the service about their current location and their availability to pick passengers. Passengers get to see all the nearby available drivers. Customer can request a ride; nearby drivers are notified that a customer is ready to be picked up. Once a driver and a customer accept a ride, they can constantly see each other\u2019s current location until the trip finishes. Upon reaching the destination, the driver marks the journey complete to become available for the next ride.","title":"2. Requirements and Goals of the System"},{"location":"DesigningUberBackend/#3-capacity-estimation-and-constraints","text":"Let\u2019s assume we have 300M customers and 1M drivers with 1M daily active customers and 500K daily active drivers. Let\u2019s assume 1M daily rides. Let\u2019s assume that all active drivers notify their current location every three seconds. Once a customer puts in a request for a ride, the system should be able to contact drivers in real-time.","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningUberBackend/#4-basic-system-design-and-algorithm","text":"We will take the solution discussed in Designing Yelp and modify it to make it work for the above-mentioned \u201cUber\u201d use cases. The biggest difference we have is that our QuadTree was not built keeping in mind that there would be frequent updates to it. So, we have two issues with our Dynamic Grid solution: Since all active drivers are reporting their locations every three seconds, we need to update our data structures to reflect that. If we have to update the QuadTree for every change in the driver\u2019s position, it will take a lot of time and resources. To update a driver to its new location, we must find the right grid based on the driver\u2019s previous location. If the new position does not belong to the current grid, we have to remove the driver from the current grid and move/reinsert the user to the correct grid. After this move, if the new grid reaches the maximum limit of drivers, we have to repartition it. We need to have a quick mechanism to propagate the current location of all the nearby drivers to any active customer in that area. Also, when a ride is in progress, our system needs to notify both the driver and passenger about the current location of the car. Although our QuadTree helps us find nearby drivers quickly, a fast update in the tree is not guaranteed. Do we need to modify our QuadTree every time a driver reports their location? If we don\u2019t update our QuadTree with every update from the driver, it will have some old data and will not reflect the current location of drivers correctly. If you recall, our purpose of building the QuadTree was to find nearby drivers (or places) efficiently. Since all active drivers report their location every three seconds, therefore there will be a lot more updates happening to our tree than querying for nearby drivers. So, what if we keep the latest position reported by all drivers in a hash table and update our QuadTree a little less frequently? Let\u2019s assume we guarantee that a driver\u2019s current location will be reflected in the QuadTree within 15 seconds. Meanwhile, we will maintain a hash table that will store the current location reported by drivers; let\u2019s call this DriverLocationHT. How much memory we need for DriverLocationHT? We need to store DriveID, their present and old location, in the hash table. So, we need a total of 35 bytes to store one record: DriverID (3 bytes - 1 million drivers) Old latitude (8 bytes) Old longitude (8 bytes) New latitude (8 bytes) New longitude (8 bytes) Total = 35 bytes If we have 1 million total drivers, we need the following memory (ignoring hash table overhead): 1 million * 35 bytes => 35 MB How much bandwidth will our service consume to receive location updates from all drivers? If we get DriverID and their location, it will be (3+16 => 19 bytes). If we receive this information every three seconds from 500K daily active drivers, we will be getting 9.5MB per three seconds. Do we need to distribute DriverLocationHT onto multiple servers? Although our memory and bandwidth requirements don\u2019t require this, since all this information can easily be stored on one server, but, for scalability, performance, and fault tolerance, we should distribute DriverLocationHT onto multiple servers. We can distribute based on the DriverID to make the distribution completely random. Let\u2019s call the machines holding DriverLocationHT the Driver Location server. Other than storing the driver\u2019s location, each of these servers will do two things: As soon as the server receives an update for a driver\u2019s location, they will broadcast that information to all the interested customers. The server needs to notify the respective QuadTree server to refresh the driver\u2019s location. As discussed above, this can happen every 10 seconds. How can we efficiently broadcast the driver\u2019s location to customers? We can have a Push Model where the server will push the positions to all the relevant users. We can have a dedicated Notification Service that can broadcast the current location of drivers to all the interested customers. We can build our Notification service on a publisher/subscriber model. When a customer opens the Uber app on their cell phone, they query the server to find nearby drivers. On the server side, before returning the list of drivers to the customer, we will subscribe the customer for all the updates from those drivers. We can maintain a list of customers (subscribers) interested in knowing the location of a driver and, whenever we have an update in DriverLocationHT for that driver, we can broadcast the current location of the driver to all subscribed customers. This way, our system makes sure that we always show the driver\u2019s current position to the customer. How much memory will we need to store all these subscriptions? As we have estimated above, we will have 1M daily active customers and 500K daily active drivers. On average let\u2019s assume that five customers subscribe to one driver. Let\u2019s assume we store all this information in a hash table so that we can update it efficiently. We need to store driver and customer IDs to maintain the subscriptions. Assuming we will need 3 bytes for DriverID and 8 bytes for CustomerID, we will need 21MB of memory. (500K * 3) + (500K * 5 * 8 ) ~= 21 MB How much bandwidth will we need to broadcast the driver\u2019s location to customers? For every active driver, we have five subscribers, so the total subscribers we have: 5 * 500K => 2.5M To all these customers we need to send DriverID (3 bytes) and their location (16 bytes) every second, so, we need the following bandwidth: 2.5M * 19 bytes => 47.5 MB/s How can we efficiently implement Notification service? We can either use HTTP long polling or push notifications. How will the new publishers/drivers get added for a current customer? As we have proposed above, customers will be subscribed to nearby drivers when they open the Uber app for the first time, what will happen when a new driver enters the area the customer is looking at? To add a new customer/driver subscription dynamically, we need to keep track of the area the customer is watching. This will make our solution complicated; how about if instead of pushing this information, clients pull it from the server? How about if clients pull information about nearby drivers from the server? Clients can send their current location, and the server will find all the nearby drivers from the QuadTree to return them to the client. Upon receiving this information, the client can update their screen to reflect the current positions of the drivers. Clients can query every five seconds to limit the number of round trips to the server. This solution looks simpler compared to the push model described above. Do we need to repartition a grid as soon as it reaches the maximum limit? We can have a cushion to let each grid grow a little bigger beyond the limit before we decide to partition it. Let\u2019s say our grids can grow/shrink an extra 10% before we partition/merge them. This should decrease the load for a grid partition or merge on high traffic grids. How would \u201cRequest Ride\u201d use case work? The customer will put a request for a ride. One of the Aggregator servers will take the request and asks QuadTree servers to return nearby drivers. The Aggregator server collects all the results and sorts them by ratings. The Aggregator server will send a notification to the top (say three) drivers simultaneously, whichever driver accepts the request first will be assigned the ride. The other drivers will receive a cancellation request. If none of the three drivers respond, the Aggregator will request a ride from the next three drivers from the list. Once a driver accepts a request, the customer is notified.","title":"4. Basic System Design and Algorithm"},{"location":"DesigningUberBackend/#5-fault-tolerance-and-replication","text":"What if a Driver Location server or Notification server dies? We would need replicas of these servers, so that if the primary dies the secondary can take control. Also, we can store this data in some persistent storage like SSDs that can provide fast IOs; this will ensure that if both primary and secondary servers die we can recover the data from the persistent storage.","title":"5. Fault Tolerance and Replication"},{"location":"DesigningUberBackend/#6-ranking","text":"How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we return top rated drivers within a given radius? Let\u2019s assume we keep track of the overall ratings of each driver in our database and QuadTree. An aggregated number can represent this popularity in our system, e.g., how many stars does a driver get out of ten? While searching for the top 10 drivers within a given radius, we can ask each partition of the QuadTree to return the top 10 drivers with a maximum rating. The aggregator server can then determine the top 10 drivers among all the drivers returned by different partitions.","title":"6. Ranking"},{"location":"DesigningUberBackend/#7-advanced-issues","text":"How will we handle clients on slow and disconnecting networks? What if a client gets disconnected when they are a part of a ride? How will we handle billing in such a scenario? How about if clients pull all the information, compared to servers always pushing it?","title":"7. Advanced Issues"},{"location":"DesigningWebCrawler/","text":"Designing a Web Crawler Let's design a Web Crawler that will systematically browse and download the World Wide Web. Web crawlers are also known as web spiders, robots, worms, walkers, and bots. Difficulty Level: Hard 1. What is a Web Crawler? A web crawler is a software program which browses the World Wide Web in a methodical and automated manner. It collects documents by recursively fetching links from a set of starting pages. Many sites, particularly search engines, use web crawling as a means of providing up-to-date data. Search engines download all the pages to create an index on them to perform faster searches. Some other uses of web crawlers are: To test web pages and links for valid syntax and structure. To monitor sites to see when their structure or contents change. To maintain mirror sites for popular Web sites. To search for copyright infringements. To build a special-purpose index, e.g., one that has some understanding of the content stored in multimedia files on the Web. 2. Requirements and Goals of the System Let\u2019s assume we need to crawl all the web. Scalability: Our service needs to be scalable such that it can crawl the entire Web and can be used to fetch hundreds of millions of Web documents. Extensibility: Our service should be designed in a modular way with the expectation that new functionality will be added to it. There could be newer document types that needs to be downloaded and processed in the future. 3. Some Design Considerations Crawling the web is a complex task, and there are many ways to go about it. We should be asking a few questions before going any further: Is it a crawler for HTML pages only? Or should we fetch and store other types of media, such as sound files, images, videos, etc.? This is important because the answer can change the design. If we are writing a general-purpose crawler to download different media types, we might want to break down the parsing module into different sets of modules: one for HTML, another for images, or another for videos, where each module extracts what is considered interesting for that media type. Let\u2019s assume for now that our crawler is going to deal with HTML only, but it should be extensible and make it easy to add support for new media types. What protocols are we looking at? HTTP? What about FTP links? What different protocols should our crawler handle? For the sake of the exercise, we will assume HTTP. Again, it shouldn\u2019t be hard to extend the design to use FTP and other protocols later. What is the expected number of pages we will crawl? How big will the URL database become? Let\u2019s assume we need to crawl one billion websites. Since a website can contain many, many URLs, let\u2019s assume an upper bound of 15 billion different web pages that will be reached by our crawler. What is \u2018RobotsExclusion\u2019 and how should we deal with it? Courteous Web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web crawler to fetch a special document called robot.txt which contains these declarations from a Web site before downloading any real content from it. 4. Capacity Estimation and Constraints If we want to crawl 15 billion pages within four weeks, how many pages do we need to fetch per second? 15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec What about storage? Page sizes vary a lot, but, as mentioned above since, we will be dealing with HTML text only, let\u2019s assume an average page size of 100KB. With each page, if we are storing 500 bytes of metadata, total storage we would need: 15B * (100KB + 500) ~= 1.5 petabytes Assuming a 70% capacity model (we don\u2019t want to go above 70% of the total capacity of our storage system), total storage we will need: 1.5 petabytes / 0.7 ~= 2.14 petabytes 5. High Level design The basic algorithm executed by any Web crawler is to take a list of seed URLs as its input and repeatedly execute the following steps. Pick a URL from the unvisited URL list. Determine the IP Address of its host-name. Establish a connection to the host to download the corresponding document. Parse the document contents to look for new URLs. Add the new URLs to the list of unvisited URLs. Process the downloaded document, e.g., store it or index its contents, etc. Go back to step 1 How to crawl? Breadth first or depth first? Breadth-first search (BFS) is usually used. However, Depth First Search (DFS) is also utilized in some situations, such as, if your crawler has already established a connection with the website, it might just DFS all the URLs within this website to save some handshaking overhead. Path-ascending crawling: Path-ascending crawling can help discover a lot of isolated resources or resources for which no inbound link would have been found in regular crawling of a particular Web site. In this scheme, a crawler would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of http://foo.com/a/b/page.html, it will attempt to crawl /a/b/, /a/, and /. Difficulties in implementing efficient web crawler There are two important characteristics of the Web that makes Web crawling a very difficult task: 1. Large volume of Web pages: A large volume of web pages implies that web crawler can only download a fraction of the web pages at any time and hence it is critical that web crawler should be intelligent enough to prioritize download. 2. Rate of change on web pages. Another problem with today\u2019s dynamic world is that web pages on the internet change very frequently. As a result, by the time the crawler is downloading the last page from a site, the page may change, or a new page may be added to the site. A bare minimum crawler needs at least these components: URL frontier: To store the list of URLs to download and also prioritize which URLs should be crawled first. HTTP Fetcher: To retrieve a web page from the server. Extractor: To extract links from HTML documents. Duplicate Eliminator: To make sure the same content is not extracted twice unintentionally. Datastore: To store retrieved pages, URLs, and other metadata. 6. Detailed Component Design Let\u2019s assume our crawler is running on one server and all the crawling is done by multiple working threads where each working thread performs all the steps needed to download and process a document in a loop. The first step of this loop is to remove an absolute URL from the shared URL frontier for downloading. An absolute URL begins with a scheme (e.g., \u201cHTTP\u201d) which identifies the network protocol that should be used to download it. We can implement these protocols in a modular way for extensibility, so that later if our crawler needs to support more protocols, it can be easily done. Based on the URL\u2019s scheme, the worker calls the appropriate protocol module to download the document. After downloading, the document is placed into a Document Input Stream (DIS). Putting documents into DIS will enable other modules to re-read the document multiple times. Once the document has been written to the DIS, the worker thread invokes the dedupe test to determine whether this document (associated with a different URL) has been seen before. If so, the document is not processed any further and the worker thread removes the next URL from the frontier. Next, our crawler needs to process the downloaded document. Each document can have a different MIME type like HTML page, Image, Video, etc. We can implement these MIME schemes in a modular way, so that later if our crawler needs to support more types, we can easily implement them. Based on the downloaded document\u2019s MIME type, the worker invokes the process method of each processing module associated with that MIME type. Furthermore, our HTML processing module will extract all links from the page. Each link is converted into an absolute URL and tested against a user-supplied URL filter to determine if it should be downloaded. If the URL passes the filter, the worker performs the URL-seen test, which checks if the URL has been seen before, namely, if it is in the URL frontier or has already been downloaded. If the URL is new, it is added to the frontier. Let\u2019s discuss these components one by one, and see how they can be distributed onto multiple machines: 1. The URL frontier: The URL frontier is the data structure that contains all the URLs that remain to be downloaded. We can crawl by performing a breadth-first traversal of the Web, starting from the pages in the seed set. Such traversals are easily implemented by using a FIFO queue. Since we\u2019ll be having a huge list of URLs to crawl, we can distribute our URL frontier into multiple servers. Let\u2019s assume on each server we have multiple worker threads performing the crawling tasks. Let\u2019s also assume that our hash function maps each URL to a server which will be responsible for crawling it. Following politeness requirements must be kept in mind while designing a distributed URL frontier: Our crawler should not overload a server by downloading a lot of pages from it. We should not have multiple machines connecting a web server. To implement this politeness constraint our crawler can have a collection of distinct FIFO sub-queues on each server. Each worker thread will have its separate sub-queue, from which it removes URLs for crawling. When a new URL needs to be added, the FIFO sub-queue in which it is placed will be determined by the URL\u2019s canonical hostname. Our hash function can map each hostname to a thread number. Together, these two points imply that, at most, one worker thread will download documents from a given Web server and also, by using FIFO queue, it\u2019ll not overload a Web server. How big will our URL frontier be? The size would be in the hundreds of millions of URLs. Hence, we need to store our URLs on a disk. We can implement our queues in such a way that they have separate buffers for enqueuing and dequeuing. Enqueue buffer, once filled, will be dumped to the disk, whereas dequeue buffer will keep a cache of URLs that need to be visited; it can periodically read from disk to fill the buffer. 2. The fetcher module: The purpose of a fetcher module is to download the document corresponding to a given URL using the appropriate network protocol like HTTP. As discussed above, webmasters create robot.txt to make certain parts of their websites off limits for the crawler. To avoid downloading this file on every request, our crawler\u2019s HTTP protocol module can maintain a fixed-sized cache mapping host-names to their robot\u2019s exclusion rules. 3. Document input stream: Our crawler\u2019s design enables the same document to be processed by multiple processing modules. To avoid downloading a document multiple times, we cache the document locally using an abstraction called a Document Input Stream (DIS). A DIS is an input stream that caches the entire contents of the document read from the internet. It also provides methods to re-read the document. The DIS can cache small documents (64 KB or less) entirely in memory, while larger documents can be temporarily written to a backing file. Each worker thread has an associated DIS, which it reuses from document to document. After extracting a URL from the frontier, the worker passes that URL to the relevant protocol module, which initializes the DIS from a network connection to contain the document\u2019s contents. The worker then passes the DIS to all relevant processing modules. 4. Document Dedupe test: Many documents on the Web are available under multiple, different URLs. There are also many cases in which documents are mirrored on various servers. Both of these effects will cause any Web crawler to download the same document multiple times. To prevent processing of a document more than once, we perform a dedupe test on each document to remove duplication. To perform this test, we can calculate a 64-bit checksum of every processed document and store it in a database. For every new document, we can compare its checksum to all the previously calculated checksums to see the document has been seen before. We can use MD5 or SHA to calculate these checksums. How big would be the checksum store? If the whole purpose of our checksum store is to do dedupe, then we just need to keep a unique set containing checksums of all previously processed document. Considering 15 billion distinct web pages, we would need: 15B * 8 bytes => 120 GB Although this can fit into a modern-day server\u2019s memory, if we don\u2019t have enough memory available, we can keep smaller LRU based cache on each server with everything backed by persistent storage. The dedupe test first checks if the checksum is present in the cache. If not, it has to check if the checksum resides in the back storage. If the checksum is found, we will ignore the document. Otherwise, it will be added to the cache and back storage. 5. URL filters: The URL filtering mechanism provides a customizable way to control the set of URLs that are downloaded. This is used to blacklist websites so that our crawler can ignore them. Before adding each URL to the frontier, the worker thread consults the user-supplied URL filter. We can define filters to restrict URLs by domain, prefix, or protocol type. 6. Domain name resolution: Before contacting a Web server, a Web crawler must use the Domain Name Service (DNS) to map the Web server\u2019s hostname into an IP address. DNS name resolution will be a big bottleneck of our crawlers given the amount of URLs we will be working with. To avoid repeated requests, we can start caching DNS results by building our local DNS server. 7. URL dedupe test: While extracting links, any Web crawler will encounter multiple links to the same document. To avoid downloading and processing a document multiple times, a URL dedupe test must be performed on each extracted link before adding it to the URL frontier. To perform the URL dedupe test, we can store all the URLs seen by our crawler in canonical form in a database. To save space, we do not store the textual representation of each URL in the URL set, but rather a fixed-sized checksum. To reduce the number of operations on the database store, we can keep an in-memory cache of popular URLs on each host shared by all threads. The reason to have this cache is that links to some URLs are quite common, so caching the popular ones in memory will lead to a high in-memory hit rate. How much storage we would need for URL\u2019s store? If the whole purpose of our checksum is to do URL dedupe, then we just need to keep a unique set containing checksums of all previously seen URLs. Considering 15 billion distinct URLs and 4 bytes for checksum, we would need: 15B * 4 bytes => 60 GB Can we use bloom filters for deduping? Bloom filters are a probabilistic data structure for set membership testing that may yield false positives. A large bit vector represents the set. An element is added to the set by computing \u2018n\u2019 hash functions of the element and setting the corresponding bits. An element is deemed to be in the set if the bits at all \u2018n\u2019 of the element\u2019s hash locations are set. Hence, a document may incorrectly be deemed to be in the set, but false negatives are not possible. The disadvantage of using a bloom filter for the URL seen test is that each false positive will cause the URL not to be added to the frontier and, therefore, the document will never be downloaded. The chance of a false positive can be reduced by making the bit vector larger. 8. Checkpointing: A crawl of the entire Web takes weeks to complete. To guard against failures, our crawler can write regular snapshots of its state to the disk. An interrupted or aborted crawl can easily be restarted from the latest checkpoint. 7. Fault tolerance We should use consistent hashing for distribution among crawling servers. Consistent hashing will not only help in replacing a dead host, but also help in distributing load among crawling servers. All our crawling servers will be performing regular checkpointing and storing their FIFO queues to disks. If a server goes down, we can replace it. Meanwhile, consistent hashing should shift the load to other servers. 8. Data Partitioning Our crawler will be dealing with three kinds of data: 1) URLs to visit 2) URL checksums for dedupe 3) Document checksums for dedupe. Since we are distributing URLs based on the hostnames, we can store these data on the same host. So, each host will store its set of URLs that need to be visited, checksums of all the previously visited URLs and checksums of all the downloaded documents. Since we will be using consistent hashing, we can assume that URLs will be redistributed from overloaded hosts. Each host will perform checkpointing periodically and dump a snapshot of all the data it is holding onto a remote server. This will ensure that if a server dies down another server can replace it by taking its data from the last snapshot. 9. Crawler Traps There are many crawler traps, spam sites, and cloaked content. A crawler trap is a URL or set of URLs that cause a crawler to crawl indefinitely. Some crawler traps are unintentional. For example, a symbolic link within a file system can create a cycle. Other crawler traps are introduced intentionally. For example, people have written traps that dynamically generate an infinite Web of documents. The motivations behind such traps vary. Anti-spam traps are designed to catch crawlers used by spammers looking for email addresses, while other sites use traps to catch search engine crawlers to boost their search ratings.","title":"Designing a Web Crawler"},{"location":"DesigningWebCrawler/#designing-a-web-crawler","text":"Let's design a Web Crawler that will systematically browse and download the World Wide Web. Web crawlers are also known as web spiders, robots, worms, walkers, and bots. Difficulty Level: Hard","title":"Designing a Web Crawler"},{"location":"DesigningWebCrawler/#1-what-is-a-web-crawler","text":"A web crawler is a software program which browses the World Wide Web in a methodical and automated manner. It collects documents by recursively fetching links from a set of starting pages. Many sites, particularly search engines, use web crawling as a means of providing up-to-date data. Search engines download all the pages to create an index on them to perform faster searches. Some other uses of web crawlers are: To test web pages and links for valid syntax and structure. To monitor sites to see when their structure or contents change. To maintain mirror sites for popular Web sites. To search for copyright infringements. To build a special-purpose index, e.g., one that has some understanding of the content stored in multimedia files on the Web.","title":"1. What is a Web Crawler?"},{"location":"DesigningWebCrawler/#2-requirements-and-goals-of-the-system","text":"Let\u2019s assume we need to crawl all the web. Scalability: Our service needs to be scalable such that it can crawl the entire Web and can be used to fetch hundreds of millions of Web documents. Extensibility: Our service should be designed in a modular way with the expectation that new functionality will be added to it. There could be newer document types that needs to be downloaded and processed in the future.","title":"2. Requirements and Goals of the System"},{"location":"DesigningWebCrawler/#3-some-design-considerations","text":"Crawling the web is a complex task, and there are many ways to go about it. We should be asking a few questions before going any further: Is it a crawler for HTML pages only? Or should we fetch and store other types of media, such as sound files, images, videos, etc.? This is important because the answer can change the design. If we are writing a general-purpose crawler to download different media types, we might want to break down the parsing module into different sets of modules: one for HTML, another for images, or another for videos, where each module extracts what is considered interesting for that media type. Let\u2019s assume for now that our crawler is going to deal with HTML only, but it should be extensible and make it easy to add support for new media types. What protocols are we looking at? HTTP? What about FTP links? What different protocols should our crawler handle? For the sake of the exercise, we will assume HTTP. Again, it shouldn\u2019t be hard to extend the design to use FTP and other protocols later. What is the expected number of pages we will crawl? How big will the URL database become? Let\u2019s assume we need to crawl one billion websites. Since a website can contain many, many URLs, let\u2019s assume an upper bound of 15 billion different web pages that will be reached by our crawler. What is \u2018RobotsExclusion\u2019 and how should we deal with it? Courteous Web crawlers implement the Robots Exclusion Protocol, which allows Webmasters to declare parts of their sites off limits to crawlers. The Robots Exclusion Protocol requires a Web crawler to fetch a special document called robot.txt which contains these declarations from a Web site before downloading any real content from it.","title":"3. Some Design Considerations"},{"location":"DesigningWebCrawler/#4-capacity-estimation-and-constraints","text":"If we want to crawl 15 billion pages within four weeks, how many pages do we need to fetch per second? 15B / (4 weeks * 7 days * 86400 sec) ~= 6200 pages/sec What about storage? Page sizes vary a lot, but, as mentioned above since, we will be dealing with HTML text only, let\u2019s assume an average page size of 100KB. With each page, if we are storing 500 bytes of metadata, total storage we would need: 15B * (100KB + 500) ~= 1.5 petabytes Assuming a 70% capacity model (we don\u2019t want to go above 70% of the total capacity of our storage system), total storage we will need: 1.5 petabytes / 0.7 ~= 2.14 petabytes","title":"4. Capacity Estimation and Constraints"},{"location":"DesigningWebCrawler/#5-high-level-design","text":"The basic algorithm executed by any Web crawler is to take a list of seed URLs as its input and repeatedly execute the following steps. Pick a URL from the unvisited URL list. Determine the IP Address of its host-name. Establish a connection to the host to download the corresponding document. Parse the document contents to look for new URLs. Add the new URLs to the list of unvisited URLs. Process the downloaded document, e.g., store it or index its contents, etc. Go back to step 1 How to crawl? Breadth first or depth first? Breadth-first search (BFS) is usually used. However, Depth First Search (DFS) is also utilized in some situations, such as, if your crawler has already established a connection with the website, it might just DFS all the URLs within this website to save some handshaking overhead. Path-ascending crawling: Path-ascending crawling can help discover a lot of isolated resources or resources for which no inbound link would have been found in regular crawling of a particular Web site. In this scheme, a crawler would ascend to every path in each URL that it intends to crawl. For example, when given a seed URL of http://foo.com/a/b/page.html, it will attempt to crawl /a/b/, /a/, and /. Difficulties in implementing efficient web crawler There are two important characteristics of the Web that makes Web crawling a very difficult task: 1. Large volume of Web pages: A large volume of web pages implies that web crawler can only download a fraction of the web pages at any time and hence it is critical that web crawler should be intelligent enough to prioritize download. 2. Rate of change on web pages. Another problem with today\u2019s dynamic world is that web pages on the internet change very frequently. As a result, by the time the crawler is downloading the last page from a site, the page may change, or a new page may be added to the site. A bare minimum crawler needs at least these components: URL frontier: To store the list of URLs to download and also prioritize which URLs should be crawled first. HTTP Fetcher: To retrieve a web page from the server. Extractor: To extract links from HTML documents. Duplicate Eliminator: To make sure the same content is not extracted twice unintentionally. Datastore: To store retrieved pages, URLs, and other metadata.","title":"5. High Level design"},{"location":"DesigningWebCrawler/#6-detailed-component-design","text":"Let\u2019s assume our crawler is running on one server and all the crawling is done by multiple working threads where each working thread performs all the steps needed to download and process a document in a loop. The first step of this loop is to remove an absolute URL from the shared URL frontier for downloading. An absolute URL begins with a scheme (e.g., \u201cHTTP\u201d) which identifies the network protocol that should be used to download it. We can implement these protocols in a modular way for extensibility, so that later if our crawler needs to support more protocols, it can be easily done. Based on the URL\u2019s scheme, the worker calls the appropriate protocol module to download the document. After downloading, the document is placed into a Document Input Stream (DIS). Putting documents into DIS will enable other modules to re-read the document multiple times. Once the document has been written to the DIS, the worker thread invokes the dedupe test to determine whether this document (associated with a different URL) has been seen before. If so, the document is not processed any further and the worker thread removes the next URL from the frontier. Next, our crawler needs to process the downloaded document. Each document can have a different MIME type like HTML page, Image, Video, etc. We can implement these MIME schemes in a modular way, so that later if our crawler needs to support more types, we can easily implement them. Based on the downloaded document\u2019s MIME type, the worker invokes the process method of each processing module associated with that MIME type. Furthermore, our HTML processing module will extract all links from the page. Each link is converted into an absolute URL and tested against a user-supplied URL filter to determine if it should be downloaded. If the URL passes the filter, the worker performs the URL-seen test, which checks if the URL has been seen before, namely, if it is in the URL frontier or has already been downloaded. If the URL is new, it is added to the frontier. Let\u2019s discuss these components one by one, and see how they can be distributed onto multiple machines: 1. The URL frontier: The URL frontier is the data structure that contains all the URLs that remain to be downloaded. We can crawl by performing a breadth-first traversal of the Web, starting from the pages in the seed set. Such traversals are easily implemented by using a FIFO queue. Since we\u2019ll be having a huge list of URLs to crawl, we can distribute our URL frontier into multiple servers. Let\u2019s assume on each server we have multiple worker threads performing the crawling tasks. Let\u2019s also assume that our hash function maps each URL to a server which will be responsible for crawling it. Following politeness requirements must be kept in mind while designing a distributed URL frontier: Our crawler should not overload a server by downloading a lot of pages from it. We should not have multiple machines connecting a web server. To implement this politeness constraint our crawler can have a collection of distinct FIFO sub-queues on each server. Each worker thread will have its separate sub-queue, from which it removes URLs for crawling. When a new URL needs to be added, the FIFO sub-queue in which it is placed will be determined by the URL\u2019s canonical hostname. Our hash function can map each hostname to a thread number. Together, these two points imply that, at most, one worker thread will download documents from a given Web server and also, by using FIFO queue, it\u2019ll not overload a Web server. How big will our URL frontier be? The size would be in the hundreds of millions of URLs. Hence, we need to store our URLs on a disk. We can implement our queues in such a way that they have separate buffers for enqueuing and dequeuing. Enqueue buffer, once filled, will be dumped to the disk, whereas dequeue buffer will keep a cache of URLs that need to be visited; it can periodically read from disk to fill the buffer. 2. The fetcher module: The purpose of a fetcher module is to download the document corresponding to a given URL using the appropriate network protocol like HTTP. As discussed above, webmasters create robot.txt to make certain parts of their websites off limits for the crawler. To avoid downloading this file on every request, our crawler\u2019s HTTP protocol module can maintain a fixed-sized cache mapping host-names to their robot\u2019s exclusion rules. 3. Document input stream: Our crawler\u2019s design enables the same document to be processed by multiple processing modules. To avoid downloading a document multiple times, we cache the document locally using an abstraction called a Document Input Stream (DIS). A DIS is an input stream that caches the entire contents of the document read from the internet. It also provides methods to re-read the document. The DIS can cache small documents (64 KB or less) entirely in memory, while larger documents can be temporarily written to a backing file. Each worker thread has an associated DIS, which it reuses from document to document. After extracting a URL from the frontier, the worker passes that URL to the relevant protocol module, which initializes the DIS from a network connection to contain the document\u2019s contents. The worker then passes the DIS to all relevant processing modules. 4. Document Dedupe test: Many documents on the Web are available under multiple, different URLs. There are also many cases in which documents are mirrored on various servers. Both of these effects will cause any Web crawler to download the same document multiple times. To prevent processing of a document more than once, we perform a dedupe test on each document to remove duplication. To perform this test, we can calculate a 64-bit checksum of every processed document and store it in a database. For every new document, we can compare its checksum to all the previously calculated checksums to see the document has been seen before. We can use MD5 or SHA to calculate these checksums. How big would be the checksum store? If the whole purpose of our checksum store is to do dedupe, then we just need to keep a unique set containing checksums of all previously processed document. Considering 15 billion distinct web pages, we would need: 15B * 8 bytes => 120 GB Although this can fit into a modern-day server\u2019s memory, if we don\u2019t have enough memory available, we can keep smaller LRU based cache on each server with everything backed by persistent storage. The dedupe test first checks if the checksum is present in the cache. If not, it has to check if the checksum resides in the back storage. If the checksum is found, we will ignore the document. Otherwise, it will be added to the cache and back storage. 5. URL filters: The URL filtering mechanism provides a customizable way to control the set of URLs that are downloaded. This is used to blacklist websites so that our crawler can ignore them. Before adding each URL to the frontier, the worker thread consults the user-supplied URL filter. We can define filters to restrict URLs by domain, prefix, or protocol type. 6. Domain name resolution: Before contacting a Web server, a Web crawler must use the Domain Name Service (DNS) to map the Web server\u2019s hostname into an IP address. DNS name resolution will be a big bottleneck of our crawlers given the amount of URLs we will be working with. To avoid repeated requests, we can start caching DNS results by building our local DNS server. 7. URL dedupe test: While extracting links, any Web crawler will encounter multiple links to the same document. To avoid downloading and processing a document multiple times, a URL dedupe test must be performed on each extracted link before adding it to the URL frontier. To perform the URL dedupe test, we can store all the URLs seen by our crawler in canonical form in a database. To save space, we do not store the textual representation of each URL in the URL set, but rather a fixed-sized checksum. To reduce the number of operations on the database store, we can keep an in-memory cache of popular URLs on each host shared by all threads. The reason to have this cache is that links to some URLs are quite common, so caching the popular ones in memory will lead to a high in-memory hit rate. How much storage we would need for URL\u2019s store? If the whole purpose of our checksum is to do URL dedupe, then we just need to keep a unique set containing checksums of all previously seen URLs. Considering 15 billion distinct URLs and 4 bytes for checksum, we would need: 15B * 4 bytes => 60 GB Can we use bloom filters for deduping? Bloom filters are a probabilistic data structure for set membership testing that may yield false positives. A large bit vector represents the set. An element is added to the set by computing \u2018n\u2019 hash functions of the element and setting the corresponding bits. An element is deemed to be in the set if the bits at all \u2018n\u2019 of the element\u2019s hash locations are set. Hence, a document may incorrectly be deemed to be in the set, but false negatives are not possible. The disadvantage of using a bloom filter for the URL seen test is that each false positive will cause the URL not to be added to the frontier and, therefore, the document will never be downloaded. The chance of a false positive can be reduced by making the bit vector larger. 8. Checkpointing: A crawl of the entire Web takes weeks to complete. To guard against failures, our crawler can write regular snapshots of its state to the disk. An interrupted or aborted crawl can easily be restarted from the latest checkpoint.","title":"6. Detailed Component Design"},{"location":"DesigningWebCrawler/#7-fault-tolerance","text":"We should use consistent hashing for distribution among crawling servers. Consistent hashing will not only help in replacing a dead host, but also help in distributing load among crawling servers. All our crawling servers will be performing regular checkpointing and storing their FIFO queues to disks. If a server goes down, we can replace it. Meanwhile, consistent hashing should shift the load to other servers.","title":"7. Fault tolerance"},{"location":"DesigningWebCrawler/#8-data-partitioning","text":"Our crawler will be dealing with three kinds of data: 1) URLs to visit 2) URL checksums for dedupe 3) Document checksums for dedupe. Since we are distributing URLs based on the hostnames, we can store these data on the same host. So, each host will store its set of URLs that need to be visited, checksums of all the previously visited URLs and checksums of all the downloaded documents. Since we will be using consistent hashing, we can assume that URLs will be redistributed from overloaded hosts. Each host will perform checkpointing periodically and dump a snapshot of all the data it is holding onto a remote server. This will ensure that if a server dies down another server can replace it by taking its data from the last snapshot.","title":"8. Data Partitioning"},{"location":"DesigningWebCrawler/#9-crawler-traps","text":"There are many crawler traps, spam sites, and cloaked content. A crawler trap is a URL or set of URLs that cause a crawler to crawl indefinitely. Some crawler traps are unintentional. For example, a symbolic link within a file system can create a cycle. Other crawler traps are introduced intentionally. For example, people have written traps that dynamically generate an infinite Web of documents. The motivations behind such traps vary. Anti-spam traps are designed to catch crawlers used by spammers looking for email addresses, while other sites use traps to catch search engine crawlers to boost their search ratings.","title":"9. Crawler Traps"},{"location":"DesigningYelporNearbyFriends/","text":"Designing Yelp or Nearby Friends Let's design a Yelp like service, where users can search for nearby places like restaurants, theaters, or shopping malls, etc., and can also add/view reviews of places. Similar Services: Proximity server. Difficulty Level: Hard 1. Why Yelp or Proximity Server? Proximity servers are used to discover nearby attractions like places, events, etc. If you haven\u2019t used yelp.com before, please try it before proceeding (you can search for nearby restaurants, theaters, etc.) and spend some time understanding different options that the website offers. This will help you a lot in understanding this chapter better. 2. Requirements and Goals of the System What do we wish to achieve from a Yelp like service? Our service will be storing information about different places so that users can perform a search on them. Upon querying, our service will return a list of places around the user. Our Yelp-like service should meet the following requirements: Functional Requirements: Users should be able to add/delete/update Places. Given their location (longitude/latitude), users should be able to find all nearby places within a given radius. Users should be able to add feedback/review about a place. The feedback can have pictures, text, and a rating. Non-functional Requirements: Users should have a real-time search experience with minimum latency. Our service should support a heavy search load. There will be a lot of search requests compared to adding a new place. 3. Scale Estimation Let\u2019s build our system assuming that we have 500M places and 100K queries per second (QPS). Let\u2019s also assume a 20% growth in the number of places and QPS each year. 4. Database Schema Each Place can have the following fields: LocationID (8 bytes): Uniquely identifies a location. Name (256 bytes) Latitude (8 bytes) Longitude (8 bytes) Description (512 bytes) Category (1 byte): E.g., coffee shop, restaurant, theater, etc. Although a four bytes number can uniquely identify 500M locations, with future growth in mind, we will go with 8 bytes for LocationID. Total size: 8 + 256 + 8 + 8 + 512 + 1 => 793 bytes We also need to store reviews, photos, and ratings of a Place. We can have a separate table to store reviews for Places: LocationID (8 bytes) ReviewID (4 bytes): Uniquely identifies a review, assuming any location will not have more than 2^32 reviews. ReviewText (512 bytes) Rating (1 byte): how many stars a place gets out of ten. Similarly, we can have a separate table to store photos for Places and Reviews. 5. System APIs We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the API for searching: search(api_dev_key, search_terms, user_location, radius_filter, maximum_results_to_return, category_filter, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. user_location (string): Location of the user performing the search. radius_filter (number): Optional search radius in meters. maximum_results_to_return (number): Number of business results to return. category_filter (string): Optional category to filter search results, e.g., Restaurants, Shopping Centers, etc. sort (number): Optional sort mode: Best matched (0 - default), Minimum distance (1), Highest rated (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of businesses matching the search query. Each result entry will have the business name, address, category, rating, and thumbnail. 6. Basic System Design and Algorithm At a high level, we need to store and index each dataset described above (places, reviews, etc.). For users to query this massive database, the indexing should be read efficient, since while searching for the nearby places users expect to see the results in real-time. Given that the location of a place doesn\u2019t change that often, we don\u2019t need to worry about frequent updates of the data. As a contrast, if we intend to build a service where objects do change their location frequently, e.g., people or taxis, then we might come up with a very different design. Let\u2019s see what are different ways to store this data and also find out which method will suit best for our use cases: a. SQL solution One simple solution could be to store all the data in a database like MySQL. Each place will be stored in a separate row, uniquely identified by LocationID. Each place will have its longitude and latitude stored separately in two different columns, and to perform a fast search; we should have indexes on both these fields. To find all the nearby places of a given location (X, Y) within a radius \u2018D\u2019, we can query like this: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D The above query is not completely accurate, as we know that to find the distance between two points we have to use the distance formula (Pythagorean theorem), but for simplicity let\u2019s take this. How efficient would this query be? We have estimated 500M places to be stored in our service. Since we have two separate indexes, each index can return a huge list of places and performing an intersection on those two lists won\u2019t be efficient. Another way to look at this problem is that there could be too many locations between \u2018X-D\u2019 and \u2018X+D\u2019, and similarly between \u2018Y-D\u2019 and \u2018Y+D\u2019. If we can somehow shorten these lists, it can improve the performance of our query. b. Grids We can divide the whole map into smaller grids to group locations into smaller sets. Each grid will store all the Places residing within a specific range of longitude and latitude. This scheme would enable us to query only a few grids to find nearby places. Based on a given location and radius, we can find all the neighboring grids and then query these grids to find nearby places. Let\u2019s assume that GridID (a four bytes number) would uniquely identify grids in our system. What could be a reasonable grid size? Grid size could be equal to the distance we would like to query since we also want to reduce the number of grids. If the grid size is equal to the distance we want to query, then we only need to search within the grid which contains the given location and neighboring eight grids. Since our grids would be statically defined (from the fixed grid size), we can easily find the grid number of any location (lat, long) and its neighboring grids. In the database, we can store the GridID with each location and have an index on it, too, for faster searching. Now, our query will look like: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D and GridID in (GridID, GridID1, GridID2, ..., GridID8) This will undoubtedly improve the runtime of our query. Should we keep our index in memory? Maintaining the index in memory will improve the performance of our service. We can keep our index in a hash table where \u2018key\u2019 is the grid number and \u2018value\u2019 is the list of places contained in that grid. How much memory will we need to store the index? Let\u2019s assume our search radius is 10 miles; given that the total area of the earth is around 200 million square miles, we will have 20 million grids. We would need a four bytes number to uniquely identify each grid and, since LocationID is 8 bytes, we would need 4GB of memory (ignoring hash table overhead) to store the index. (4 * 20M) + (8 * 500M) ~= 4 GB This solution can still run slow for those grids that have a lot of places since our places are not uniformly distributed among grids. We can have a thickly dense area with a lot of places, and on the other hand, we can have areas which are sparsely populated. This problem can be solved if we can dynamically adjust our grid size such that whenever we have a grid with a lot of places we break it down to create smaller grids. A couple of challenges with this approach could be: 1) how to map these grids to locations and 2) how to find all the neighboring grids of a grid. c. Dynamic size grids Let\u2019s assume we don\u2019t want to have more than 500 places in a grid so that we can have a faster searching. So, whenever a grid reaches this limit, we break it down into four grids of equal size and distribute places among them. This means thickly populated areas like downtown San Francisco will have a lot of grids, and sparsely populated area like the Pacific Ocean will have large grids with places only around the coastal lines. What data-structure can hold this information? A tree in which each node has four children can serve our purpose. Each node will represent a grid and will contain information about all the places in that grid. If a node reaches our limit of 500 places, we will break it down to create four child nodes under it and distribute places among them. In this way, all the leaf nodes will represent the grids that cannot be further broken down. So leaf nodes will keep a list of places with them. This tree structure in which each node can have four children is called a QuadTree How will we build a QuadTree? We will start with one node that will represent the whole world in one grid. Since it will have more than 500 locations, we will break it down into four nodes and distribute locations among them. We will keep repeating this process with each child node until there are no nodes left with more than 500 locations. How will we find the grid for a given location? We will start with the root node and search downward to find our required node/grid. At each step, we will see if the current node we are visiting has children. If it has, we will move to the child node that contains our desired location and repeat this process. If the node does not have any children, then that is our desired node. How will we find neighboring grids of a given grid? Since only leaf nodes contain a list of locations, we can connect all leaf nodes with a doubly linked list. This way we can iterate forward or backward among the neighboring leaf nodes to find out our desired locations. Another approach for finding adjacent grids would be through parent nodes. We can keep a pointer in each node to access its parent, and since each parent node has pointers to all of its children, we can easily find siblings of a node. We can keep expanding our search for neighboring grids by going up through the parent pointers. Once we have nearby LocationIDs, we can query the backend database to find details about those places. What will be the search workflow? We will first find the node that contains the user\u2019s location. If that node has enough desired places, we can return them to the user. If not, we will keep expanding to the neighboring nodes (either through the parent pointers or doubly linked list) until either we find the required number of places or exhaust our search based on the maximum radius. How much memory will be needed to store the QuadTree? For each Place, if we cache only LocationID and Lat/Long, we would need 12GB to store all places. 24 * 500M => 12 GB Since each grid can have a maximum of 500 places, and we have 500M locations, how many total grids we will have? 500M / 500 => 1M grids Which means we will have 1M leaf nodes and they will be holding 12GB of location data. A QuadTree with 1M leaf nodes will have approximately 1/3rd internal nodes, and each internal node will have 4 pointers (for its children). If each pointer is 8 bytes, then the memory we need to store all internal nodes would be: 1M * 1/3 * 4 * 8 = 10 MB So, total memory required to hold the whole QuadTree would be 12.01GB. This can easily fit into a modern-day server. How would we insert a new Place into our system? Whenever a new Place is added by a user, we need to insert it into the databases as well as in the QuadTree. If our tree resides on one server, it is easy to add a new Place, but if the QuadTree is distributed among different servers, first we need to find the grid/server of the new Place and then add it there (discussed in the next section). 7. Data Partitioning What if we have a huge number of places such that our index does not fit into a single machine\u2019s memory? With 20% growth each year we will reach the memory limit of the server in the future. Also, what if one server cannot serve the desired read traffic? To resolve these issues, we must partition our QuadTree! We will explore two solutions here (both of these partitioning schemes can be applied to databases, too): a. Sharding based on regions: We can divide our places into regions (like zip codes), such that all places belonging to a region will be stored on a fixed node. To store a place we will find the server through its region and, similarly, while querying for nearby places we will ask the region server that contains user\u2019s location. This approach has a couple of issues: What if a region becomes hot? There would be a lot of queries on the server holding that region, making it perform slow. This will affect the performance of our service. Over time, some regions can end up storing a lot of places compared to others. Hence, maintaining a uniform distribution of places, while regions are growing is quite difficult. To recover from these situations, either we have to repartition our data or use consistent hashing. b. Sharding based on LocationID: Our hash function will map each LocationID to a server where we will store that place. While building our QuadTree, we will iterate through all the places and calculate the hash of each LocationID to find a server where it would be stored. To find places near a location, we have to query all servers and each server will return a set of nearby places. A centralized server will aggregate these results to return them to the user. Will we have different QuadTree structure on different partitions? Yes, this can happen since it is not guaranteed that we will have an equal number of places in any given grid on all partitions. However, we do make sure that all servers have approximately an equal number of Places. This different tree structure on different servers will not cause any issue though, as we will be searching all the neighboring grids within the given radius on all partitions. The remaining part of this chapter assumes that we have partitioned our data based on LocationID. 8. Replication and Fault Tolerance Having replicas of QuadTree servers can provide an alternate to data partitioning. To distribute read traffic, we can have replicas of each QuadTree server. We can have a master-slave configuration where replicas (slaves) will only serve read traffic; all write traffic will first go to the master and then applied to slaves. Slaves might not have some recently inserted places (a few milliseconds delay will be there), but this could be acceptable. What will happen when a QuadTree server dies? We can have a secondary replica of each server and, if primary dies, it can take control after the failover. Both primary and secondary servers will have the same QuadTree structure. What if both primary and secondary servers die at the same time? We have to allocate a new server and rebuild the same QuadTree on it. How can we do that, since we don\u2019t know what places were kept on this server? The brute-force solution would be to iterate through the whole database and filter LocationIDs using our hash function to figure out all the required places that will be stored on this server. This would be inefficient and slow; also, during the time when the server is being rebuilt, we will not be able to serve any query from it, thus missing some places that should have been seen by users. How can we efficiently retrieve a mapping between Places and QuadTree server? We have to build a reverse index that will map all the Places to their QuadTree server. We can have a separate QuadTree Index server that will hold this information. We will need to build a HashMap where the \u2018key\u2019 is the QuadTree server number and the \u2018value\u2019 is a HashSet containing all the Places being kept on that QuadTree server. We need to store LocationID and Lat/Long with each place because information servers can build their QuadTrees through this. Notice that we are keeping Places\u2019 data in a HashSet, this will enable us to add/remove Places from our index quickly. So now, whenever a QuadTree server needs to rebuild itself, it can simply ask the QuadTree Index server for all the Places it needs to store. This approach will surely be quite fast. We should also have a replica of the QuadTree Index server for fault tolerance. If a QuadTree Index server dies, it can always rebuild its index from iterating through the database. 9. Cache To deal with hot Places, we can introduce a cache in front of our database. We can use an off-the-shelf solution like Memcache, which can store all data about hot places. Application servers, before hitting the backend database, can quickly check if the cache has that Place. Based on clients\u2019 usage pattern, we can adjust how many cache servers we need. For cache eviction policy, Least Recently Used (LRU) seems suitable for our system. 10. Load Balancing (LB) We can add LB layer at two places in our system 1) Between Clients and Application servers and 2) Between Application servers and Backend server. Initially, a simple Round Robin approach can be adopted; that will distribute all incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is if a server is dead the load balancer will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is, it won\u2019t take server load into consideration. If a server is overloaded or slow, the load balancer will not stop sending new requests to that server. To handle this, a more intelligent LB solution would be needed that periodically queries backend server about their load and adjusts traffic based on that. 11. Ranking How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we return most popular places within a given radius? Let\u2019s assume we keep track of the overall popularity of each place. An aggregated number can represent this popularity in our system, e.g., how many stars a place gets out of ten (this would be an average of different rankings given by users)? We will store this number in the database as well as in the QuadTree. While searching for the top 100 places within a given radius, we can ask each partition of the QuadTree to return the top 100 places with maximum popularity. Then the aggregator server can determine the top 100 places among all the places returned by different partitions. Remember that we didn\u2019t build our system to update place\u2019s data frequently. With this design, how can we modify the popularity of a place in our QuadTree? Although we can search a place and update its popularity in the QuadTree, it would take a lot of resources and can affect search requests and system throughput. Assuming the popularity of a place is not expected to reflect in the system within a few hours, we can decide to update it once or twice a day, especially when the load on the system is minimum. Our next problem, Designing Uber backend, discusses dynamic updates of the QuadTree in detail.","title":"Designing Yelp or Nearby Friends"},{"location":"DesigningYelporNearbyFriends/#designing-yelp-or-nearby-friends","text":"Let's design a Yelp like service, where users can search for nearby places like restaurants, theaters, or shopping malls, etc., and can also add/view reviews of places. Similar Services: Proximity server. Difficulty Level: Hard","title":"Designing Yelp or Nearby Friends"},{"location":"DesigningYelporNearbyFriends/#1-why-yelp-or-proximity-server","text":"Proximity servers are used to discover nearby attractions like places, events, etc. If you haven\u2019t used yelp.com before, please try it before proceeding (you can search for nearby restaurants, theaters, etc.) and spend some time understanding different options that the website offers. This will help you a lot in understanding this chapter better.","title":"1. Why Yelp or Proximity Server?"},{"location":"DesigningYelporNearbyFriends/#2-requirements-and-goals-of-the-system","text":"What do we wish to achieve from a Yelp like service? Our service will be storing information about different places so that users can perform a search on them. Upon querying, our service will return a list of places around the user. Our Yelp-like service should meet the following requirements: Functional Requirements: Users should be able to add/delete/update Places. Given their location (longitude/latitude), users should be able to find all nearby places within a given radius. Users should be able to add feedback/review about a place. The feedback can have pictures, text, and a rating. Non-functional Requirements: Users should have a real-time search experience with minimum latency. Our service should support a heavy search load. There will be a lot of search requests compared to adding a new place.","title":"2. Requirements and Goals of the System"},{"location":"DesigningYelporNearbyFriends/#3-scale-estimation","text":"Let\u2019s build our system assuming that we have 500M places and 100K queries per second (QPS). Let\u2019s also assume a 20% growth in the number of places and QPS each year.","title":"3. Scale Estimation"},{"location":"DesigningYelporNearbyFriends/#4-database-schema","text":"Each Place can have the following fields: LocationID (8 bytes): Uniquely identifies a location. Name (256 bytes) Latitude (8 bytes) Longitude (8 bytes) Description (512 bytes) Category (1 byte): E.g., coffee shop, restaurant, theater, etc. Although a four bytes number can uniquely identify 500M locations, with future growth in mind, we will go with 8 bytes for LocationID. Total size: 8 + 256 + 8 + 8 + 512 + 1 => 793 bytes We also need to store reviews, photos, and ratings of a Place. We can have a separate table to store reviews for Places: LocationID (8 bytes) ReviewID (4 bytes): Uniquely identifies a review, assuming any location will not have more than 2^32 reviews. ReviewText (512 bytes) Rating (1 byte): how many stars a place gets out of ten. Similarly, we can have a separate table to store photos for Places and Reviews.","title":"4. Database Schema"},{"location":"DesigningYelporNearbyFriends/#5-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definition of the API for searching: search(api_dev_key, search_terms, user_location, radius_filter, maximum_results_to_return, category_filter, sort, page_token) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. search_terms (string): A string containing the search terms. user_location (string): Location of the user performing the search. radius_filter (number): Optional search radius in meters. maximum_results_to_return (number): Number of business results to return. category_filter (string): Optional category to filter search results, e.g., Restaurants, Shopping Centers, etc. sort (number): Optional sort mode: Best matched (0 - default), Minimum distance (1), Highest rated (2). page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about a list of businesses matching the search query. Each result entry will have the business name, address, category, rating, and thumbnail.","title":"5. System APIs"},{"location":"DesigningYelporNearbyFriends/#6-basic-system-design-and-algorithm","text":"At a high level, we need to store and index each dataset described above (places, reviews, etc.). For users to query this massive database, the indexing should be read efficient, since while searching for the nearby places users expect to see the results in real-time. Given that the location of a place doesn\u2019t change that often, we don\u2019t need to worry about frequent updates of the data. As a contrast, if we intend to build a service where objects do change their location frequently, e.g., people or taxis, then we might come up with a very different design. Let\u2019s see what are different ways to store this data and also find out which method will suit best for our use cases: a. SQL solution One simple solution could be to store all the data in a database like MySQL. Each place will be stored in a separate row, uniquely identified by LocationID. Each place will have its longitude and latitude stored separately in two different columns, and to perform a fast search; we should have indexes on both these fields. To find all the nearby places of a given location (X, Y) within a radius \u2018D\u2019, we can query like this: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D The above query is not completely accurate, as we know that to find the distance between two points we have to use the distance formula (Pythagorean theorem), but for simplicity let\u2019s take this. How efficient would this query be? We have estimated 500M places to be stored in our service. Since we have two separate indexes, each index can return a huge list of places and performing an intersection on those two lists won\u2019t be efficient. Another way to look at this problem is that there could be too many locations between \u2018X-D\u2019 and \u2018X+D\u2019, and similarly between \u2018Y-D\u2019 and \u2018Y+D\u2019. If we can somehow shorten these lists, it can improve the performance of our query. b. Grids We can divide the whole map into smaller grids to group locations into smaller sets. Each grid will store all the Places residing within a specific range of longitude and latitude. This scheme would enable us to query only a few grids to find nearby places. Based on a given location and radius, we can find all the neighboring grids and then query these grids to find nearby places. Let\u2019s assume that GridID (a four bytes number) would uniquely identify grids in our system. What could be a reasonable grid size? Grid size could be equal to the distance we would like to query since we also want to reduce the number of grids. If the grid size is equal to the distance we want to query, then we only need to search within the grid which contains the given location and neighboring eight grids. Since our grids would be statically defined (from the fixed grid size), we can easily find the grid number of any location (lat, long) and its neighboring grids. In the database, we can store the GridID with each location and have an index on it, too, for faster searching. Now, our query will look like: Select * from Places where Latitude between X-D and X+D and Longitude between Y-D and Y+D and GridID in (GridID, GridID1, GridID2, ..., GridID8) This will undoubtedly improve the runtime of our query. Should we keep our index in memory? Maintaining the index in memory will improve the performance of our service. We can keep our index in a hash table where \u2018key\u2019 is the grid number and \u2018value\u2019 is the list of places contained in that grid. How much memory will we need to store the index? Let\u2019s assume our search radius is 10 miles; given that the total area of the earth is around 200 million square miles, we will have 20 million grids. We would need a four bytes number to uniquely identify each grid and, since LocationID is 8 bytes, we would need 4GB of memory (ignoring hash table overhead) to store the index. (4 * 20M) + (8 * 500M) ~= 4 GB This solution can still run slow for those grids that have a lot of places since our places are not uniformly distributed among grids. We can have a thickly dense area with a lot of places, and on the other hand, we can have areas which are sparsely populated. This problem can be solved if we can dynamically adjust our grid size such that whenever we have a grid with a lot of places we break it down to create smaller grids. A couple of challenges with this approach could be: 1) how to map these grids to locations and 2) how to find all the neighboring grids of a grid. c. Dynamic size grids Let\u2019s assume we don\u2019t want to have more than 500 places in a grid so that we can have a faster searching. So, whenever a grid reaches this limit, we break it down into four grids of equal size and distribute places among them. This means thickly populated areas like downtown San Francisco will have a lot of grids, and sparsely populated area like the Pacific Ocean will have large grids with places only around the coastal lines. What data-structure can hold this information? A tree in which each node has four children can serve our purpose. Each node will represent a grid and will contain information about all the places in that grid. If a node reaches our limit of 500 places, we will break it down to create four child nodes under it and distribute places among them. In this way, all the leaf nodes will represent the grids that cannot be further broken down. So leaf nodes will keep a list of places with them. This tree structure in which each node can have four children is called a QuadTree How will we build a QuadTree? We will start with one node that will represent the whole world in one grid. Since it will have more than 500 locations, we will break it down into four nodes and distribute locations among them. We will keep repeating this process with each child node until there are no nodes left with more than 500 locations. How will we find the grid for a given location? We will start with the root node and search downward to find our required node/grid. At each step, we will see if the current node we are visiting has children. If it has, we will move to the child node that contains our desired location and repeat this process. If the node does not have any children, then that is our desired node. How will we find neighboring grids of a given grid? Since only leaf nodes contain a list of locations, we can connect all leaf nodes with a doubly linked list. This way we can iterate forward or backward among the neighboring leaf nodes to find out our desired locations. Another approach for finding adjacent grids would be through parent nodes. We can keep a pointer in each node to access its parent, and since each parent node has pointers to all of its children, we can easily find siblings of a node. We can keep expanding our search for neighboring grids by going up through the parent pointers. Once we have nearby LocationIDs, we can query the backend database to find details about those places. What will be the search workflow? We will first find the node that contains the user\u2019s location. If that node has enough desired places, we can return them to the user. If not, we will keep expanding to the neighboring nodes (either through the parent pointers or doubly linked list) until either we find the required number of places or exhaust our search based on the maximum radius. How much memory will be needed to store the QuadTree? For each Place, if we cache only LocationID and Lat/Long, we would need 12GB to store all places. 24 * 500M => 12 GB Since each grid can have a maximum of 500 places, and we have 500M locations, how many total grids we will have? 500M / 500 => 1M grids Which means we will have 1M leaf nodes and they will be holding 12GB of location data. A QuadTree with 1M leaf nodes will have approximately 1/3rd internal nodes, and each internal node will have 4 pointers (for its children). If each pointer is 8 bytes, then the memory we need to store all internal nodes would be: 1M * 1/3 * 4 * 8 = 10 MB So, total memory required to hold the whole QuadTree would be 12.01GB. This can easily fit into a modern-day server. How would we insert a new Place into our system? Whenever a new Place is added by a user, we need to insert it into the databases as well as in the QuadTree. If our tree resides on one server, it is easy to add a new Place, but if the QuadTree is distributed among different servers, first we need to find the grid/server of the new Place and then add it there (discussed in the next section).","title":"6. Basic System Design and Algorithm"},{"location":"DesigningYelporNearbyFriends/#7-data-partitioning","text":"What if we have a huge number of places such that our index does not fit into a single machine\u2019s memory? With 20% growth each year we will reach the memory limit of the server in the future. Also, what if one server cannot serve the desired read traffic? To resolve these issues, we must partition our QuadTree! We will explore two solutions here (both of these partitioning schemes can be applied to databases, too): a. Sharding based on regions: We can divide our places into regions (like zip codes), such that all places belonging to a region will be stored on a fixed node. To store a place we will find the server through its region and, similarly, while querying for nearby places we will ask the region server that contains user\u2019s location. This approach has a couple of issues: What if a region becomes hot? There would be a lot of queries on the server holding that region, making it perform slow. This will affect the performance of our service. Over time, some regions can end up storing a lot of places compared to others. Hence, maintaining a uniform distribution of places, while regions are growing is quite difficult. To recover from these situations, either we have to repartition our data or use consistent hashing. b. Sharding based on LocationID: Our hash function will map each LocationID to a server where we will store that place. While building our QuadTree, we will iterate through all the places and calculate the hash of each LocationID to find a server where it would be stored. To find places near a location, we have to query all servers and each server will return a set of nearby places. A centralized server will aggregate these results to return them to the user. Will we have different QuadTree structure on different partitions? Yes, this can happen since it is not guaranteed that we will have an equal number of places in any given grid on all partitions. However, we do make sure that all servers have approximately an equal number of Places. This different tree structure on different servers will not cause any issue though, as we will be searching all the neighboring grids within the given radius on all partitions. The remaining part of this chapter assumes that we have partitioned our data based on LocationID.","title":"7. Data Partitioning"},{"location":"DesigningYelporNearbyFriends/#8-replication-and-fault-tolerance","text":"Having replicas of QuadTree servers can provide an alternate to data partitioning. To distribute read traffic, we can have replicas of each QuadTree server. We can have a master-slave configuration where replicas (slaves) will only serve read traffic; all write traffic will first go to the master and then applied to slaves. Slaves might not have some recently inserted places (a few milliseconds delay will be there), but this could be acceptable. What will happen when a QuadTree server dies? We can have a secondary replica of each server and, if primary dies, it can take control after the failover. Both primary and secondary servers will have the same QuadTree structure. What if both primary and secondary servers die at the same time? We have to allocate a new server and rebuild the same QuadTree on it. How can we do that, since we don\u2019t know what places were kept on this server? The brute-force solution would be to iterate through the whole database and filter LocationIDs using our hash function to figure out all the required places that will be stored on this server. This would be inefficient and slow; also, during the time when the server is being rebuilt, we will not be able to serve any query from it, thus missing some places that should have been seen by users. How can we efficiently retrieve a mapping between Places and QuadTree server? We have to build a reverse index that will map all the Places to their QuadTree server. We can have a separate QuadTree Index server that will hold this information. We will need to build a HashMap where the \u2018key\u2019 is the QuadTree server number and the \u2018value\u2019 is a HashSet containing all the Places being kept on that QuadTree server. We need to store LocationID and Lat/Long with each place because information servers can build their QuadTrees through this. Notice that we are keeping Places\u2019 data in a HashSet, this will enable us to add/remove Places from our index quickly. So now, whenever a QuadTree server needs to rebuild itself, it can simply ask the QuadTree Index server for all the Places it needs to store. This approach will surely be quite fast. We should also have a replica of the QuadTree Index server for fault tolerance. If a QuadTree Index server dies, it can always rebuild its index from iterating through the database.","title":"8. Replication and Fault Tolerance"},{"location":"DesigningYelporNearbyFriends/#9-cache","text":"To deal with hot Places, we can introduce a cache in front of our database. We can use an off-the-shelf solution like Memcache, which can store all data about hot places. Application servers, before hitting the backend database, can quickly check if the cache has that Place. Based on clients\u2019 usage pattern, we can adjust how many cache servers we need. For cache eviction policy, Least Recently Used (LRU) seems suitable for our system.","title":"9. Cache"},{"location":"DesigningYelporNearbyFriends/#10-load-balancing-lb","text":"We can add LB layer at two places in our system 1) Between Clients and Application servers and 2) Between Application servers and Backend server. Initially, a simple Round Robin approach can be adopted; that will distribute all incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is if a server is dead the load balancer will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is, it won\u2019t take server load into consideration. If a server is overloaded or slow, the load balancer will not stop sending new requests to that server. To handle this, a more intelligent LB solution would be needed that periodically queries backend server about their load and adjusts traffic based on that.","title":"10. Load Balancing (LB)"},{"location":"DesigningYelporNearbyFriends/#11-ranking","text":"How about if we want to rank the search results not just by proximity but also by popularity or relevance? How can we return most popular places within a given radius? Let\u2019s assume we keep track of the overall popularity of each place. An aggregated number can represent this popularity in our system, e.g., how many stars a place gets out of ten (this would be an average of different rankings given by users)? We will store this number in the database as well as in the QuadTree. While searching for the top 100 places within a given radius, we can ask each partition of the QuadTree to return the top 100 places with maximum popularity. Then the aggregator server can determine the top 100 places among all the places returned by different partitions. Remember that we didn\u2019t build our system to update place\u2019s data frequently. With this design, how can we modify the popularity of a place in our QuadTree? Although we can search a place and update its popularity in the QuadTree, it would take a lot of resources and can affect search requests and system throughput. Assuming the popularity of a place is not expected to reflect in the system within a few hours, we can decide to update it once or twice a day, especially when the load on the system is minimum. Our next problem, Designing Uber backend, discusses dynamic updates of the QuadTree in detail.","title":"11. Ranking"},{"location":"DesigningYoutubeorNetflix/","text":"Designing Youtube or Netflix Let's design a video sharing service like Youtube, where users will be able to upload/view/search videos. Similar Services: netflix.com, vimeo.com, dailymotion.com, veoh.com Difficulty Level: Medium 1. Why Youtube? Youtube is one of the most popular video sharing websites in the world. Users of the service can upload, view, share, rate, and report videos as well as add comments on videos. 2. Requirements and Goals of the System For the sake of this exercise, we plan to design a simpler version of Youtube with following requirements: Functional Requirements: Users should be able to upload videos. Users should be able to share and view videos. Users should be able to perform searches based on video titles. Our services should be able to record stats of videos, e.g., likes/dislikes, total number of views, etc. Users should be able to add and view comments on videos. Non-Functional Requirements: The system should be highly reliable, any video uploaded should not be lost. The system should be highly available. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a video for a while, it should be fine. Users should have a real time experience while watching videos and should not feel any lag. Not in scope: Video recommendations, most popular videos, channels, subscriptions, watch later, favorites, etc. 3. Capacity Estimation and Constraints Let\u2019s assume we have 1.5 billion total users, 800 million of whom are daily active users. If, on average, a user views five videos per day then the total video-views per second would be: 800M * 5 / 86400 sec => 46K videos/sec Let\u2019s assume our upload:view ratio is 1:200, i.e., for every video upload we have 200 videos viewed, giving us 230 videos uploaded per second. 46K / 200 => 230 videos/sec Storage Estimates: Let\u2019s assume that every minute 500 hours worth of videos are uploaded to Youtube. If on average, one minute of video needs 50MB of storage (videos need to be stored in multiple formats), the total storage needed for videos uploaded in a minute would be: 500 hours * 60 min * 50MB => 1500 GB/min (25 GB/sec) These numbers are estimated with ignoring video compression and replication, which would change our estimates. Bandwidth estimates: With 500 hours of video uploads per minute and assuming each video upload takes a bandwidth of 10MB/min, we would be getting 300GB of uploads every minute. 500 hours * 60 mins * 10MB => 300GB/min (5GB/sec) Assuming an upload:view ratio of 1:200, we would need 1TB/s outgoing bandwidth. 4. System APIs We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definitions of the APIs for uploading and searching videos: uploadVideo(api_dev_key, video_title, vide_description, tags[], category_id, default_language, recording_details, video_contents) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. video_title (string): Title of the video. vide_description (string): Optional description of the video. tags (string[]): Optional tags for the video. category_id (string): Category of the video, e.g., Film, Song, People, etc. default_language (string): For example English, Mandarin, Hindi, etc. recording_details (string): Location where the video was recorded. video_contents (stream): Video to be uploaded. Returns: (string) A successful upload will return HTTP 202 (request accepted) and once the video encoding is completed the user is notified through email with a link to access the video. We can also expose a queryable API to let users know the current status of their uploaded video. searchVideo(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string): The API developer key of a registered account of our service. search_query (string): A string containing the search terms. user_location (string): Optional location of the user performing the search. maximum_videos_to_return (number): Maximum number of results returned in one request. page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about the list of video resources matching the search query. Each video resource will have a video title, a thumbnail, a video creation date, and a view count. streamVideo(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string): The API developer key of a registered account of our service. video_id (string): A string to identify the video. offset (number): We should be able to stream video from any offset; this offset would be a time in seconds from the beginning of the video. If we support playing/pausing a video from multiple devices, we will need to store the offset on the server. This will enable the users to start watching a video on any device from the same point where they left off. codec (string) & resolution(string): We should send the codec and resolution info in the API from the client to support play/pause from multiple devices. Imagine you are watching a video on your TV\u2019s Netflix app, paused it, and started watching it on your phone\u2019s Netflix app. In this case, you would need codec and resolution, as both these devices have a different resolution and use a different codec. Returns: (STREAM) A media stream (a video chunk) from the given offset. 5. High Level Design At a high-level we would need the following components: Processing Queue: Each uploaded video will be pushed to a processing queue to be de-queued later for encoding, thumbnail generation, and storage. Encoder: To encode each uploaded video into multiple formats. Thumbnails generator: To generate a few thumbnails for each video. Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage. User Database: To store user\u2019s information, e.g., name, email, address, etc. Video metadata storage: A metadata database to store all the information about videos like title, file path in the system, uploading user, total views, likes, dislikes, etc. It will also be used to store all the video comments. 6. Database Schema Video metadata storage - MySql Videos metadata can be stored in a SQL database. The following information should be stored with each video: VideoID Title Description Size Thumbnail Uploader/User Total number of likes Total number of dislikes Total number of views For each video comment, we need to store following information: CommentID VideoID UserID Comment TimeOfCreation User data storage - MySql UserID, Name, email, address, age, registration details etc. 7. Detailed Component Design The service would be read-heavy, so we will focus on building a system that can retrieve videos quickly. We can expect our read:write ratio to be 200:1, which means for every video upload there are 200 video views. Where would videos be stored? Videos can be stored in a distributed file storage system like HDFS or GlusterFS. How should we efficiently manage read traffic? We should segregate our read traffic from write traffic. Since we will have multiple copies of each video, we can distribute our read traffic on different servers. For metadata, we can have master-slave configurations where writes will go to master first and then gets applied at all the slaves. Such configurations can cause some staleness in data, e.g., when a new video is added, its metadata would be inserted in the master first and before it gets applied at the slave our slaves would not be able to see it; and therefore it will be returning stale results to the user. This staleness might be acceptable in our system as it would be very short-lived and the user would be able to see the new videos after a few milliseconds. Where would thumbnails be stored? There will be a lot more thumbnails than videos. If we assume that every video will have five thumbnails, we need to have a very efficient storage system that can serve a huge read traffic. There will be two consideration before deciding which storage system should be used for thumbnails: Thumbnails are small files with, say, a maximum 5KB each. Read traffic for thumbnails will be huge compared to videos. Users will be watching one video at a time, but they might be looking at a page that has 20 thumbnails of other videos. Let\u2019s evaluate storing all the thumbnails on a disk. Given that we have a huge number of files, we have to perform a lot of seeks to different locations on the disk to read these files. This is quite inefficient and will result in higher latencies. Bigtable can be a reasonable choice here as it combines multiple files into one block to store on the disk and is very efficient in reading a small amount of data. Both of these are the two most significant requirements of our service. Keeping hot thumbnails in the cache will also help in improving the latencies and, given that thumbnails files are small in size, we can easily cache a large number of such files in memory. Video Uploads: Since videos could be huge, if while uploading the connection drops we should support resuming from the same point. Video Encoding: Newly uploaded videos are stored on the server and a new task is added to the processing queue to encode the video into multiple formats. Once all the encoding will be completed the uploader will be notified and the video is made available for view/sharing. 8. Metadata Sharding Since we have a huge number of new videos every day and our read load is extremely high, therefore, we need to distribute our data onto multiple machines so that we can perform read/write operations efficiently. We have many options to shard our data. Let\u2019s go through different strategies of sharding this data one by one: Sharding based on UserID: We can try storing all the data for a particular user on one server. While storing, we can pass the UserID to our hash function which will map the user to a database server where we will store all the metadata for that user\u2019s videos. While querying for videos of a user, we can ask our hash function to find the server holding the user\u2019s data and then read it from there. To search videos by titles we will have to query all servers and each server will return a set of videos. A centralized server will then aggregate and rank these results before returning them to the user. This approach has a couple of issues: What if a user becomes popular? There could be a lot of queries on the server holding that user; this could create a performance bottleneck. This will also affect the overall performance of our service. Over time, some users can end up storing a lot of videos compared to others. Maintaining a uniform distribution of growing user data is quite tricky. To recover from these situations either we have to repartition/redistribute our data or used consistent hashing to balance the load between servers. Sharding based on VideoID: Our hash function will map each VideoID to a random server where we will store that Video\u2019s metadata. To find videos of a user we will query all servers and each server will return a set of videos. A centralized server will aggregate and rank these results before returning them to the user. This approach solves our problem of popular users but shifts it to popular videos. We can further improve our performance by introducing a cache to store hot videos in front of the database servers. 9. Video Deduplication With a huge number of users uploading a massive amount of video data our service will have to deal with widespread video duplication. Duplicate videos often differ in aspect ratios or encodings, can contain overlays or additional borders, or can be excerpts from a longer original video. The proliferation of duplicate videos can have an impact on many levels: Data Storage: We could be wasting storage space by keeping multiple copies of the same video. Caching: Duplicate videos would result in degraded cache efficiency by taking up space that could be used for unique content. Network usage: Duplicate videos will also increase the amount of data that must be sent over the network to in-network caching systems. Energy consumption: Higher storage, inefficient cache, and network usage could result in energy wastage. For the end user, these inefficiencies will be realized in the form of duplicate search results, longer video startup times, and interrupted streaming. For our service, deduplication makes most sense early; when a user is uploading a video as compared to post-processing it to find duplicate videos later. Inline deduplication will save us a lot of resources that can be used to encode, transfer, and store the duplicate copy of the video. As soon as any user starts uploading a video, our service can run video matching algorithms (e.g., Block Matching, Phase Correlation, etc.) to find duplications. If we already have a copy of the video being uploaded, we can either stop the upload and use the existing copy or continue the upload and use the newly uploaded video if it is of higher quality. If the newly uploaded video is a subpart of an existing video or, vice versa, we can intelligently divide the video into smaller chunks so that we only upload the parts that are missing. 10. Load Balancing We should use Consistent Hashing among our cache servers, which will also help in balancing the load between cache servers. Since we will be using a static hash-based scheme to map videos to hostnames it can lead to an uneven load on the logical replicas due to the different popularity of each video. For instance, if a video becomes popular, the logical replica corresponding to that video will experience more traffic than other servers. These uneven loads for logical replicas can then translate into uneven load distribution on corresponding physical servers. To resolve this issue any busy server in one location can redirect a client to a less busy server in the same cache location. We can use dynamic HTTP redirections for this scenario. However, the use of redirections also has its drawbacks. First, since our service tries to load balance locally, it leads to multiple redirections if the host that receives the redirection can\u2019t serve the video. Also, each redirection requires a client to make an additional HTTP request; it also leads to higher delays before the video starts playing back. Moreover, inter-tier (or cross data-center) redirections lead a client to a distant cache location because the higher tier caches are only present at a small number of locations. 11. Cache To serve globally distributed users, our service needs a massive-scale video delivery system. Our service should push its content closer to the user using a large number of geographically distributed video cache servers. We need to have a strategy that will maximize user performance and also evenly distributes the load on its cache servers. We can introduce a cache for metadata servers to cache hot database rows. Using Memcache to cache the data and Application servers before hitting database can quickly check if the cache has the desired rows. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system. Under this policy, we discard the least recently viewed row first. How can we build more intelligent cache? If we go with 80-20 rule, i.e., 20% of daily read volume for videos is generating 80% of traffic, meaning that certain videos are so popular that the majority of people view them; it follows that we can try caching 20% of daily read volume of videos and metadata. 12. Content Delivery Network (CDN) A CDN is a system of distributed servers that deliver web content to a user based in the geographic locations of the user, the origin of the web page and a content delivery server. Take a look at \u2018CDN\u2019 section in our Caching chapter. Our service can move popular videos to CDNs: CDNs replicate content in multiple places. There\u2019s a better chance of videos being closer to the user and, with fewer hops, videos will stream from a friendlier network. CDN machines make heavy use of caching and can mostly serve videos out of memory. Less popular videos (1-20 views per day) that are not cached by CDNs can be served by our servers in various data centers. 13. Fault Tolerance We should use Consistent Hashing for distribution among database servers. Consistent hashing will not only help in replacing a dead server, but also help in distributing load among servers.","title":"Designing Youtube or Netflix"},{"location":"DesigningYoutubeorNetflix/#designing-youtube-or-netflix","text":"Let's design a video sharing service like Youtube, where users will be able to upload/view/search videos. Similar Services: netflix.com, vimeo.com, dailymotion.com, veoh.com Difficulty Level: Medium","title":"Designing Youtube or Netflix"},{"location":"DesigningYoutubeorNetflix/#1-why-youtube","text":"Youtube is one of the most popular video sharing websites in the world. Users of the service can upload, view, share, rate, and report videos as well as add comments on videos.","title":"1. Why Youtube?"},{"location":"DesigningYoutubeorNetflix/#2-requirements-and-goals-of-the-system","text":"For the sake of this exercise, we plan to design a simpler version of Youtube with following requirements: Functional Requirements: Users should be able to upload videos. Users should be able to share and view videos. Users should be able to perform searches based on video titles. Our services should be able to record stats of videos, e.g., likes/dislikes, total number of views, etc. Users should be able to add and view comments on videos. Non-Functional Requirements: The system should be highly reliable, any video uploaded should not be lost. The system should be highly available. Consistency can take a hit (in the interest of availability); if a user doesn\u2019t see a video for a while, it should be fine. Users should have a real time experience while watching videos and should not feel any lag. Not in scope: Video recommendations, most popular videos, channels, subscriptions, watch later, favorites, etc.","title":"2. Requirements and Goals of the System"},{"location":"DesigningYoutubeorNetflix/#3-capacity-estimation-and-constraints","text":"Let\u2019s assume we have 1.5 billion total users, 800 million of whom are daily active users. If, on average, a user views five videos per day then the total video-views per second would be: 800M * 5 / 86400 sec => 46K videos/sec Let\u2019s assume our upload:view ratio is 1:200, i.e., for every video upload we have 200 videos viewed, giving us 230 videos uploaded per second. 46K / 200 => 230 videos/sec Storage Estimates: Let\u2019s assume that every minute 500 hours worth of videos are uploaded to Youtube. If on average, one minute of video needs 50MB of storage (videos need to be stored in multiple formats), the total storage needed for videos uploaded in a minute would be: 500 hours * 60 min * 50MB => 1500 GB/min (25 GB/sec) These numbers are estimated with ignoring video compression and replication, which would change our estimates. Bandwidth estimates: With 500 hours of video uploads per minute and assuming each video upload takes a bandwidth of 10MB/min, we would be getting 300GB of uploads every minute. 500 hours * 60 mins * 10MB => 300GB/min (5GB/sec) Assuming an upload:view ratio of 1:200, we would need 1TB/s outgoing bandwidth.","title":"3. Capacity Estimation and Constraints"},{"location":"DesigningYoutubeorNetflix/#4-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. The following could be the definitions of the APIs for uploading and searching videos: uploadVideo(api_dev_key, video_title, vide_description, tags[], category_id, default_language, recording_details, video_contents) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. video_title (string): Title of the video. vide_description (string): Optional description of the video. tags (string[]): Optional tags for the video. category_id (string): Category of the video, e.g., Film, Song, People, etc. default_language (string): For example English, Mandarin, Hindi, etc. recording_details (string): Location where the video was recorded. video_contents (stream): Video to be uploaded. Returns: (string) A successful upload will return HTTP 202 (request accepted) and once the video encoding is completed the user is notified through email with a link to access the video. We can also expose a queryable API to let users know the current status of their uploaded video. searchVideo(api_dev_key, search_query, user_location, maximum_videos_to_return, page_token) Parameters: api_dev_key (string): The API developer key of a registered account of our service. search_query (string): A string containing the search terms. user_location (string): Optional location of the user performing the search. maximum_videos_to_return (number): Maximum number of results returned in one request. page_token (string): This token will specify a page in the result set that should be returned. Returns: (JSON) A JSON containing information about the list of video resources matching the search query. Each video resource will have a video title, a thumbnail, a video creation date, and a view count. streamVideo(api_dev_key, video_id, offset, codec, resolution) Parameters: api_dev_key (string): The API developer key of a registered account of our service. video_id (string): A string to identify the video. offset (number): We should be able to stream video from any offset; this offset would be a time in seconds from the beginning of the video. If we support playing/pausing a video from multiple devices, we will need to store the offset on the server. This will enable the users to start watching a video on any device from the same point where they left off. codec (string) & resolution(string): We should send the codec and resolution info in the API from the client to support play/pause from multiple devices. Imagine you are watching a video on your TV\u2019s Netflix app, paused it, and started watching it on your phone\u2019s Netflix app. In this case, you would need codec and resolution, as both these devices have a different resolution and use a different codec. Returns: (STREAM) A media stream (a video chunk) from the given offset.","title":"4. System APIs"},{"location":"DesigningYoutubeorNetflix/#5-high-level-design","text":"At a high-level we would need the following components: Processing Queue: Each uploaded video will be pushed to a processing queue to be de-queued later for encoding, thumbnail generation, and storage. Encoder: To encode each uploaded video into multiple formats. Thumbnails generator: To generate a few thumbnails for each video. Video and Thumbnail storage: To store video and thumbnail files in some distributed file storage. User Database: To store user\u2019s information, e.g., name, email, address, etc. Video metadata storage: A metadata database to store all the information about videos like title, file path in the system, uploading user, total views, likes, dislikes, etc. It will also be used to store all the video comments.","title":"5. High Level Design"},{"location":"DesigningYoutubeorNetflix/#6-database-schema","text":"Video metadata storage - MySql Videos metadata can be stored in a SQL database. The following information should be stored with each video: VideoID Title Description Size Thumbnail Uploader/User Total number of likes Total number of dislikes Total number of views For each video comment, we need to store following information: CommentID VideoID UserID Comment TimeOfCreation User data storage - MySql UserID, Name, email, address, age, registration details etc.","title":"6. Database Schema"},{"location":"DesigningYoutubeorNetflix/#7-detailed-component-design","text":"The service would be read-heavy, so we will focus on building a system that can retrieve videos quickly. We can expect our read:write ratio to be 200:1, which means for every video upload there are 200 video views. Where would videos be stored? Videos can be stored in a distributed file storage system like HDFS or GlusterFS. How should we efficiently manage read traffic? We should segregate our read traffic from write traffic. Since we will have multiple copies of each video, we can distribute our read traffic on different servers. For metadata, we can have master-slave configurations where writes will go to master first and then gets applied at all the slaves. Such configurations can cause some staleness in data, e.g., when a new video is added, its metadata would be inserted in the master first and before it gets applied at the slave our slaves would not be able to see it; and therefore it will be returning stale results to the user. This staleness might be acceptable in our system as it would be very short-lived and the user would be able to see the new videos after a few milliseconds. Where would thumbnails be stored? There will be a lot more thumbnails than videos. If we assume that every video will have five thumbnails, we need to have a very efficient storage system that can serve a huge read traffic. There will be two consideration before deciding which storage system should be used for thumbnails: Thumbnails are small files with, say, a maximum 5KB each. Read traffic for thumbnails will be huge compared to videos. Users will be watching one video at a time, but they might be looking at a page that has 20 thumbnails of other videos. Let\u2019s evaluate storing all the thumbnails on a disk. Given that we have a huge number of files, we have to perform a lot of seeks to different locations on the disk to read these files. This is quite inefficient and will result in higher latencies. Bigtable can be a reasonable choice here as it combines multiple files into one block to store on the disk and is very efficient in reading a small amount of data. Both of these are the two most significant requirements of our service. Keeping hot thumbnails in the cache will also help in improving the latencies and, given that thumbnails files are small in size, we can easily cache a large number of such files in memory. Video Uploads: Since videos could be huge, if while uploading the connection drops we should support resuming from the same point. Video Encoding: Newly uploaded videos are stored on the server and a new task is added to the processing queue to encode the video into multiple formats. Once all the encoding will be completed the uploader will be notified and the video is made available for view/sharing.","title":"7. Detailed Component Design"},{"location":"DesigningYoutubeorNetflix/#8-metadata-sharding","text":"Since we have a huge number of new videos every day and our read load is extremely high, therefore, we need to distribute our data onto multiple machines so that we can perform read/write operations efficiently. We have many options to shard our data. Let\u2019s go through different strategies of sharding this data one by one: Sharding based on UserID: We can try storing all the data for a particular user on one server. While storing, we can pass the UserID to our hash function which will map the user to a database server where we will store all the metadata for that user\u2019s videos. While querying for videos of a user, we can ask our hash function to find the server holding the user\u2019s data and then read it from there. To search videos by titles we will have to query all servers and each server will return a set of videos. A centralized server will then aggregate and rank these results before returning them to the user. This approach has a couple of issues: What if a user becomes popular? There could be a lot of queries on the server holding that user; this could create a performance bottleneck. This will also affect the overall performance of our service. Over time, some users can end up storing a lot of videos compared to others. Maintaining a uniform distribution of growing user data is quite tricky. To recover from these situations either we have to repartition/redistribute our data or used consistent hashing to balance the load between servers. Sharding based on VideoID: Our hash function will map each VideoID to a random server where we will store that Video\u2019s metadata. To find videos of a user we will query all servers and each server will return a set of videos. A centralized server will aggregate and rank these results before returning them to the user. This approach solves our problem of popular users but shifts it to popular videos. We can further improve our performance by introducing a cache to store hot videos in front of the database servers.","title":"8. Metadata Sharding"},{"location":"DesigningYoutubeorNetflix/#9-video-deduplication","text":"With a huge number of users uploading a massive amount of video data our service will have to deal with widespread video duplication. Duplicate videos often differ in aspect ratios or encodings, can contain overlays or additional borders, or can be excerpts from a longer original video. The proliferation of duplicate videos can have an impact on many levels: Data Storage: We could be wasting storage space by keeping multiple copies of the same video. Caching: Duplicate videos would result in degraded cache efficiency by taking up space that could be used for unique content. Network usage: Duplicate videos will also increase the amount of data that must be sent over the network to in-network caching systems. Energy consumption: Higher storage, inefficient cache, and network usage could result in energy wastage. For the end user, these inefficiencies will be realized in the form of duplicate search results, longer video startup times, and interrupted streaming. For our service, deduplication makes most sense early; when a user is uploading a video as compared to post-processing it to find duplicate videos later. Inline deduplication will save us a lot of resources that can be used to encode, transfer, and store the duplicate copy of the video. As soon as any user starts uploading a video, our service can run video matching algorithms (e.g., Block Matching, Phase Correlation, etc.) to find duplications. If we already have a copy of the video being uploaded, we can either stop the upload and use the existing copy or continue the upload and use the newly uploaded video if it is of higher quality. If the newly uploaded video is a subpart of an existing video or, vice versa, we can intelligently divide the video into smaller chunks so that we only upload the parts that are missing.","title":"9. Video Deduplication"},{"location":"DesigningYoutubeorNetflix/#10-load-balancing","text":"We should use Consistent Hashing among our cache servers, which will also help in balancing the load between cache servers. Since we will be using a static hash-based scheme to map videos to hostnames it can lead to an uneven load on the logical replicas due to the different popularity of each video. For instance, if a video becomes popular, the logical replica corresponding to that video will experience more traffic than other servers. These uneven loads for logical replicas can then translate into uneven load distribution on corresponding physical servers. To resolve this issue any busy server in one location can redirect a client to a less busy server in the same cache location. We can use dynamic HTTP redirections for this scenario. However, the use of redirections also has its drawbacks. First, since our service tries to load balance locally, it leads to multiple redirections if the host that receives the redirection can\u2019t serve the video. Also, each redirection requires a client to make an additional HTTP request; it also leads to higher delays before the video starts playing back. Moreover, inter-tier (or cross data-center) redirections lead a client to a distant cache location because the higher tier caches are only present at a small number of locations.","title":"10. Load Balancing"},{"location":"DesigningYoutubeorNetflix/#11-cache","text":"To serve globally distributed users, our service needs a massive-scale video delivery system. Our service should push its content closer to the user using a large number of geographically distributed video cache servers. We need to have a strategy that will maximize user performance and also evenly distributes the load on its cache servers. We can introduce a cache for metadata servers to cache hot database rows. Using Memcache to cache the data and Application servers before hitting database can quickly check if the cache has the desired rows. Least Recently Used (LRU) can be a reasonable cache eviction policy for our system. Under this policy, we discard the least recently viewed row first. How can we build more intelligent cache? If we go with 80-20 rule, i.e., 20% of daily read volume for videos is generating 80% of traffic, meaning that certain videos are so popular that the majority of people view them; it follows that we can try caching 20% of daily read volume of videos and metadata.","title":"11. Cache"},{"location":"DesigningYoutubeorNetflix/#12-content-delivery-network-cdn","text":"A CDN is a system of distributed servers that deliver web content to a user based in the geographic locations of the user, the origin of the web page and a content delivery server. Take a look at \u2018CDN\u2019 section in our Caching chapter. Our service can move popular videos to CDNs: CDNs replicate content in multiple places. There\u2019s a better chance of videos being closer to the user and, with fewer hops, videos will stream from a friendlier network. CDN machines make heavy use of caching and can mostly serve videos out of memory. Less popular videos (1-20 views per day) that are not cached by CDNs can be served by our servers in various data centers.","title":"12. Content Delivery Network (CDN)"},{"location":"DesigningYoutubeorNetflix/#13-fault-tolerance","text":"We should use Consistent Hashing for distribution among database servers. Consistent hashing will not only help in replacing a dead server, but also help in distributing load among servers.","title":"13. Fault Tolerance"},{"location":"Events/","text":"WebSockets vs. Server-Sent Events vs. Long-Polling **Can you explain the differences between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are some of the most popular communication protocols between a client and a web server. Let's start with a basic understanding of how a regular HTTP web request looks. The sequence of events for a standard HTTP request is as follows: The client establishes a connection with the server and requests data. The response is calculated by the server. On the opened request, the server provides the response back to the client. Polling with Ajax The great majority of AJAX apps employ polling as a common strategy. The basic concept is that the client polls (or requests) data from the server on a regular basis. The client submits a request and awaits a response from the server. An empty response is returned if no data is available. The client establishes a connection with the server and sends a standard HTTP request for data. The requested webpage sends periodic queries to the server (e.g., 0.5 seconds). Just like regular HTTP traffic, the server calculates the response and sends it back. To acquire updates from the server, the client performs the first three steps on a regular basis. Polling has the drawback of requiring the client to continually requesting the server for new data. As a result, many answers are blank, resulting in HTTP overhead. Long-Polling HTTP This is a version of traditional polling that allows the server to push data to a client whenever it is available. Long-Polling involves the client requesting information from the server in the same way that standard polling does, but with the caveat that the server may not respond right away. This is why this approach is also known as a \"Hanging GET.\" Instead of returning an empty answer if the server does not have any data for the client, the server holds the request and waits for data to become available. A full response is delivered to the client once the data is accessible. The client then instantly re-requests information from the server, ensuring that the server has a waiting request ready to give data in response to an event virtually all of the time. The following is the basic life cycle of an application that uses HTTP Long-Polling: The client issues a standard HTTP request and then waits for a response. The server waits until an update is available or a timeout occurs before responding. When an update is available, the server sends the client a complete response. After getting a response, the client normally sends a new long-poll request, either immediately or after a delay to provide for an acceptable latency duration. There is a timeout on each Long-Poll request. After a connection is lost owing to timeouts, the client must rejoin on a regular basis. WebSockets Over a single TCP connection, WebSocket delivers full duplex communication channels. It establishes a permanent link between a client and a server, allowing both parties to begin transferring data at any moment. The WebSocket handshake is the process by which the client establishes a WebSocket connection. If the operation is successful, the server and client can freely communicate data in both ways. The WebSocket protocol allows for low-latency communication between a client and a server, allowing for real-time data flow from and to the server. This is accomplished by providing a standardized method for the server to deliver content to the browser without the client's permission, as well as allowing messages to be passed back and forth while the connection is open. A two-way (bi-directional) continuing dialogue between a client and a server can be established in this manner. Server-Delivered Events (SSEs) SSEs allow the client to establish a long-term and persistent connection with the server. This connection is used by the server to send data to a client. If the client wants to communicate data to the server, it will have to do so using a different technology/protocol. A client uses HTTP to request data from a server. The requesting webpage establishes a server connection. When new information becomes available, the server provides it to the client. When real-time transmission from the server to the client is required, or if the server generates data in a loop and will be transmitting several events to the client, SSEs are the ideal choice.","title":"Long-Polling vs WebSockets vs Server-Sent Events"},{"location":"Events/#websockets-vs-server-sent-events-vs-long-polling","text":"**Can you explain the differences between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are some of the most popular communication protocols between a client and a web server. Let's start with a basic understanding of how a regular HTTP web request looks. The sequence of events for a standard HTTP request is as follows: The client establishes a connection with the server and requests data. The response is calculated by the server. On the opened request, the server provides the response back to the client.","title":"WebSockets vs. Server-Sent Events vs. Long-Polling"},{"location":"Events/#polling-with-ajax","text":"The great majority of AJAX apps employ polling as a common strategy. The basic concept is that the client polls (or requests) data from the server on a regular basis. The client submits a request and awaits a response from the server. An empty response is returned if no data is available. The client establishes a connection with the server and sends a standard HTTP request for data. The requested webpage sends periodic queries to the server (e.g., 0.5 seconds). Just like regular HTTP traffic, the server calculates the response and sends it back. To acquire updates from the server, the client performs the first three steps on a regular basis. Polling has the drawback of requiring the client to continually requesting the server for new data. As a result, many answers are blank, resulting in HTTP overhead.","title":"Polling with Ajax"},{"location":"Events/#long-polling-http","text":"This is a version of traditional polling that allows the server to push data to a client whenever it is available. Long-Polling involves the client requesting information from the server in the same way that standard polling does, but with the caveat that the server may not respond right away. This is why this approach is also known as a \"Hanging GET.\" Instead of returning an empty answer if the server does not have any data for the client, the server holds the request and waits for data to become available. A full response is delivered to the client once the data is accessible. The client then instantly re-requests information from the server, ensuring that the server has a waiting request ready to give data in response to an event virtually all of the time. The following is the basic life cycle of an application that uses HTTP Long-Polling: The client issues a standard HTTP request and then waits for a response. The server waits until an update is available or a timeout occurs before responding. When an update is available, the server sends the client a complete response. After getting a response, the client normally sends a new long-poll request, either immediately or after a delay to provide for an acceptable latency duration. There is a timeout on each Long-Poll request. After a connection is lost owing to timeouts, the client must rejoin on a regular basis.","title":"Long-Polling HTTP"},{"location":"Events/#websockets","text":"Over a single TCP connection, WebSocket delivers full duplex communication channels. It establishes a permanent link between a client and a server, allowing both parties to begin transferring data at any moment. The WebSocket handshake is the process by which the client establishes a WebSocket connection. If the operation is successful, the server and client can freely communicate data in both ways. The WebSocket protocol allows for low-latency communication between a client and a server, allowing for real-time data flow from and to the server. This is accomplished by providing a standardized method for the server to deliver content to the browser without the client's permission, as well as allowing messages to be passed back and forth while the connection is open. A two-way (bi-directional) continuing dialogue between a client and a server can be established in this manner.","title":"WebSockets"},{"location":"Events/#server-delivered-events-sses","text":"SSEs allow the client to establish a long-term and persistent connection with the server. This connection is used by the server to send data to a client. If the client wants to communicate data to the server, it will have to do so using a different technology/protocol. A client uses HTTP to request data from a server. The requesting webpage establishes a server connection. When new information becomes available, the server provides it to the client. When real-time transmission from the server to the client is required, or if the server generates data in a loop and will be transmitting several events to the client, SSEs are the ideal choice.","title":"Server-Delivered Events (SSEs)"},{"location":"Indexes/","text":"Indexes When it comes to databases, indexes are well-known. Database performance will eventually degrade to the point where it is no longer acceptable. When this happens, database indexing is one of the first things you should do. The purpose of setting an index on a table in a database is to make it easier to search through the table and discover the row or rows we're looking for. Indexes can be built utilizing one or more columns from a database table, allowing for quick random lookups as well as efficient access to ordered items. An example would be a library catalog. A library catalog is a list of books found in a library that is kept in a register. The catalog is laid out in the format of a database table, with four columns: book title, author, subject, and publication date. Typically, there are two such catalogs: one ordered by book title and the other sorted by author name. That way, you may either think of a writer you'd like to read and then browse their books, or hunt up a specific book title you'd like to read if you don't know the author's name. These catalogs function as indexes for the book database. They give you a sorted set of data that you can search for pertinent information in. Simply said, an index is a data structure that functions similarly to a table of contents in that it directs us to the real data. When we establish an index on a table column, we store the column as well as a pointer to the entire row in the index. Assuming you have a table with a list of books, the diagram below depicts how an index on the 'Title' column would look: This notion can be used to larger datasets in the same way that a standard relational data store can. When it comes to indexes, the trick is to think about how users will access the data. Indexes are essential for maximizing data access when data sets are many terabytes in size but have very little payloads (e.g., 1 KB). Finding a tiny payload in such a vast dataset can be difficult because we can't iterate over all of it in an acceptable amount of time. Furthermore, such a big data collection is extremely likely to be dispersed among numerous physical devices, necessitating the development of a method for locating the correct physical location of the needed data. The best way to do this is to use indexes. How do Indexes decrease write performance? An index can drastically speed up data retrieval, but because of the additional keys, it can be rather huge, slowing down data input and update. When adding rows to a table with an active index or updating existing rows, we must not only write the data but also update the index. The writing performance will suffer as a result of this. All insert, update, and delete actions for the table suffer from this speed decrease. As a result, adding unnecessary indexes to tables should be avoided, and obsolete indexes should be eliminated. To be clear, increasing indexes is all about enhancing the speed of search queries. If the purpose of the database is to offer a data store that is frequently written to but seldom read from, then slowing down the more common process, writing, is probably not worth the performance boost we obtain from reading. See this page for further information. Database Indexes","title":"Indexes"},{"location":"Indexes/#indexes","text":"When it comes to databases, indexes are well-known. Database performance will eventually degrade to the point where it is no longer acceptable. When this happens, database indexing is one of the first things you should do. The purpose of setting an index on a table in a database is to make it easier to search through the table and discover the row or rows we're looking for. Indexes can be built utilizing one or more columns from a database table, allowing for quick random lookups as well as efficient access to ordered items. An example would be a library catalog. A library catalog is a list of books found in a library that is kept in a register. The catalog is laid out in the format of a database table, with four columns: book title, author, subject, and publication date. Typically, there are two such catalogs: one ordered by book title and the other sorted by author name. That way, you may either think of a writer you'd like to read and then browse their books, or hunt up a specific book title you'd like to read if you don't know the author's name. These catalogs function as indexes for the book database. They give you a sorted set of data that you can search for pertinent information in. Simply said, an index is a data structure that functions similarly to a table of contents in that it directs us to the real data. When we establish an index on a table column, we store the column as well as a pointer to the entire row in the index. Assuming you have a table with a list of books, the diagram below depicts how an index on the 'Title' column would look: This notion can be used to larger datasets in the same way that a standard relational data store can. When it comes to indexes, the trick is to think about how users will access the data. Indexes are essential for maximizing data access when data sets are many terabytes in size but have very little payloads (e.g., 1 KB). Finding a tiny payload in such a vast dataset can be difficult because we can't iterate over all of it in an acceptable amount of time. Furthermore, such a big data collection is extremely likely to be dispersed among numerous physical devices, necessitating the development of a method for locating the correct physical location of the needed data. The best way to do this is to use indexes.","title":"Indexes"},{"location":"Indexes/#how-do-indexes-decrease-write-performance","text":"An index can drastically speed up data retrieval, but because of the additional keys, it can be rather huge, slowing down data input and update. When adding rows to a table with an active index or updating existing rows, we must not only write the data but also update the index. The writing performance will suffer as a result of this. All insert, update, and delete actions for the table suffer from this speed decrease. As a result, adding unnecessary indexes to tables should be avoided, and obsolete indexes should be eliminated. To be clear, increasing indexes is all about enhancing the speed of search queries. If the purpose of the database is to offer a data store that is frequently written to but seldom read from, then slowing down the more common process, writing, is probably not worth the performance boost we obtain from reading. See this page for further information. Database Indexes","title":"How do Indexes decrease write performance?"},{"location":"KeyCharDistributedSystem/","text":"Key Characteristics of Distributed Systems Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them: Scalability Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly. Horizontal vs. Vertical Scaling: Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#key-characteristics-of-distributed-systems","text":"Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them:","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#scalability","text":"Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.","title":"Scalability"},{"location":"KeyCharDistributedSystem/#horizontal-vs-vertical-scaling","text":"Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Horizontal vs. Vertical Scaling:"},{"location":"LoadBalancing/","text":"Load Balancing A load balancer sits in front of your server and acts as a \"traffic cop,\" directing client requests across all servers. It aids in the distribution of traffic among a cluster of servers in order to improve application, website, or database responsiveness and availability. While distributing requests, LB also maintains track of the status of all the resources. A load balancer can be a real device, a virtualized instance, or a software process that runs on specialized hardware. Load Balancer will stop transmitting traffic to a server if it is not available to accept new requests, is not responding, or has a high error rate. We can strive to balance the load at each tier of the system to achieve full scalability and redundancy. Load balancers can be added in three places: Between the web server and the user Between web servers and a platform layer within the company, such as application servers or cache servers Between the database and the internal platform layer. Load Balancing Algorithms There are several load balancing methods available, each of which employs a different algorithm to meet the needs of the user. 1. Least Connection Method : Traffic is sent to the server with the fewest active connections using this strategy. When there are a significant number of persistent client connections that are distributed unevenly throughout the servers, this strategy is quite useful. 2. Least Response Time Method : Traffic is routed to the server with the fewest active connections and the fastest average response time. 3. Least Bandwidth Method : This approach chooses the server with the least amount of traffic (measured in megabits per second) at the time (Mbps). 4. Round Robin Method : This approach sends each new request to the next server after cycling through a list of servers. It restarts from the beginning when it reaches the end of the list. It's most useful when the servers are of similar specs and there aren't a lot of permanent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is intended to accommodate servers with varying processing capacities more effectively. A weight is allocated to each server (an integer value that indicates the processing capacity). Higher-weighted servers receive new connections before lower-weighted servers, and higher-weighted servers receive more connections than lower-weighted servers. 6. IP Hash : To redirect the request to a server, this approach calculates a hash of the client's IP address. Redundant Load Balancers The load balancer can be a single point of failure; to avoid this, link a second load balancer to the first to form a cluster. Each load balancer keeps an eye on the other's health, and since they're both capable of providing traffic and detecting failures, if the main load balancer fails, the second load balancer takes over. Benefits of Load Balancing Users benefit from faster and more consistent service. Users will not have to wait for a single underperforming server to complete its earlier responsibilities. Instead, their queries are forwarded to a more readily available resource right away. There is reduced downtime and higher throughput for service providers. Even a complete server failure will have no effect on the end user experience because the load balancer will simply redirect traffic to a healthy server. Load balancing makes it easier for system administrators to deal with incoming requests while reducing the amount of time users have to wait. Predictive analytics, which detects traffic bottlenecks before they occur, is one of the advantages of smart load balancers. As a result, the smart load balancer provides actionable data to a business. These are essential for automation and can aid in commercial decision-making. There are fewer defective or strained components for system managers to deal with. Instead of a single device doing a lot of work, load balancing distributes the workload among numerous devices. The following links contain useful information regarding load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing","text":"A load balancer sits in front of your server and acts as a \"traffic cop,\" directing client requests across all servers. It aids in the distribution of traffic among a cluster of servers in order to improve application, website, or database responsiveness and availability. While distributing requests, LB also maintains track of the status of all the resources. A load balancer can be a real device, a virtualized instance, or a software process that runs on specialized hardware. Load Balancer will stop transmitting traffic to a server if it is not available to accept new requests, is not responding, or has a high error rate. We can strive to balance the load at each tier of the system to achieve full scalability and redundancy. Load balancers can be added in three places: Between the web server and the user Between web servers and a platform layer within the company, such as application servers or cache servers Between the database and the internal platform layer.","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing-algorithms","text":"There are several load balancing methods available, each of which employs a different algorithm to meet the needs of the user. 1. Least Connection Method : Traffic is sent to the server with the fewest active connections using this strategy. When there are a significant number of persistent client connections that are distributed unevenly throughout the servers, this strategy is quite useful. 2. Least Response Time Method : Traffic is routed to the server with the fewest active connections and the fastest average response time. 3. Least Bandwidth Method : This approach chooses the server with the least amount of traffic (measured in megabits per second) at the time (Mbps). 4. Round Robin Method : This approach sends each new request to the next server after cycling through a list of servers. It restarts from the beginning when it reaches the end of the list. It's most useful when the servers are of similar specs and there aren't a lot of permanent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is intended to accommodate servers with varying processing capacities more effectively. A weight is allocated to each server (an integer value that indicates the processing capacity). Higher-weighted servers receive new connections before lower-weighted servers, and higher-weighted servers receive more connections than lower-weighted servers. 6. IP Hash : To redirect the request to a server, this approach calculates a hash of the client's IP address.","title":"Load Balancing Algorithms"},{"location":"LoadBalancing/#redundant-load-balancers","text":"The load balancer can be a single point of failure; to avoid this, link a second load balancer to the first to form a cluster. Each load balancer keeps an eye on the other's health, and since they're both capable of providing traffic and detecting failures, if the main load balancer fails, the second load balancer takes over.","title":"Redundant Load Balancers"},{"location":"LoadBalancing/#benefits-of-load-balancing","text":"Users benefit from faster and more consistent service. Users will not have to wait for a single underperforming server to complete its earlier responsibilities. Instead, their queries are forwarded to a more readily available resource right away. There is reduced downtime and higher throughput for service providers. Even a complete server failure will have no effect on the end user experience because the load balancer will simply redirect traffic to a healthy server. Load balancing makes it easier for system administrators to deal with incoming requests while reducing the amount of time users have to wait. Predictive analytics, which detects traffic bottlenecks before they occur, is one of the advantages of smart load balancers. As a result, the smart load balancer provides actionable data to a business. These are essential for automation and can aid in commercial decision-making. There are fewer defective or strained components for system managers to deal with. Instead of a single device doing a lot of work, load balancing distributes the workload among numerous devices. The following links contain useful information regarding load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Benefits of Load Balancing"},{"location":"PraticeDesignProblems/","text":"Pratice on full Screen","title":"Pratice Design Problems"},{"location":"PreRequisites/","text":"Pre Requisites In this part, we'll presume that you: Have worked with a relational database before ( like MySQL ). Have a basic understanding of NoSQL databases. Know the fundamentals of the following: 1. Concurrency : Are you familiar with the terms threads, deadlock, and starvation? When numerous processes / threads attempt to edit the same data, what happens? A fundamental grasp of read and write locks is required. 2. Networking : Do you have a basic understanding of networking protocols such as TCP and UDP? Do you know what switches and routers are for? 3. File systems : You should have a good understanding of the systems you're working with. Do you have a basic understanding of how an operating system, file system, and database work? Do you understand the several levels of caching in a modern operating system?","title":"Pre Requisites"},{"location":"PreRequisites/#pre-requisites","text":"In this part, we'll presume that you: Have worked with a relational database before ( like MySQL ). Have a basic understanding of NoSQL databases. Know the fundamentals of the following: 1. Concurrency : Are you familiar with the terms threads, deadlock, and starvation? When numerous processes / threads attempt to edit the same data, what happens? A fundamental grasp of read and write locks is required. 2. Networking : Do you have a basic understanding of networking protocols such as TCP and UDP? Do you know what switches and routers are for? 3. File systems : You should have a good understanding of the systems you're working with. Do you have a basic understanding of how an operating system, file system, and database work? Do you understand the several levels of caching in a modern operating system?","title":"Pre Requisites"},{"location":"Proxies/","text":"Proxy Servers A proxy server is a server that sits in the middle of the client and the back-end server. Clients connect to proxy servers to request services such as a web page, a file, a connection, and so on. In a nutshell, a proxy server is a piece of software or hardware that acts as a middleman between clients and other servers when they request resources. Proxy servers are commonly used to filter requests, log requests, and occasionally alter requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another benefit of a proxy server is that it can handle a large number of requests in its cache. If a resource is accessed by numerous clients, the proxy server can cache it and serve it to all clients without having to go to the remote server. Types of Proxy Servers Proxies can be found on the client's local server or between the client and remote servers. Here are a few well-known proxy server types: Open Proxy An open proxy is a proxy server that anyone on the Internet can visit. In general, a proxy server only permits users within a network group (i.e., a closed proxy) to store and forward Internet services like DNS or web pages in order to reduce and regulate the group's bandwidth usage. An open proxy, on the other hand, allows any Internet user to use this forwarding service. There are two well-known open proxy types: Anonymous Proxy - This proxy identifies itself as a server but does not reveal the client's real IP address. Though this proxy server is easily detectable, it can be beneficial to some users because it hides their IP address. Tr\u0430nsp\u0430rent Proxy \u2013 tr\u0430nsp\u0430rent Proxy \u2013 This proxy server identifies itself, and the first IP address can be viewed with the use of HTTP headers. The main advantage of using this type of server is its ability to cache web pages. Reverse Proxy A reverse proxy retrieves resources from one or more servers on behalf of a client. These resources are then returned to the client, making it look as if they came from the proxy server. References Open Proxy Reverse Proxy","title":"Proxies"},{"location":"Proxies/#proxy-servers","text":"A proxy server is a server that sits in the middle of the client and the back-end server. Clients connect to proxy servers to request services such as a web page, a file, a connection, and so on. In a nutshell, a proxy server is a piece of software or hardware that acts as a middleman between clients and other servers when they request resources. Proxy servers are commonly used to filter requests, log requests, and occasionally alter requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another benefit of a proxy server is that it can handle a large number of requests in its cache. If a resource is accessed by numerous clients, the proxy server can cache it and serve it to all clients without having to go to the remote server.","title":"Proxy Servers"},{"location":"Proxies/#types-of-proxy-servers","text":"Proxies can be found on the client's local server or between the client and remote servers. Here are a few well-known proxy server types:","title":"Types of Proxy Servers"},{"location":"Proxies/#open-proxy","text":"An open proxy is a proxy server that anyone on the Internet can visit. In general, a proxy server only permits users within a network group (i.e., a closed proxy) to store and forward Internet services like DNS or web pages in order to reduce and regulate the group's bandwidth usage. An open proxy, on the other hand, allows any Internet user to use this forwarding service. There are two well-known open proxy types: Anonymous Proxy - This proxy identifies itself as a server but does not reveal the client's real IP address. Though this proxy server is easily detectable, it can be beneficial to some users because it hides their IP address. Tr\u0430nsp\u0430rent Proxy \u2013 tr\u0430nsp\u0430rent Proxy \u2013 This proxy server identifies itself, and the first IP address can be viewed with the use of HTTP headers. The main advantage of using this type of server is its ability to cache web pages.","title":"Open Proxy"},{"location":"Proxies/#reverse-proxy","text":"A reverse proxy retrieves resources from one or more servers on behalf of a client. These resources are then returned to the client, making it look as if they came from the proxy server. References Open Proxy Reverse Proxy","title":"Reverse Proxy"},{"location":"RedundancyReplication/","text":"Replication and Redundancy Redundancy is the duplication of important components or functions in a system with the goal of boosting the system's reliability, usually as a backup or fail-safe, or improving real system performance. If a single copy of a file is stored on a single server, for example, losing that server means losing the file. Because losing data is rarely a good thing, we can remedy this problem by making duplicate or redundant copies of the file. Redundancy is important because it eliminates single points of failure in the system and offers backups in the event of a disaster. For instance, if one of two service instances running in production fails, the system can failover to the other. Replication is the process of exchanging data among redundant resources, such as software or hardware components, in order to improve dependability, fault tolerance, or accessibility. Many database management systems (DBMS) use replication, which is commonly done using a master-slave connection between the original and the copies. The master receives all updates, which are then sent to the slaves. Each slave sends a message indicating that the update was properly received, enabling for the delivery of following updates. References Replication Redundancy Fault Tolerance","title":"Redundancy and Replication"},{"location":"RedundancyReplication/#replication-and-redundancy","text":"Redundancy is the duplication of important components or functions in a system with the goal of boosting the system's reliability, usually as a backup or fail-safe, or improving real system performance. If a single copy of a file is stored on a single server, for example, losing that server means losing the file. Because losing data is rarely a good thing, we can remedy this problem by making duplicate or redundant copies of the file. Redundancy is important because it eliminates single points of failure in the system and offers backups in the event of a disaster. For instance, if one of two service instances running in production fails, the system can failover to the other. Replication is the process of exchanging data among redundant resources, such as software or hardware components, in order to improve dependability, fault tolerance, or accessibility. Many database management systems (DBMS) use replication, which is commonly done using a master-slave connection between the original and the copies. The master receives all updates, which are then sent to the slaves. Each slave sends a message indicating that the update was properly received, enabling for the delivery of following updates. References Replication Redundancy Fault Tolerance","title":"Replication and Redundancy"},{"location":"SQLvsNoSQL/","text":"SQL vs. NoSQL There are two sorts of database solutions in the world: SQL and NoSQL (or relational databases and non-relational databases). The manner they were designed, the type of information they hold, and the storage mechanism they use are all different. Relational databases, like phone books that record phone numbers and addresses, are structured and have predetermined schemas. Non-relational databases, like file folders, are unstructured, distributed, and feature a dynamic schema that stores everything from a person's address and phone number to their Facebook \"likes\" and online shopping preferences. SQL Data is stored in rows and columns in relational databases. Each row holds all of the data for a single entity, whereas each column has all of the individual data points. MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB are some of the most popular relational databases. NoSQL The most prevalent forms of NoSQL are as follows: Key-Value Stores: Data is kept as a key-value pair array. A 'key' is a term for an attribute that is associated to a 'value.' Redis, Voldemort, and Dynamo are examples of well-known key-value stores. Document Databases: Data is kept in documents (rather than rows and columns in a table) in these databases, and these documents are grouped together in collections. The structure of any document can be completely different. CouchDB and MongoDB are two document databases. Wide-Column Databases: Column families, rather than 'tables,' are containers for rows in columnar databases. We don't need to know all the columns up front, and each row doesn't have to have the same number of columns as a relational database. Columnar databases, such as Cassandra and HBase, are ideally suited for analyzing massive datasets. Graph Databases: These databases are used to hold information that can best be represented as a graph. Nodes (entities), attributes (information about the entities), and lines are used to store data in graph topologies (connections between the entities). Neo4J and Infinite Graph are two examples of graph databases. Differences in SQL vs NoSQL on a high level Storage: SQL saves data in tables, with each row representing an entity and each column representing a data point about that entity; for example, if we're keeping a car entity in a table, distinct columns might be 'Color, Make, Model,' and so on. The data storage models used by NoSQL databases vary. Key-value, document, graph, and columnar are the most common types. Below, we'll compare and contrast these two databases. Schema: Each record in SQL must adhere to a fixed schema, which means that the columns must be determined and chosen prior to data entry, and each row must have data for each field. Later, the schema can be changed, but it will require updating the entire database and going offline. Schemas in NoSQL are dynamic. Columns can be added at any time, and each 'row' (or equivalent) does not need to include data for each 'column.' Querying: SQL (structured query language) is a powerful language for defining and manipulating data in SQL databases. Queries in a NoSQL database are focused on a set of documents. UnQL is another name for it (Unstructured Query Language). The syntax for using UnQL varies depending on the database. Scalability: In most cases, SQL databases are vertically scalable, meaning that they may be scaled up by increasing the hardware's horsepower (memory, CPU, etc.), which can be quite costly. A relational database can be scaled across numerous servers, but it is a difficult and time-consuming operation. NoSQL databases, on the other hand, are horizontally scalable, which means we can quickly add more servers to our NoSQL database infrastructure to manage a large amount of traffic. NoSQL databases may be hosted on any cheap commodity hardware or cloud instances, making it much more cost-effective than vertical scaling. Many NoSQL technologies also automatically spread data across servers. Atomicity, Consistency, Isolation, and Durability (ACID) Compliance: ACID compliance is seen in the vast majority of relational databases. So, when it comes to data consistency and transaction security, SQL databases are still the preferable option. For performance and scalability, most NoSQL solutions forgo ACID compliance. SQL vs. NoSQL: Which is Better? There is no such thing as a one-size-fits-all solution when it comes to database technology. As a result, many firms use both relational and non-relational databases for various purposes. Despite the fact that NoSQL databases are becoming more popular due to their speed and scalability, there are still occasions where a highly organized SQL database may outperform; the proper technology depends on the use case. Benefits of Using a SQL Database Here are some of the benefits of using a SQL database: We must ensure that ACID compliance is met. By specifying exactly how transactions interact with the database, ACID compliance lowers anomalies and maintains the integrity of your database. In general, NoSQL databases forego ACID compliance in favor of scalability and processing speed, yet an ACID-compliant database remains the preferred solution for many e-commerce and financial applications. Your data is well-organized and stable. There may be no reason to employ a system built to support a range of data kinds and large traffic volume if your firm is not seeing enormous expansion that would necessitate more servers and if you're simply working with data that is consistent. Benefits of Using a NoSQL Database When all of our application's other components are running smoothly, NoSQL databases keep data from becoming a bottleneck. Big data is helping NoSQL databases achieve great success, owing to the fact that it processes data differently than traditional relational databases. MongoDB, CouchDB, Cassandra, and HBase are some examples of NoSQL databases. Managing enormous amounts of data with little to no organization. A NoSQL database has no restrictions on the types of data that can be stored together and allows us to add new types as our needs evolve. You can store data in one place with document-based databases without needing to describe what \"kind\" of data you're storing in advance. Using cloud computing and storage to its full potential. Cloud storage is a great way to save money, but it requires data to be easily transferred over several servers in order to scale up. Using commodity (cheap, smaller) hardware on-site or in the cloud eliminates the need for additional software, and NoSQL databases like Cassandra are built to scale across numerous data centers without causing a lot of headaches out of the box. Rapid progress. Because NoSQL does not require any prior preparation, it is ideal for quick development. A relational database will slow you down if you're working on quick iterations of your system that require frequent adjustments to the data structure with little downtime between versions.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql-vs-nosql","text":"There are two sorts of database solutions in the world: SQL and NoSQL (or relational databases and non-relational databases). The manner they were designed, the type of information they hold, and the storage mechanism they use are all different. Relational databases, like phone books that record phone numbers and addresses, are structured and have predetermined schemas. Non-relational databases, like file folders, are unstructured, distributed, and feature a dynamic schema that stores everything from a person's address and phone number to their Facebook \"likes\" and online shopping preferences.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql","text":"Data is stored in rows and columns in relational databases. Each row holds all of the data for a single entity, whereas each column has all of the individual data points. MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB are some of the most popular relational databases.","title":"SQL"},{"location":"SQLvsNoSQL/#nosql","text":"The most prevalent forms of NoSQL are as follows: Key-Value Stores: Data is kept as a key-value pair array. A 'key' is a term for an attribute that is associated to a 'value.' Redis, Voldemort, and Dynamo are examples of well-known key-value stores. Document Databases: Data is kept in documents (rather than rows and columns in a table) in these databases, and these documents are grouped together in collections. The structure of any document can be completely different. CouchDB and MongoDB are two document databases. Wide-Column Databases: Column families, rather than 'tables,' are containers for rows in columnar databases. We don't need to know all the columns up front, and each row doesn't have to have the same number of columns as a relational database. Columnar databases, such as Cassandra and HBase, are ideally suited for analyzing massive datasets. Graph Databases: These databases are used to hold information that can best be represented as a graph. Nodes (entities), attributes (information about the entities), and lines are used to store data in graph topologies (connections between the entities). Neo4J and Infinite Graph are two examples of graph databases.","title":"NoSQL"},{"location":"SQLvsNoSQL/#differences-in-sql-vs-nosql-on-a-high-level","text":"Storage: SQL saves data in tables, with each row representing an entity and each column representing a data point about that entity; for example, if we're keeping a car entity in a table, distinct columns might be 'Color, Make, Model,' and so on. The data storage models used by NoSQL databases vary. Key-value, document, graph, and columnar are the most common types. Below, we'll compare and contrast these two databases. Schema: Each record in SQL must adhere to a fixed schema, which means that the columns must be determined and chosen prior to data entry, and each row must have data for each field. Later, the schema can be changed, but it will require updating the entire database and going offline. Schemas in NoSQL are dynamic. Columns can be added at any time, and each 'row' (or equivalent) does not need to include data for each 'column.' Querying: SQL (structured query language) is a powerful language for defining and manipulating data in SQL databases. Queries in a NoSQL database are focused on a set of documents. UnQL is another name for it (Unstructured Query Language). The syntax for using UnQL varies depending on the database. Scalability: In most cases, SQL databases are vertically scalable, meaning that they may be scaled up by increasing the hardware's horsepower (memory, CPU, etc.), which can be quite costly. A relational database can be scaled across numerous servers, but it is a difficult and time-consuming operation. NoSQL databases, on the other hand, are horizontally scalable, which means we can quickly add more servers to our NoSQL database infrastructure to manage a large amount of traffic. NoSQL databases may be hosted on any cheap commodity hardware or cloud instances, making it much more cost-effective than vertical scaling. Many NoSQL technologies also automatically spread data across servers. Atomicity, Consistency, Isolation, and Durability (ACID) Compliance: ACID compliance is seen in the vast majority of relational databases. So, when it comes to data consistency and transaction security, SQL databases are still the preferable option. For performance and scalability, most NoSQL solutions forgo ACID compliance.","title":"Differences in SQL vs NoSQL on a high level"},{"location":"SQLvsNoSQL/#sql-vs-nosql-which-is-better","text":"There is no such thing as a one-size-fits-all solution when it comes to database technology. As a result, many firms use both relational and non-relational databases for various purposes. Despite the fact that NoSQL databases are becoming more popular due to their speed and scalability, there are still occasions where a highly organized SQL database may outperform; the proper technology depends on the use case.","title":"SQL vs. NoSQL: Which is Better?"},{"location":"SQLvsNoSQL/#benefits-of-using-a-sql-database","text":"Here are some of the benefits of using a SQL database: We must ensure that ACID compliance is met. By specifying exactly how transactions interact with the database, ACID compliance lowers anomalies and maintains the integrity of your database. In general, NoSQL databases forego ACID compliance in favor of scalability and processing speed, yet an ACID-compliant database remains the preferred solution for many e-commerce and financial applications. Your data is well-organized and stable. There may be no reason to employ a system built to support a range of data kinds and large traffic volume if your firm is not seeing enormous expansion that would necessitate more servers and if you're simply working with data that is consistent.","title":"Benefits of Using a SQL Database"},{"location":"SQLvsNoSQL/#benefits-of-using-a-nosql-database","text":"When all of our application's other components are running smoothly, NoSQL databases keep data from becoming a bottleneck. Big data is helping NoSQL databases achieve great success, owing to the fact that it processes data differently than traditional relational databases. MongoDB, CouchDB, Cassandra, and HBase are some examples of NoSQL databases. Managing enormous amounts of data with little to no organization. A NoSQL database has no restrictions on the types of data that can be stored together and allows us to add new types as our needs evolve. You can store data in one place with document-based databases without needing to describe what \"kind\" of data you're storing in advance. Using cloud computing and storage to its full potential. Cloud storage is a great way to save money, but it requires data to be easily transferred over several servers in order to scale up. Using commodity (cheap, smaller) hardware on-site or in the cloud eliminates the need for additional software, and NoSQL databases like Cassandra are built to scale across numerous data centers without causing a lot of headaches out of the box. Rapid progress. Because NoSQL does not require any prior preparation, it is ideal for quick development. A relational database will slow you down if you're working on quick iterations of your system that require frequent adjustments to the data structure with little downtime between versions.","title":"Benefits of Using a NoSQL Database"},{"location":"StepsToApproachAProblem/","text":"Steps To Approach A Problem It is suggested that you solve the problem by following the procedures below. Feature Expectations (First 2 Minutes): As previously said, there is no such thing as a bad design. There are only excellent and terrible designs, and the same solution may be good for one use case but awful for another. As a result, it is critical to have a thorough understanding of the question's requirements. Approximations ( 2-5 mins ) The next step is usually to calculate the system's scale. The purpose of this step is to figure out how much sharding is needed (if any) and to narrow down the system's design goals. For example, if all of the system's data fits on a single machine, we might not need to worry about sharding of the other issues that come with a distributed system architecture. Alternatively, if the most frequently used data fits on a single machine, caching might be done there. Design Objectives ( 1 mins ) Determine what the system's most critical objectives are. It's possible that some systems are latency systems, in which case a solution that ignores this could result in poor design. The design's skeleton ( 4 - 5 mins ) There isn't enough time to go over each component in depth in 30-40 minutes. As a result, a solid method is to talk with the interviewer at a general level and then dive into the components that the interviewer has asked about. Extensive research ( 20-30 mins ) Using low level and high level design, go over the problem in detail. The more detailed question needs to think and asked can be seen in next session In the next session, we'll look at the more detailed questions that need to be considered and asked.","title":"Steps To Approach A Problem"},{"location":"StepsToApproachAProblem/#steps-to-approach-a-problem","text":"It is suggested that you solve the problem by following the procedures below.","title":"Steps To Approach A Problem"},{"location":"StepsToApproachAProblem/#feature-expectations-first-2-minutes","text":"As previously said, there is no such thing as a bad design. There are only excellent and terrible designs, and the same solution may be good for one use case but awful for another. As a result, it is critical to have a thorough understanding of the question's requirements.","title":"Feature Expectations (First 2 Minutes):"},{"location":"StepsToApproachAProblem/#approximations-2-5-mins","text":"The next step is usually to calculate the system's scale. The purpose of this step is to figure out how much sharding is needed (if any) and to narrow down the system's design goals. For example, if all of the system's data fits on a single machine, we might not need to worry about sharding of the other issues that come with a distributed system architecture. Alternatively, if the most frequently used data fits on a single machine, caching might be done there.","title":"Approximations ( 2-5 mins )"},{"location":"StepsToApproachAProblem/#design-objectives-1-mins","text":"Determine what the system's most critical objectives are. It's possible that some systems are latency systems, in which case a solution that ignores this could result in poor design.","title":"Design Objectives ( 1 mins )"},{"location":"StepsToApproachAProblem/#the-designs-skeleton-4-5-mins","text":"There isn't enough time to go over each component in depth in 30-40 minutes. As a result, a solid method is to talk with the interviewer at a general level and then dive into the components that the interviewer has asked about.","title":"The design's skeleton ( 4 - 5 mins )"},{"location":"StepsToApproachAProblem/#extensive-research-20-30-mins","text":"Using low level and high level design, go over the problem in detail. The more detailed question needs to think and asked can be seen in next session In the next session, we'll look at the more detailed questions that need to be considered and asked.","title":"Extensive research ( 20-30 mins )"},{"location":"SystemDesignBasic/","text":"System Design Basics Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basics"},{"location":"SystemDesignBasic/#system-design-basics","text":"Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basics"},{"location":"SystemDesignInterviews/","text":"A Step-by-Step Guide to System Design Interviews Many software developers have difficulty with system design interviews (SDIs) for three reasons: SDIs are unstructured in nature, with applicants being asked to work on an open-ended design challenge with no standard solution. Candidates are lacking in intricate and large-scale system development experience. Candidates did not devote enough time on SDI preparation. Candidates who haven't put out a concerted effort to prepare for SDIs, like those who haven't put forth a concerted effort to prepare for coding interviews, typically perform poorly, particularly at top organizations such as Google, Facebook, Amazon, Microsoft, and others. Candidates who do not perform above average at these companies have a slim probability of being hired. A good performance, on the other hand, invariably leads to a better offer (a higher job and compensation), as it demonstrates the candidate's capacity to manage a complicated system. This article will take a step-by-step approach to solving numerous design issues. Let's begin with the steps below: Step 1: Clarification of requirements It's always a good idea to ask about the scope of the problem we're attempting to solve. Because design questions are typically open-ended and don't have a single proper solution, it's vital to clear up any misunderstandings early in the interview. Candidates that take the effort to establish the system's end goals have a better chance of succeeding in the interview. We should also define which aspects of the system we will be working on given we only have 35-40 minutes to build a (apparently) vast system. Let's have a look at a real-world example of how to construct a Twitter-like service. Before moving on to the next steps, you should answer the following questions about Twitter design: Will our service's users be able to publish tweets and follow others? Should we also design for the user's timeline to be created and displayed? Will images and videos be included in tweets? Are we concentrating just on the backend or are we also working on the front-end? Will users be able to use Twitter to search for tweets? Is it necessary to show hot subjects that are currently trending? Will new (or important) tweets be notified via push notification? All of these considerations will influence the final design. Step 2: Estimation from the back of the envelope Estimating the scope of the system we're going to design is always a smart idea. This will also come in handy later when we're dealing with scaling, partitioning, load balancing, and caching. On what scale should the system operate (e.g., number of new tweets, tweet views, timeline generation per second, etc.)? What kind of storage will we require? If users may include photographs and videos in their tweets, we will have different storage needs. What kind of network bandwidth utilization do we anticipate? This will be critical in determining how we will manage traffic and load balance among servers. Step 3: Define the system interface Define which APIs the system should provide. This will not only define the exact contract that the system should deliver, but it will also guarantee that we haven't missed any criteria. APIs for our Twitter-like service will include the following: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026) Step 4: Defining the data model Defining the data model early in the interview will help to define how data will move between the system's various components. It will later provide instructions for data partitioning and administration. The candidate must be able to recognize various system entities, how they will interact with one another, and various areas of data management such as storage, transportation, encryption, and so on. For our Twitter-like service, here are some entities: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Should we use a relational database system or a relational database system? Will a NoSQL database, such as Cassandra, be the greatest fit for our purposes, or should we go with a MySQL-like solution? To store photographs and videos, what type of block storage should we use? Step 5: High-level design Create a block diagram with 5-6 boxes to illustrate the system's essential components. We should determine how many components are required to tackle the problem from beginning to conclusion. At a high level, numerous application servers will be required to service all read/write requests for Twitter, with load balancers in front of them to distribute traffic. If we anticipate a high volume of read traffic (in comparison to write traffic), we can set up separate servers to handle these circumstances. We'll need an efficient database on the backend to hold all of the tweets and support a large number of reads. For storing photographs and movies, we'll also need a distributed file storage system. Step 6: Detailed design Examine two or three important components in further depth; the interviewer's input should always point us in the direction of which portions of the system require more study. We should be able to demonstrate many approaches, their benefits and drawbacks, and explain why we prefer one over the other. Remember that there is no one-size-fits-all solution; the most important thing is to weigh the benefits and drawbacks of various solutions while keeping system limits in mind. How should we split our data to distribute it across various databases, given that we will be storing a large amount of data? Should we aim to keep all of a user's data in the same database? What kind of problem may it cause? How will we deal with popular users who send out a lot of tweets or have a lot of followers? Should we aim to store our data in such a way that it is optimized for scanning the most current (and relevant) tweets, as users' timelines will contain the most recent (and relevant) tweets? To speed things up, how much and at which layer should we introduce cache? Which components require improved load balancing? Step 7: Identifying bottlenecks and fixing them Try to cover as many bottlenecks as possible, as well as potential solutions. Does our system have any single points of failure? So, what are we going to do about it? Do we have enough data copies so we can continue serve our users if a few servers fail? Do we have enough copies of different services running such that a few failures do not result in the entire system being shut down? How do we keep track of our service's performance? Do we be notified if one of our important components fails or if its performance deteriorates? Conclusion In short, the keys to success in system design interviews are preparation and organization during the interview. The processes outlined above should help you stay on track and cover all of the major components of system design. Let's use the recommendations above to develop a few systems that are commonly asked for in SDIs. If you understand the concept and basic technologies mentioned in the previous section, you can skip Level 2 and go straight to Level 3 problem solution.","title":"System Design Interviews"},{"location":"SystemDesignInterviews/#a-step-by-step-guide-to-system-design-interviews","text":"Many software developers have difficulty with system design interviews (SDIs) for three reasons: SDIs are unstructured in nature, with applicants being asked to work on an open-ended design challenge with no standard solution. Candidates are lacking in intricate and large-scale system development experience. Candidates did not devote enough time on SDI preparation. Candidates who haven't put out a concerted effort to prepare for SDIs, like those who haven't put forth a concerted effort to prepare for coding interviews, typically perform poorly, particularly at top organizations such as Google, Facebook, Amazon, Microsoft, and others. Candidates who do not perform above average at these companies have a slim probability of being hired. A good performance, on the other hand, invariably leads to a better offer (a higher job and compensation), as it demonstrates the candidate's capacity to manage a complicated system. This article will take a step-by-step approach to solving numerous design issues. Let's begin with the steps below:","title":"A Step-by-Step Guide to System Design Interviews"},{"location":"SystemDesignInterviews/#step-1-clarification-of-requirements","text":"It's always a good idea to ask about the scope of the problem we're attempting to solve. Because design questions are typically open-ended and don't have a single proper solution, it's vital to clear up any misunderstandings early in the interview. Candidates that take the effort to establish the system's end goals have a better chance of succeeding in the interview. We should also define which aspects of the system we will be working on given we only have 35-40 minutes to build a (apparently) vast system. Let's have a look at a real-world example of how to construct a Twitter-like service. Before moving on to the next steps, you should answer the following questions about Twitter design: Will our service's users be able to publish tweets and follow others? Should we also design for the user's timeline to be created and displayed? Will images and videos be included in tweets? Are we concentrating just on the backend or are we also working on the front-end? Will users be able to use Twitter to search for tweets? Is it necessary to show hot subjects that are currently trending? Will new (or important) tweets be notified via push notification? All of these considerations will influence the final design.","title":"Step 1: Clarification of requirements"},{"location":"SystemDesignInterviews/#step-2-estimation-from-the-back-of-the-envelope","text":"Estimating the scope of the system we're going to design is always a smart idea. This will also come in handy later when we're dealing with scaling, partitioning, load balancing, and caching. On what scale should the system operate (e.g., number of new tweets, tweet views, timeline generation per second, etc.)? What kind of storage will we require? If users may include photographs and videos in their tweets, we will have different storage needs. What kind of network bandwidth utilization do we anticipate? This will be critical in determining how we will manage traffic and load balance among servers.","title":"Step 2: Estimation from the back of the envelope"},{"location":"SystemDesignInterviews/#step-3-define-the-system-interface","text":"Define which APIs the system should provide. This will not only define the exact contract that the system should deliver, but it will also guarantee that we haven't missed any criteria. APIs for our Twitter-like service will include the following: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026)","title":"Step 3: Define the system interface"},{"location":"SystemDesignInterviews/#step-4-defining-the-data-model","text":"Defining the data model early in the interview will help to define how data will move between the system's various components. It will later provide instructions for data partitioning and administration. The candidate must be able to recognize various system entities, how they will interact with one another, and various areas of data management such as storage, transportation, encryption, and so on. For our Twitter-like service, here are some entities: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Should we use a relational database system or a relational database system? Will a NoSQL database, such as Cassandra, be the greatest fit for our purposes, or should we go with a MySQL-like solution? To store photographs and videos, what type of block storage should we use?","title":"Step 4: Defining the data model"},{"location":"SystemDesignInterviews/#step-5-high-level-design","text":"Create a block diagram with 5-6 boxes to illustrate the system's essential components. We should determine how many components are required to tackle the problem from beginning to conclusion. At a high level, numerous application servers will be required to service all read/write requests for Twitter, with load balancers in front of them to distribute traffic. If we anticipate a high volume of read traffic (in comparison to write traffic), we can set up separate servers to handle these circumstances. We'll need an efficient database on the backend to hold all of the tweets and support a large number of reads. For storing photographs and movies, we'll also need a distributed file storage system.","title":"Step 5: High-level design"},{"location":"SystemDesignInterviews/#step-6-detailed-design","text":"Examine two or three important components in further depth; the interviewer's input should always point us in the direction of which portions of the system require more study. We should be able to demonstrate many approaches, their benefits and drawbacks, and explain why we prefer one over the other. Remember that there is no one-size-fits-all solution; the most important thing is to weigh the benefits and drawbacks of various solutions while keeping system limits in mind. How should we split our data to distribute it across various databases, given that we will be storing a large amount of data? Should we aim to keep all of a user's data in the same database? What kind of problem may it cause? How will we deal with popular users who send out a lot of tweets or have a lot of followers? Should we aim to store our data in such a way that it is optimized for scanning the most current (and relevant) tweets, as users' timelines will contain the most recent (and relevant) tweets? To speed things up, how much and at which layer should we introduce cache? Which components require improved load balancing?","title":"Step 6: Detailed design"},{"location":"SystemDesignInterviews/#step-7-identifying-bottlenecks-and-fixing-them","text":"Try to cover as many bottlenecks as possible, as well as potential solutions. Does our system have any single points of failure? So, what are we going to do about it? Do we have enough data copies so we can continue serve our users if a few servers fail? Do we have enough copies of different services running such that a few failures do not result in the entire system being shut down? How do we keep track of our service's performance? Do we be notified if one of our important components fails or if its performance deteriorates?","title":"Step 7: Identifying bottlenecks and fixing them"},{"location":"SystemDesignInterviews/#conclusion","text":"In short, the keys to success in system design interviews are preparation and organization during the interview. The processes outlined above should help you stay on track and cover all of the major components of system design. Let's use the recommendations above to develop a few systems that are commonly asked for in SDIs. If you understand the concept and basic technologies mentioned in the previous section, you can skip Level 2 and go straight to Level 3 problem solution.","title":"Conclusion"},{"location":"about/","text":"","title":"About"}]}