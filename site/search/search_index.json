{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"CAPTheorem/","text":"CAP Theorem CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of the following guarantees (CAP): Consistency, Availability, and Partition tolerance. When we design a distributed system, trading off among CAP is almost the first thing we want to consider. CAP theorem says while designing a distributed system we can pick only two of the following three options: Consistency: All nodes see the same data at the same time. Consistency is achieved by updating several nodes before allowing further reads. Availability: Every request gets a response on success/failure. Availability is achieved by replicating the data across different servers. Partition tolerance: The system continues to work despite message loss or partial failure. A system that is partition-tolerant can sustain any amount of network failure that doesn\u2019t result in a failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages. We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures. We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.","title":"CAP Theorem"},{"location":"CAPTheorem/#cap-theorem","text":"CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of the following guarantees (CAP): Consistency, Availability, and Partition tolerance. When we design a distributed system, trading off among CAP is almost the first thing we want to consider. CAP theorem says while designing a distributed system we can pick only two of the following three options: Consistency: All nodes see the same data at the same time. Consistency is achieved by updating several nodes before allowing further reads. Availability: Every request gets a response on success/failure. Availability is achieved by replicating the data across different servers. Partition tolerance: The system continues to work despite message loss or partial failure. A system that is partition-tolerant can sustain any amount of network failure that doesn\u2019t result in a failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages. We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures. We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.","title":"CAP Theorem"},{"location":"Caching/","text":"Caching Load balancing helps you scale horizontally across an ever-increasing number of servers, but caching will enable you to make vastly better use of the resources you already have as well as making otherwise unattainable product requirements feasible. Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again. They are used in almost every layer of computing: hardware, operating systems, web browsers, web applications, and more. A cache is like short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items. Caches can exist at all levels in architecture, but are often found at the level nearest to the front end where they are implemented to return data quickly without taxing downstream levels Application server cache Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local cached data if it exists. If it is not in the cache, the requesting node will query the data from disk. The cache on one request layer node could also be located both in memory (which is very fast) and on the node\u2019s local disk (faster than going to network storage). What happens when you expand this to many nodes? If the request layer is expanded to multiple nodes, it\u2019s still quite possible to have each node host its own cache. However, if your load balancer randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. Two choices for overcoming this hurdle are global caches and distributed caches. Content Distribution Network (CDN) CDNs are a kind of cache that comes into play for sites serving large amounts of static media. In a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that content if it has it locally available. If it isn\u2019t available, the CDN will query the back-end servers for the file, cache it locally, and serve it to the requesting user. If the system we are building isn\u2019t yet large enough to have its own CDN, we can ease a future transition by serving the static media off a separate subdomain (e.g. static.yourservice.com) using a lightweight HTTP server like Nginx, and cut-over the DNS from your servers to a CDN later. Cache Invalidation While caching is fantastic, it does require some maintenance for keeping cache coherent with the source of truth (e.g., database). If the data is modified in the database, it should be invalidated in the cache; if not, this can cause inconsistent application behavior. Solving this problem is known as cache invalidation; there are three main schemes that are used: Write-through cache: Under this scheme, data is written into the cache and the corresponding database at the same time. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions. Although, write through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations. Write-around cache: This technique is similar to write through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a \u201ccache miss\u201d and must be read from slower back-end storage and experience higher latency. Write-back cache: Under this scheme, data is written to cache alone and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low latency and high throughput for write-intensive applications, however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache. Cache eviction policies Following are some of the most common cache eviction policies: First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before. Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before. Least Recently Used (LRU): Discards the least recently used items first. Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first. Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first. Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. Following links have some good discussion about caching: Cache Introduction to architecting systems","title":"Caching"},{"location":"Caching/#caching","text":"Load balancing helps you scale horizontally across an ever-increasing number of servers, but caching will enable you to make vastly better use of the resources you already have as well as making otherwise unattainable product requirements feasible. Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again. They are used in almost every layer of computing: hardware, operating systems, web browsers, web applications, and more. A cache is like short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items. Caches can exist at all levels in architecture, but are often found at the level nearest to the front end where they are implemented to return data quickly without taxing downstream levels","title":"Caching"},{"location":"Caching/#application-server-cache","text":"Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local cached data if it exists. If it is not in the cache, the requesting node will query the data from disk. The cache on one request layer node could also be located both in memory (which is very fast) and on the node\u2019s local disk (faster than going to network storage). What happens when you expand this to many nodes? If the request layer is expanded to multiple nodes, it\u2019s still quite possible to have each node host its own cache. However, if your load balancer randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. Two choices for overcoming this hurdle are global caches and distributed caches.","title":"Application server cache"},{"location":"Caching/#content-distribution-network-cdn","text":"CDNs are a kind of cache that comes into play for sites serving large amounts of static media. In a typical CDN setup, a request will first ask the CDN for a piece of static media; the CDN will serve that content if it has it locally available. If it isn\u2019t available, the CDN will query the back-end servers for the file, cache it locally, and serve it to the requesting user. If the system we are building isn\u2019t yet large enough to have its own CDN, we can ease a future transition by serving the static media off a separate subdomain (e.g. static.yourservice.com) using a lightweight HTTP server like Nginx, and cut-over the DNS from your servers to a CDN later.","title":"Content Distribution Network (CDN)"},{"location":"Caching/#cache-invalidation","text":"While caching is fantastic, it does require some maintenance for keeping cache coherent with the source of truth (e.g., database). If the data is modified in the database, it should be invalidated in the cache; if not, this can cause inconsistent application behavior. Solving this problem is known as cache invalidation; there are three main schemes that are used:","title":"Cache Invalidation"},{"location":"Caching/#write-through-cache","text":"Under this scheme, data is written into the cache and the corresponding database at the same time. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions. Although, write through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations.","title":"Write-through cache:"},{"location":"Caching/#write-around-cache","text":"This technique is similar to write through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a \u201ccache miss\u201d and must be read from slower back-end storage and experience higher latency.","title":"Write-around cache:"},{"location":"Caching/#write-back-cache","text":"Under this scheme, data is written to cache alone and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low latency and high throughput for write-intensive applications, however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache.","title":"Write-back cache:"},{"location":"Caching/#cache-eviction-policies","text":"Following are some of the most common cache eviction policies: First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before. Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before. Least Recently Used (LRU): Discards the least recently used items first. Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first. Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first. Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. Following links have some good discussion about caching: Cache Introduction to architecting systems","title":"Cache eviction policies"},{"location":"ConsistentHashing/","text":"Consistent Hashing Distributed Hash Table (DHT) is one of the fundamental components used in distributed scalable systems. Hash Tables need a key, a value, and a hash function where hash function maps the key to a location where the value is stored. index = hash_function(key) Suppose we are designing a distributed caching system. Given \u2018n\u2019 cache servers, an intuitive hash function would be \u2018key % n\u2019. It is simple and commonly used. But it has two major drawbacks: It is NOT horizontally scalable. Whenever a new cache host is added to the system, all existing mappings are broken. It will be a pain point in maintenance if the caching system contains lots of data. Practically, it becomes difficult to schedule a downtime to update all caching mappings. It may NOT be load balanced, especially for non-uniformly distributed data. In practice, it can be easily assumed that the data will not be distributed uniformly. For the caching system, it translates into some caches becoming hot and saturated while the others idle and are almost empty. In such situations, consistent hashing is a good way to improve the caching system. What is Consistent Hashing? Consistent hashing is a very useful strategy for distributed caching system and DHTs. It allows us to distribute data across a cluster in such a way that will minimize reorganization when nodes are added or removed. Hence, the caching system will be easier to scale up or scale down. In Consistent Hashing, when the hash table is resized (e.g. a new cache host is added to the system), only \u2018k/n\u2019 keys need to be remapped where \u2018k\u2019 is the total number of keys and \u2018n\u2019 is the total number of servers. Recall that in a caching system using the \u2018mod\u2019 as the hash function, all keys need to be remapped. In Consistent Hashing, objects are mapped to the same host if possible. When a host is removed from the system, the objects on that host are shared by other hosts; when a new host is added, it takes its share from a few hosts without touching other\u2019s shares. How does it work? As a typical hash function, consistent hashing maps a key to an integer. Suppose the output of the hash function is in the range of [0, 256). Imagine that the integers in the range are placed on a ring such that the values are wrapped around. Here\u2019s how consistent hashing works: Given a list of cache servers, hash them to integers in the range. To map a key to a server, a. Hash it to a single integer. b. Move clockwise on the ring until finding the first cache it encounters. c. That cache is the one that contains the key. See below as an example: key1 maps to cache A; key2 maps to cache C. To add a new server, say D, keys that were originally residing at C will be split. Some of them will be shifted to D, while other keys will not be touched. To remove a cache or, if a cache fails, say A, all keys that were originally mapped to A will fall into B, and only those keys need to be moved to B; other keys will not be affected. For load balancing, as we discussed in the beginning, the real data is essentially randomly distributed and thus may not be uniform. It may make the keys on caches unbalanced. To handle this issue, we add \u201cvirtual replicas\u201d for caches. Instead of mapping each cache to a single point on the ring, we map it to multiple points on the ring, i.e. replicas. This way, each cache is associated with multiple portions of the ring. If the hash function \u201cmixes well,\u201d as the number of replicas increases, the keys will be more balanced.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#consistent-hashing","text":"Distributed Hash Table (DHT) is one of the fundamental components used in distributed scalable systems. Hash Tables need a key, a value, and a hash function where hash function maps the key to a location where the value is stored. index = hash_function(key) Suppose we are designing a distributed caching system. Given \u2018n\u2019 cache servers, an intuitive hash function would be \u2018key % n\u2019. It is simple and commonly used. But it has two major drawbacks: It is NOT horizontally scalable. Whenever a new cache host is added to the system, all existing mappings are broken. It will be a pain point in maintenance if the caching system contains lots of data. Practically, it becomes difficult to schedule a downtime to update all caching mappings. It may NOT be load balanced, especially for non-uniformly distributed data. In practice, it can be easily assumed that the data will not be distributed uniformly. For the caching system, it translates into some caches becoming hot and saturated while the others idle and are almost empty. In such situations, consistent hashing is a good way to improve the caching system.","title":"Consistent Hashing"},{"location":"ConsistentHashing/#what-is-consistent-hashing","text":"Consistent hashing is a very useful strategy for distributed caching system and DHTs. It allows us to distribute data across a cluster in such a way that will minimize reorganization when nodes are added or removed. Hence, the caching system will be easier to scale up or scale down. In Consistent Hashing, when the hash table is resized (e.g. a new cache host is added to the system), only \u2018k/n\u2019 keys need to be remapped where \u2018k\u2019 is the total number of keys and \u2018n\u2019 is the total number of servers. Recall that in a caching system using the \u2018mod\u2019 as the hash function, all keys need to be remapped. In Consistent Hashing, objects are mapped to the same host if possible. When a host is removed from the system, the objects on that host are shared by other hosts; when a new host is added, it takes its share from a few hosts without touching other\u2019s shares.","title":"What is Consistent Hashing?"},{"location":"ConsistentHashing/#how-does-it-work","text":"As a typical hash function, consistent hashing maps a key to an integer. Suppose the output of the hash function is in the range of [0, 256). Imagine that the integers in the range are placed on a ring such that the values are wrapped around. Here\u2019s how consistent hashing works: Given a list of cache servers, hash them to integers in the range. To map a key to a server, a. Hash it to a single integer. b. Move clockwise on the ring until finding the first cache it encounters. c. That cache is the one that contains the key. See below as an example: key1 maps to cache A; key2 maps to cache C. To add a new server, say D, keys that were originally residing at C will be split. Some of them will be shifted to D, while other keys will not be touched. To remove a cache or, if a cache fails, say A, all keys that were originally mapped to A will fall into B, and only those keys need to be moved to B; other keys will not be affected. For load balancing, as we discussed in the beginning, the real data is essentially randomly distributed and thus may not be uniform. It may make the keys on caches unbalanced. To handle this issue, we add \u201cvirtual replicas\u201d for caches. Instead of mapping each cache to a single point on the ring, we map it to multiple points on the ring, i.e. replicas. This way, each cache is associated with multiple portions of the ring. If the hash function \u201cmixes well,\u201d as the number of replicas increases, the keys will be more balanced.","title":"How does it work?"},{"location":"DataPartitioning/","text":"Data Partitioning Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers. Partitioning Methods There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications. a. Horizontal partitioning: In this scheme, we put different rows into different tables. For example, if we are storing different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table. This is also called a range based partitioning as we are storing different ranges of data in separate tables. Horizontal partitioning is also called as Data Sharding. The key problem with this approach is that if the value whose range is used for partitioning isn\u2019t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting location based on their zip codes assumes that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities. b. Vertical Partitioning: In this scheme, we divide our data to store tables related to a specific feature in their own server. For example, if we are building Instagram like application - where we need to store data related to users, photos they upload, and people they follow - we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server. Vertical partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a feature specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users). c. Directory Based Partitioning: A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application. Partitioning Criteria a. Key or Hash-based partitioning: Under this scheme, we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. For example, if we have 100 DB servers and our ID is a numeric value that gets incremented by one each time a new record is inserted. In this example, the hash function could be \u2018ID % 100\u2019, which will give us the server number where we can store/read that record. This approach should ensure a uniform allocation of data among servers. The fundamental problem with this approach is that it effectively fixes the total number of DB servers, since adding new servers means changing the hash function which would require redistribution of data and downtime for the service. A workaround for this problem is to use Consistent Hashing. b. List partitioning: In this scheme, each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries. c. Round-robin partitioning: This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n). d. Composite partitioning: Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed. Common Problems of Data Partitioning On a partitioned database, there are certain extra constraints on the different operations that can be performed. Most of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server. Below are some of the constraints and additional complexities introduced by partitioning: a. Joins and Denormalization: Performing joins on a database which is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with all the perils of denormalization such as data inconsistency. b. Referential integrity: As we saw that performing a cross-partition query on a partitioned database is not feasible, similarly, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult. Most of RDBMS do not support foreign keys constraints across databases on different database servers. Which means that applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references. c. Rebalancing: There could be many reasons we have to change our partitioning scheme: The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition. There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos. In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).","title":"Data Partitioning"},{"location":"DataPartitioning/#data-partitioning","text":"Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.","title":"Data Partitioning"},{"location":"DataPartitioning/#partitioning-methods","text":"There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications.","title":"Partitioning Methods"},{"location":"DataPartitioning/#a-horizontal-partitioning","text":"In this scheme, we put different rows into different tables. For example, if we are storing different places in a table, we can decide that locations with ZIP codes less than 10000 are stored in one table and places with ZIP codes greater than 10000 are stored in a separate table. This is also called a range based partitioning as we are storing different ranges of data in separate tables. Horizontal partitioning is also called as Data Sharding. The key problem with this approach is that if the value whose range is used for partitioning isn\u2019t chosen carefully, then the partitioning scheme will lead to unbalanced servers. In the previous example, splitting location based on their zip codes assumes that places will be evenly distributed across the different zip codes. This assumption is not valid as there will be a lot of places in a thickly populated area like Manhattan as compared to its suburb cities.","title":"a. Horizontal partitioning:"},{"location":"DataPartitioning/#b-vertical-partitioning","text":"In this scheme, we divide our data to store tables related to a specific feature in their own server. For example, if we are building Instagram like application - where we need to store data related to users, photos they upload, and people they follow - we can decide to place user profile information on one DB server, friend lists on another, and photos on a third server. Vertical partitioning is straightforward to implement and has a low impact on the application. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition a feature specific DB across various servers (e.g. it would not be possible for a single server to handle all the metadata queries for 10 billion photos by 140 million users).","title":"b. Vertical Partitioning:"},{"location":"DataPartitioning/#c-directory-based-partitioning","text":"A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application.","title":"c. Directory Based Partitioning:"},{"location":"DataPartitioning/#partitioning-criteria","text":"","title":"Partitioning Criteria"},{"location":"DataPartitioning/#a-key-or-hash-based-partitioning","text":"Under this scheme, we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. For example, if we have 100 DB servers and our ID is a numeric value that gets incremented by one each time a new record is inserted. In this example, the hash function could be \u2018ID % 100\u2019, which will give us the server number where we can store/read that record. This approach should ensure a uniform allocation of data among servers. The fundamental problem with this approach is that it effectively fixes the total number of DB servers, since adding new servers means changing the hash function which would require redistribution of data and downtime for the service. A workaround for this problem is to use Consistent Hashing.","title":"a. Key or Hash-based partitioning:"},{"location":"DataPartitioning/#b-list-partitioning","text":"In this scheme, each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries.","title":"b. List partitioning:"},{"location":"DataPartitioning/#c-round-robin-partitioning","text":"This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n).","title":"c. Round-robin partitioning:"},{"location":"DataPartitioning/#d-composite-partitioning","text":"Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed.","title":"d. Composite partitioning:"},{"location":"DataPartitioning/#common-problems-of-data-partitioning","text":"On a partitioned database, there are certain extra constraints on the different operations that can be performed. Most of these constraints are due to the fact that operations across multiple tables or multiple rows in the same table will no longer run on the same server. Below are some of the constraints and additional complexities introduced by partitioning:","title":"Common Problems of Data Partitioning"},{"location":"DataPartitioning/#a-joins-and-denormalization","text":"Performing joins on a database which is running on one server is straightforward, but once a database is partitioned and spread across multiple machines it is often not feasible to perform joins that span database partitions. Such joins will not be performance efficient since data has to be compiled from multiple servers. A common workaround for this problem is to denormalize the database so that queries that previously required joins can be performed from a single table. Of course, the service now has to deal with all the perils of denormalization such as data inconsistency.","title":"a. Joins and Denormalization:"},{"location":"DataPartitioning/#b-referential-integrity","text":"As we saw that performing a cross-partition query on a partitioned database is not feasible, similarly, trying to enforce data integrity constraints such as foreign keys in a partitioned database can be extremely difficult. Most of RDBMS do not support foreign keys constraints across databases on different database servers. Which means that applications that require referential integrity on partitioned databases often have to enforce it in application code. Often in such cases, applications have to run regular SQL jobs to clean up dangling references.","title":"b. Referential integrity:"},{"location":"DataPartitioning/#c-rebalancing","text":"There could be many reasons we have to change our partitioning scheme: The data distribution is not uniform, e.g., there are a lot of places for a particular ZIP code that cannot fit into one database partition. There is a lot of load on a partition, e.g., there are too many requests being handled by the DB partition dedicated to user photos. In such cases, either we have to create more DB partitions or have to rebalance existing partitions, which means the partitioning scheme changed and all existing data moved to new locations. Doing this without incurring downtime is extremely difficult. Using a scheme like directory based partitioning does make rebalancing a more palatable experience at the cost of increasing the complexity of the system and creating a new single point of failure (i.e. the lookup service/database).","title":"c. Rebalancing:"},{"location":"DesigningPastebin/","text":"Designing Pastebin Let's design a Pastebin like web service, where users can store plain text. Users of the service will enter a piece of text and get a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy 1. What is Pastebin? Pastebin like services enable users to store plain text or images over the network (typically the Internet) and generate unique URLs to access the uploaded data. Such services are also used to share data over the network quickly, as users would just need to pass the URL to let other users see it. If you haven\u2019t used pastebin.com before, please try creating a new \u2018Paste\u2019 there and spend some time going through the different options their service offers. This will help you a lot in understanding this chapter. 2. Requirements and Goals of the System Our Pastebin service should meet the following requirements: Functional Requirements: Users should be able to upload or \u201cpaste\u201d their data and get a unique URL to access it. Users will only be able to upload text. Data and links will expire after a specific timespan automatically; users should also be able to specify 4. expiration time. Users should optionally be able to pick a custom alias for their paste. Non-Functional Requirements: The system should be highly reliable, any data uploaded should not be lost. The system should be highly available. This is required because if our service is down, users will not be able to access their Pastes. Users should be able to access their Pastes in real-time with minimum latency. Paste links should not be guessable (not predictable). Extended Requirements: Analytics, e.g., how many times a paste was accessed? Our service should also be accessible through REST APIs by other services. 3. Some Design Considerations Pastebin shares some requirements with URL Shortening service, but there are some additional design considerations we should keep in mind. What should be the limit on the amount of text user can paste at a time? We can limit users not to have Pastes bigger than 10MB to stop the abuse of the service. Should we impose size limits on custom URLs? Since our service supports custom URLs, users can pick any URL that they like, but providing a custom URL is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on custom URLs, so that we have a consistent URL database. Our services will be read-heavy; there will be more read requests compared to new Pastes creation. We can assume a 5:1 ratio between read and write. Traffic estimates Pastebin services are not expected to have traffic similar to Twitter or Facebook, let\u2019s assume here that we get one million new pastes added to our system every day. This leaves us with five million reads per day. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec Storage estimates: Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs or logs. Such texts are not huge, so let\u2019s assume that each paste on average contains 10KB. At this rate, we will be storing 10GB of data per day. 1M * 10KB => 10 GB/day If we want to store this data for ten years we would need the total storage capacity of 36TB. With 1M pastes every day we will have 3.6 billion Pastes in 10 years. We need to generate and store keys to uniquely identify these pastes. If we use base64 encoding ([A-Z, a-z, 0-9, ., -]) we would need six letters strings: 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB. Bandwidth estimates: For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service. Memory estimates: We can cache some of the hot pastes that are frequently accessed. Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes Since we have 5M read requests per day, to cache 20% of these requests, we would need: 0.2 * 5M * 10KB ~= 10 GB 5. System APIs We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs to create/retrieve/delete Pastes: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: 1. api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. 2. paste_data (string): Textual data of the paste. 3. custom_url (string): Optional custom URL. 4. user_name (string): Optional user name to be used to generate URL. 5. paste_name (string): Optional name of the paste 6. expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion returns the URL through which the paste can be accessed, otherwise, it will return an error code. Similarly, we can have retrieve and delete Paste APIs: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019. 6. Database Design A few observations about the nature of the data we are storing: We need to store billions of records. Each metadata object we are storing would be small (less than 1KB). Each paste object we are storing can be of medium size (it can be a few MB). There are no relationships between records, except if we want to store which user created what Paste. Our service is read-heavy. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. Here, \u2018URlHash\u2019 is the URL equivalent of the TinyURL and \u2018ContentKey\u2019 is a reference to an external object storing the contents of the paste; we\u2019ll discuss the external storage of the paste contents later in the chapter. 7. High Level Design At a high level, we need an application layer that will serve all the read and write requests. Application layer will talk to a storage layer to store and retrieve data. We can segregate our storage layer with one database storing metadata related to each paste, users, etc., while the other storing the paste contents in some object storage (like Amazon S3). This division of data will also allow us to scale them individually. 8. Component Design a. Application layer Our application layer will process all incoming and outgoing requests. The application servers will be talking to the backend data store components to serve the requests. How to handle a write request? Upon receiving a write request, our application server will generate a six-letter random string, which would serve as the key of the paste (if the user has not provided a custom key). The application server will then store the contents of the paste and the generated key in the database. After the successful insertion, the server can return the key to the user. One possible problem here could be that the insertion fails because of a duplicate key. Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don\u2019t see failure due to the duplicate key. We should return an error to the user if the custom key they have provided is already present in our database. Another solution of the above problem could be to run a standalone Key Generation Service (KGS) that generates random six letters strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to store a new paste, we will just take one of the already generated keys and use it. This approach will make things quite simple and fast since we will not be worrying about duplications or collisions. KGS will make sure all the keys inserted in key-DB are unique. KGS can use two tables to store keys, one for keys that are not used yet and one for all the used keys. As soon as KGS gives some keys to an application server, it can move these to the used keys table. KGS can always keep some keys in memory so that whenever a server needs them, it can quickly provide them. As soon as KGS loads some keys in memory, it can move them to the used keys table, this way we can make sure each server gets unique keys. If KGS dies before using all the keys loaded in memory, we will be wasting those keys. We can ignore these keys given that we have a huge number of them. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS and whenever the primary server dies it can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This could be acceptable since we have 68B unique six letters keys, which are a lot more than we require. How does it handle a paste read request? Upon receiving a read paste request, the application service layer contacts the datastore. The datastore searches for the key, and if it is found, returns the paste\u2019s contents. Otherwise, an error code is returned. b. Datastore layer We can divide our datastore layer into two: Metadata database: We can use a relational database like MySQL or a Distributed Key-Value store like Dynamo or Cassandra. Object storage: We can store our contents in an Object Storage like Amazon\u2019s S3. Whenever we feel like hitting our full capacity on content storage, we can easily increase it by adding more servers. 9. Purging or DB Cleanup Please see Designing a URL Shortening service. 10. Data Partitioning and Replication Please see Designing a URL Shortening service. 11. Cache and Load Balancer Please see Designing a URL Shortening service. 12. Security and Permissions Please see Designing a URL Shortening service.","title":"Designing Pastebin"},{"location":"DesigningPastebin/#designing-pastebin","text":"Let's design a Pastebin like web service, where users can store plain text. Users of the service will enter a piece of text and get a randomly generated URL to access it. Similar Services: pastebin.com, pasted.co, chopapp.com Difficulty Level: Easy","title":"Designing Pastebin"},{"location":"DesigningPastebin/#1-what-is-pastebin","text":"Pastebin like services enable users to store plain text or images over the network (typically the Internet) and generate unique URLs to access the uploaded data. Such services are also used to share data over the network quickly, as users would just need to pass the URL to let other users see it. If you haven\u2019t used pastebin.com before, please try creating a new \u2018Paste\u2019 there and spend some time going through the different options their service offers. This will help you a lot in understanding this chapter.","title":"1. What is Pastebin?"},{"location":"DesigningPastebin/#2-requirements-and-goals-of-the-system","text":"Our Pastebin service should meet the following requirements:","title":"2. Requirements and Goals of the System"},{"location":"DesigningPastebin/#functional-requirements","text":"Users should be able to upload or \u201cpaste\u201d their data and get a unique URL to access it. Users will only be able to upload text. Data and links will expire after a specific timespan automatically; users should also be able to specify 4. expiration time. Users should optionally be able to pick a custom alias for their paste.","title":"Functional Requirements:"},{"location":"DesigningPastebin/#non-functional-requirements","text":"The system should be highly reliable, any data uploaded should not be lost. The system should be highly available. This is required because if our service is down, users will not be able to access their Pastes. Users should be able to access their Pastes in real-time with minimum latency. Paste links should not be guessable (not predictable).","title":"Non-Functional Requirements:"},{"location":"DesigningPastebin/#extended-requirements","text":"Analytics, e.g., how many times a paste was accessed? Our service should also be accessible through REST APIs by other services.","title":"Extended Requirements:"},{"location":"DesigningPastebin/#3-some-design-considerations","text":"Pastebin shares some requirements with URL Shortening service, but there are some additional design considerations we should keep in mind. What should be the limit on the amount of text user can paste at a time? We can limit users not to have Pastes bigger than 10MB to stop the abuse of the service. Should we impose size limits on custom URLs? Since our service supports custom URLs, users can pick any URL that they like, but providing a custom URL is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on custom URLs, so that we have a consistent URL database. Our services will be read-heavy; there will be more read requests compared to new Pastes creation. We can assume a 5:1 ratio between read and write.","title":"3. Some Design Considerations"},{"location":"DesigningPastebin/#traffic-estimates","text":"Pastebin services are not expected to have traffic similar to Twitter or Facebook, let\u2019s assume here that we get one million new pastes added to our system every day. This leaves us with five million reads per day. New Pastes per second: 1M / (24 hours * 3600 seconds) ~= 12 pastes/sec Paste reads per second: 5M / (24 hours * 3600 seconds) ~= 58 reads/sec","title":"Traffic estimates"},{"location":"DesigningPastebin/#storage-estimates","text":"Users can upload maximum 10MB of data; commonly Pastebin like services are used to share source code, configs or logs. Such texts are not huge, so let\u2019s assume that each paste on average contains 10KB. At this rate, we will be storing 10GB of data per day. 1M * 10KB => 10 GB/day If we want to store this data for ten years we would need the total storage capacity of 36TB. With 1M pastes every day we will have 3.6 billion Pastes in 10 years. We need to generate and store keys to uniquely identify these pastes. If we use base64 encoding ([A-Z, a-z, 0-9, ., -]) we would need six letters strings: 64^6 ~= 68.7 billion unique strings If it takes one byte to store one character, total size required to store 3.6B keys would be: 3.6B * 6 => 22 GB 22GB is negligible compared to 36TB. To keep some margin, we will assume a 70% capacity model (meaning we don\u2019t want to use more than 70% of our total storage capacity at any point), which raises our storage needs to 51.4TB.","title":"Storage estimates:"},{"location":"DesigningPastebin/#bandwidth-estimates","text":"For write requests, we expect 12 new pastes per second, resulting in 120KB of ingress per second. 12 * 10KB => 120 KB/s As for the read request, we expect 58 requests per second. Therefore, total data egress (sent to users) will be 0.6 MB/s. 58 * 10KB => 0.6 MB/s Although total ingress and egress are not big, we should keep these numbers in mind while designing our service.","title":"Bandwidth estimates:"},{"location":"DesigningPastebin/#memory-estimates","text":"We can cache some of the hot pastes that are frequently accessed. Following the 80-20 rule, meaning 20% of hot pastes generate 80% of traffic, we would like to cache these 20% pastes Since we have 5M read requests per day, to cache 20% of these requests, we would need: 0.2 * 5M * 10KB ~= 10 GB","title":"Memory estimates:"},{"location":"DesigningPastebin/#5-system-apis","text":"We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs to create/retrieve/delete Pastes: addPaste(api_dev_key, paste_data, custom_url=None user_name=None, paste_name=None, expire_date=None) Parameters: 1. api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. 2. paste_data (string): Textual data of the paste. 3. custom_url (string): Optional custom URL. 4. user_name (string): Optional user name to be used to generate URL. 5. paste_name (string): Optional name of the paste 6. expire_date (string): Optional expiration date for the paste. Returns: (string) A successful insertion returns the URL through which the paste can be accessed, otherwise, it will return an error code. Similarly, we can have retrieve and delete Paste APIs: getPaste(api_dev_key, api_paste_key) Where \u201capi_paste_key\u201d is a string representing the Paste Key of the paste to be retrieved. This API will return the textual data of the paste. deletePaste(api_dev_key, api_paste_key) A successful deletion returns \u2018true\u2019, otherwise returns \u2018false\u2019.","title":"5. System APIs"},{"location":"DesigningPastebin/#6-database-design","text":"A few observations about the nature of the data we are storing: We need to store billions of records. Each metadata object we are storing would be small (less than 1KB). Each paste object we are storing can be of medium size (it can be a few MB). There are no relationships between records, except if we want to store which user created what Paste. Our service is read-heavy. Database Schema: We would need two tables, one for storing information about the Pastes and the other for users\u2019 data. Here, \u2018URlHash\u2019 is the URL equivalent of the TinyURL and \u2018ContentKey\u2019 is a reference to an external object storing the contents of the paste; we\u2019ll discuss the external storage of the paste contents later in the chapter.","title":"6. Database Design"},{"location":"DesigningPastebin/#7-high-level-design","text":"At a high level, we need an application layer that will serve all the read and write requests. Application layer will talk to a storage layer to store and retrieve data. We can segregate our storage layer with one database storing metadata related to each paste, users, etc., while the other storing the paste contents in some object storage (like Amazon S3). This division of data will also allow us to scale them individually.","title":"7. High Level Design"},{"location":"DesigningPastebin/#8-component-design","text":"","title":"8. Component Design"},{"location":"DesigningPastebin/#a-application-layer","text":"Our application layer will process all incoming and outgoing requests. The application servers will be talking to the backend data store components to serve the requests. How to handle a write request? Upon receiving a write request, our application server will generate a six-letter random string, which would serve as the key of the paste (if the user has not provided a custom key). The application server will then store the contents of the paste and the generated key in the database. After the successful insertion, the server can return the key to the user. One possible problem here could be that the insertion fails because of a duplicate key. Since we are generating a random key, there is a possibility that the newly generated key could match an existing one. In that case, we should regenerate a new key and try again. We should keep retrying until we don\u2019t see failure due to the duplicate key. We should return an error to the user if the custom key they have provided is already present in our database. Another solution of the above problem could be to run a standalone Key Generation Service (KGS) that generates random six letters strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to store a new paste, we will just take one of the already generated keys and use it. This approach will make things quite simple and fast since we will not be worrying about duplications or collisions. KGS will make sure all the keys inserted in key-DB are unique. KGS can use two tables to store keys, one for keys that are not used yet and one for all the used keys. As soon as KGS gives some keys to an application server, it can move these to the used keys table. KGS can always keep some keys in memory so that whenever a server needs them, it can quickly provide them. As soon as KGS loads some keys in memory, it can move them to the used keys table, this way we can make sure each server gets unique keys. If KGS dies before using all the keys loaded in memory, we will be wasting those keys. We can ignore these keys given that we have a huge number of them. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS and whenever the primary server dies it can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This could be acceptable since we have 68B unique six letters keys, which are a lot more than we require. How does it handle a paste read request? Upon receiving a read paste request, the application service layer contacts the datastore. The datastore searches for the key, and if it is found, returns the paste\u2019s contents. Otherwise, an error code is returned.","title":"a. Application layer"},{"location":"DesigningPastebin/#b-datastore-layer","text":"We can divide our datastore layer into two: Metadata database: We can use a relational database like MySQL or a Distributed Key-Value store like Dynamo or Cassandra. Object storage: We can store our contents in an Object Storage like Amazon\u2019s S3. Whenever we feel like hitting our full capacity on content storage, we can easily increase it by adding more servers.","title":"b. Datastore layer"},{"location":"DesigningPastebin/#9-purging-or-db-cleanup","text":"Please see Designing a URL Shortening service.","title":"9. Purging or DB Cleanup"},{"location":"DesigningPastebin/#10-data-partitioning-and-replication","text":"Please see Designing a URL Shortening service.","title":"10. Data Partitioning and Replication"},{"location":"DesigningPastebin/#11-cache-and-load-balancer","text":"Please see Designing a URL Shortening service.","title":"11. Cache and Load Balancer"},{"location":"DesigningPastebin/#12-security-and-permissions","text":"Please see Designing a URL Shortening service.","title":"12. Security and Permissions"},{"location":"DesigningURLShorteningService/","text":"Designing a URL Shortening service like TinyURL Let's design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy Why do we need URL shortening? URL shortening is used to create shorter aliases for long URLs. We call these shortened aliases \u201cshort links.\u201d Users are redirected to the original URL when they hit these short links. Short links save a lot of space when displayed, printed, messaged, or tweeted. Additionally, users are less likely to mistype shorter URLs. For example, if we shorten this page through TinyURL: https://www.educative.io/collection/page/5668639101419520/5649050225344512/5668600916475904/ We would get: http://tinyurl.com/jlg8zpc The shortened URL is nearly one-third the size of the actual URL. The shortened URL is nearly one-third the size of the actual URL. URL shortening is used for optimizing links across devices, tracking individual links to analyze audience and campaign performance, and hiding affiliated original URLs. If you haven\u2019t used tinyurl.com before, please try creating a new shortened URL and spend some time going through the various options their service offers. This will help you a lot in understanding this chapter. Requirements and Goals of the System \ud83d\udca1 You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind. Our URL shortening system should meet the following requirements: Functional Requirements: Given a URL, our service should generate a shorter and unique alias of it. This is called a short link. This link should be short enough to be easily copied and pasted into applications. When users access a short link, our service should redirect them to the original link. Users should optionally be able to pick a custom short link for their URL. Links will expire after a standard default timespan. Users should be able to specify the expiration time. Non-Functional Requirements: The system should be highly available. This is required because, if our service is down, all the URL redirections will start failing. URL redirection should happen in real-time with minimal latency. Shortened links should not be guessable (not predictable). Extended Requirements: 1. Analytics; e.g., how many times a redirection happened? 2. Our service should also be accessible through REST APIs by other services. Capacity Estimation and Constraints Our system will be read-heavy. There will be lots of redirection requests compared to new URL shortenings. Let\u2019s assume a 100:1 ratio between read and write. Traffic estimates: Assuming, we will have 500M new URL shortenings per month, with 100:1 read/write ratio, we can expect 50B redirections during the same period: 100 * 500M => 50B What would be Queries Per Second (QPS) for our system? New URLs shortenings per second: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let\u2019s assume we store every URL shortening request (and associated shortened link) for 5 years. Since we expect to have 500M new URLs every month, the total number of objects we expect to store will be 30 billion: 500 million * 5 years * 12 months = 30 billion Let\u2019s assume that each stored object will be approximately 500 bytes (just a ballpark estimate\u2013we will dig into it later). We will need 15TB of total storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: If we want to cache some of the hot URLs that are frequently accessed, how much memory will we need to store them? If we follow the 80-20 rule, meaning 20% of URLs generate 80% of traffic, we would like to cache these 20% hot URLs. Since we have 20K requests per second, we will be getting 1.7 billion requests per day: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to note here is that since there will be a lot of duplicate requests (of the same URL), therefore, our actual memory usage will be less than 170GB. High level estimates: Assuming 500 million new URLs per month and 100:1 read:write ratio, following is the summary of the high level estimates for our service: System APIs \ud83d\udca1 Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system. We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs for creating and deleting URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? A malicious user can put us out of business by consuming all URL keys in the current design. To prevent abuse, we can limit users via their api_dev_key. Each api_dev_key can be limited to a certain number of URL creations and redirections per some time period (which may be set to a different duration per developer key). Database Design \ud83d\udca1 Defining the DB schema in the early stages of the interview would help to understand the data flow among various components and later would guide towards data partitioning. A few observations about the nature of the data we will store: 1. We need to store billions of records. 2. Each object we store is small (less than 1K). 3. There are no relationships between records\u2014other than storing which user created a URL. 4. Our service is read-heavy. Database Schema: We would need two tables: one for storing information about the URL mappings, and one for the user\u2019s data who created the short link. What kind of database should we use? Since we anticipate storing billions of rows, and we don\u2019t need to use relationships between objects \u2013 a NoSQL store like DynamoDB, Cassandra or Riak is a better choice. A NoSQL choice would also be easier to scale. Please see SQL vs NoSQL for more details. Basic System Design and Algorithm The problem we are solving here is, how to generate a short and unique key for a given URL. In the TinyURL example in Section 1, the shortened URL is \u201chttp://tinyurl.com/jlg8zpc\u201d. The last seven characters of this URL is the short key we want to generate. We\u2019ll explore two solutions here: Encoding actual URL We can compute a unique hash (e.g., MD5 or SHA256, etc.) of the given URL. The hash can then be encoded for displaying. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add \u2018+\u2019 and \u2018/\u2019 we can use Base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8, or 10 characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. If we use the MD5 algorithm as our hash function, it\u2019ll produce a 128-bit hash value. After base64 encoding, we\u2019ll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Now we only have space for 8 characters per short key, how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication, to resolve that, we can choose some other characters out of the encoding string or swap some characters. What are the different issues with our solution? We have the following couple of problems with our encoding scheme: If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable. What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design, and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Workaround for the issues: We can append an increasing sequence number to each input URL to make it unique, and then generate a hash of it. We don\u2019t need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service. Another solution could be to append user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one. Generating keys offline We can have a standalone Key Generation Service (KGS) that generates random six-letter strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to shorten a URL, we will just take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won\u2019t have to worry about duplications or collisions. KGS will make sure all the keys inserted into key-DB are unique Can concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure it doesn\u2019t get reuse. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem? Servers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table. KGS can always keep some keys in memory so that it can quickly provide them whenever a server needs them. For simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys\u2013which could be acceptable, given the huge number of keys we have. KGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server. What would be the key-DB size? With base64 encoding, we can generate 68.7B unique six letters keys. If we need one byte to store one alpha-numeric character, we can store all these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS. Whenever the primary server dies, the standby server can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This can be acceptable since we have 68B unique six-letter keys. How would we perform a key lookup? We can look up the key in our database to get the full URL. If it\u2019s present in the DB, issue an \u201cHTTP 302 Redirect\u201d status back to the browser, passing the stored URL in the \u201cLocation\u201d field of the request. If that key is not present in our system, issue an \u201cHTTP 404 Not Found\u201d status or redirect the user back to the homepage. Should we impose size limits on custom aliases? Our service supports custom aliases. Users can pick any \u2018key\u2019 they like, but providing a custom alias is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on a custom alias to ensure we have a consistent URL database. Let\u2019s assume users can specify a maximum of 16 characters per customer key (as reflected in the above database schema). Data Partitioning and Replication To scale out our DB, we need to partition it so that it can store information about billions of URLs. We need to come up with a partitioning scheme that would divide and store our data into different DB servers. Range Based Partitioning: We can store URLs in separate partitions based on the first letter of the hash key. Hence we save all the URLs starting with letter \u2018A\u2019 (and \u2018a\u2019) in one partition, save those that start with letter \u2018B\u2019 in another partition and so on. This approach is called range-based partitioning. We can even combine certain less frequently occurring letters into one database partition. We should come up with a static partitioning scheme so that we can always store/find a URL in a predictable manner. The main problem with this approach is that it can lead to unbalanced DB servers. For example, we decide to put all URLs starting with letter \u2018E\u2019 into a DB partition, but later we realize that we have too many URLs that start with the letter \u2018E\u2019. Hash-Based Partitioning: In this scheme, we take a hash of the object we are storing. We then calculate which partition to use based upon the hash. In our case, we can take the hash of the \u2018key\u2019 or the short link to determine the partition in which we store the data object. Our hashing function will randomly distribute URLs into different partitions (e.g., our hashing function can always map any \u2018key\u2019 to a number between [1\u2026256]), and this number would represent the partition in which we store our object. This approach can still lead to overloaded partitions, which can be solved by using Consistent Hashing. Cache We can cache URLs that are frequently accessed. We can use some off-the-shelf solution like Memcached, which can store full URLs with their respective hashes. The application servers, before hitting backend storage, can quickly check if the cache has the desired URL. How much cache memory should we have? We can start with 20% of daily traffic and, based on clients\u2019 usage pattern, we can adjust how many cache servers we need. As estimated above, we need 170GB memory to cache 20% of daily traffic. Since a modern-day server can have 256GB memory, we can easily fit all the cache into one machine. Alternatively, we can use a couple of smaller servers to store all these hot URLs. Which cache eviction policy would best fit our needs? When the cache is full, and we want to replace a link with a newer/hotter URL, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently used URL first. We can use a Linked Hash Map or a similar data structure to store our URLs and Hashes, which will also keep track of the URLs that have been accessed recently. To further increase the efficiency, we can replicate our caching servers to distribute the load between them. How can each cache replica be updated? Whenever there is a cache miss, our servers would be hitting a backend database. Whenever this happens, we can update the cache and pass the new entry to all the cache replicas. Each replica can update its cache by adding the new entry. If a replica already has that entry, it can simply ignore it. Load Balancer (LB) We can add a Load balancing layer at three places in our system: Between Clients and Application servers Between Application Servers and database servers Between Application Servers and Cache servers Initially, we could use a simple Round Robin approach that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is that we don\u2019t take the server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries the backend server about its load and adjusts traffic based on that. Purging or DB cleanup Should entries stick around forever or should they be purged? If a user-specified expiration time is reached, what should happen to the link? If we chose to actively search for expired links to remove them, it would put a lot of pressure on our database. Instead, we can slowly remove expired links and do a lazy cleanup. Our service will make sure that only expired links will be deleted, although some expired links can live longer but will never be returned to users. Whenever a user tries to access an expired link, we can delete the link and return an error to the user. A separate Cleanup service can run periodically to remove expired links from our storage and cache. This service should be very lightweight and can be scheduled to run only when the user traffic is expected to be low. We can have a default expiration time for each link (e.g., two years). After removing an expired link, we can put the key back in the key-DB to be reused. Should we remove links that haven\u2019t been visited in some length of time, say six months? This could be tricky. Since storage is getting cheap, we can decide to keep links forever. Telemetry How many times a short URL has been used, what were user locations, etc.? How would we store these statistics? If it is part of a DB row that gets updated on each view, what will happen when a popular URL is slammed with a large number of concurrent requests? Some statistics worth tracking: country of the visitor, date and time of access, web page that refers the click, browser, or platform from where the page was accessed. Security and Permissions Can users create private URLs or allow a particular set of users to access a URL? We can store the permission level (public/private) with each URL in the database. We can also create a separate table to store UserIDs that have permission to see a specific URL. If a user does not have permission and tries to access a URL, we can send an error (HTTP 401) back. Given that we are storing our data in a NoSQL wide-column database like Cassandra, the key for the table storing permissions would be the \u2018Hash\u2019 (or the KGS generated \u2018key\u2019). The columns will store the UserIDs of those users that have the permission to see the URL.","title":"Designing a URL Shortening service"},{"location":"DesigningURLShorteningService/#designing-a-url-shortening-service-like-tinyurl","text":"Let's design a URL shortening service like TinyURL. This service will provide short aliases redirecting to long URLs. Similar services: bit.ly, goo.gl, qlink.me, etc. Difficulty Level: Easy","title":"Designing a URL Shortening service like TinyURL"},{"location":"DesigningURLShorteningService/#why-do-we-need-url-shortening","text":"URL shortening is used to create shorter aliases for long URLs. We call these shortened aliases \u201cshort links.\u201d Users are redirected to the original URL when they hit these short links. Short links save a lot of space when displayed, printed, messaged, or tweeted. Additionally, users are less likely to mistype shorter URLs. For example, if we shorten this page through TinyURL: https://www.educative.io/collection/page/5668639101419520/5649050225344512/5668600916475904/ We would get: http://tinyurl.com/jlg8zpc The shortened URL is nearly one-third the size of the actual URL. The shortened URL is nearly one-third the size of the actual URL. URL shortening is used for optimizing links across devices, tracking individual links to analyze audience and campaign performance, and hiding affiliated original URLs. If you haven\u2019t used tinyurl.com before, please try creating a new shortened URL and spend some time going through the various options their service offers. This will help you a lot in understanding this chapter.","title":"Why do we need URL shortening?"},{"location":"DesigningURLShorteningService/#requirements-and-goals-of-the-system","text":"\ud83d\udca1 You should always clarify requirements at the beginning of the interview. Be sure to ask questions to find the exact scope of the system that the interviewer has in mind. Our URL shortening system should meet the following requirements: Functional Requirements: Given a URL, our service should generate a shorter and unique alias of it. This is called a short link. This link should be short enough to be easily copied and pasted into applications. When users access a short link, our service should redirect them to the original link. Users should optionally be able to pick a custom short link for their URL. Links will expire after a standard default timespan. Users should be able to specify the expiration time. Non-Functional Requirements: The system should be highly available. This is required because, if our service is down, all the URL redirections will start failing. URL redirection should happen in real-time with minimal latency. Shortened links should not be guessable (not predictable). Extended Requirements: 1. Analytics; e.g., how many times a redirection happened? 2. Our service should also be accessible through REST APIs by other services.","title":"Requirements and Goals of the System"},{"location":"DesigningURLShorteningService/#capacity-estimation-and-constraints","text":"Our system will be read-heavy. There will be lots of redirection requests compared to new URL shortenings. Let\u2019s assume a 100:1 ratio between read and write. Traffic estimates: Assuming, we will have 500M new URL shortenings per month, with 100:1 read/write ratio, we can expect 50B redirections during the same period: 100 * 500M => 50B What would be Queries Per Second (QPS) for our system? New URLs shortenings per second: 500 million / (30 days * 24 hours * 3600 seconds) = ~200 URLs/s Considering 100:1 read/write ratio, URLs redirections per second will be: 100 * 200 URLs/s = 20K/ Storage estimates: Let\u2019s assume we store every URL shortening request (and associated shortened link) for 5 years. Since we expect to have 500M new URLs every month, the total number of objects we expect to store will be 30 billion: 500 million * 5 years * 12 months = 30 billion Let\u2019s assume that each stored object will be approximately 500 bytes (just a ballpark estimate\u2013we will dig into it later). We will need 15TB of total storage: 30 billion * 500 bytes = 15 TB Bandwidth estimates: For write requests, since we expect 200 new URLs every second, total incoming data for our service will be 100KB per second: 200 * 500 bytes = 100 KB/s For read requests, since every second we expect ~20K URLs redirections, total outgoing data for our service would be 10MB per second: 20K * 500 bytes = ~10 MB/s Memory estimates: If we want to cache some of the hot URLs that are frequently accessed, how much memory will we need to store them? If we follow the 80-20 rule, meaning 20% of URLs generate 80% of traffic, we would like to cache these 20% hot URLs. Since we have 20K requests per second, we will be getting 1.7 billion requests per day: 20K * 3600 seconds * 24 hours = ~1.7 billion To cache 20% of these requests, we will need 170GB of memory. 0.2 * 1.7 billion * 500 bytes = ~170GB One thing to note here is that since there will be a lot of duplicate requests (of the same URL), therefore, our actual memory usage will be less than 170GB. High level estimates: Assuming 500 million new URLs per month and 100:1 read:write ratio, following is the summary of the high level estimates for our service:","title":"Capacity Estimation and Constraints"},{"location":"DesigningURLShorteningService/#system-apis","text":"\ud83d\udca1 Once we've finalized the requirements, it's always a good idea to define the system APIs. This should explicitly state what is expected from the system. We can have SOAP or REST APIs to expose the functionality of our service. Following could be the definitions of the APIs for creating and deleting URLs: createURL(api_dev_key, original_url, custom_alias=None, user_name=None, expire_date=None) Parameters: api_dev_key (string): The API developer key of a registered account. This will be used to, among other things, throttle users based on their allocated quota. original_url (string): Original URL to be shortened. custom_alias (string): Optional custom key for the URL. user_name (string): Optional user name to be used in the encoding. expire_date (string): Optional expiration date for the shortened URL. Returns: (string) A successful insertion returns the shortened URL; otherwise, it returns an error code. deleteURL(api_dev_key, url_key) Where \u201curl_key\u201d is a string representing the shortened URL to be retrieved. A successful deletion returns \u2018URL Removed\u2019. How do we detect and prevent abuse? A malicious user can put us out of business by consuming all URL keys in the current design. To prevent abuse, we can limit users via their api_dev_key. Each api_dev_key can be limited to a certain number of URL creations and redirections per some time period (which may be set to a different duration per developer key).","title":"System APIs"},{"location":"DesigningURLShorteningService/#database-design","text":"\ud83d\udca1 Defining the DB schema in the early stages of the interview would help to understand the data flow among various components and later would guide towards data partitioning. A few observations about the nature of the data we will store: 1. We need to store billions of records. 2. Each object we store is small (less than 1K). 3. There are no relationships between records\u2014other than storing which user created a URL. 4. Our service is read-heavy. Database Schema: We would need two tables: one for storing information about the URL mappings, and one for the user\u2019s data who created the short link. What kind of database should we use? Since we anticipate storing billions of rows, and we don\u2019t need to use relationships between objects \u2013 a NoSQL store like DynamoDB, Cassandra or Riak is a better choice. A NoSQL choice would also be easier to scale. Please see SQL vs NoSQL for more details.","title":"Database Design"},{"location":"DesigningURLShorteningService/#basic-system-design-and-algorithm","text":"The problem we are solving here is, how to generate a short and unique key for a given URL. In the TinyURL example in Section 1, the shortened URL is \u201chttp://tinyurl.com/jlg8zpc\u201d. The last seven characters of this URL is the short key we want to generate. We\u2019ll explore two solutions here:","title":"Basic System Design and Algorithm"},{"location":"DesigningURLShorteningService/#encoding-actual-url","text":"We can compute a unique hash (e.g., MD5 or SHA256, etc.) of the given URL. The hash can then be encoded for displaying. This encoding could be base36 ([a-z ,0-9]) or base62 ([A-Z, a-z, 0-9]) and if we add \u2018+\u2019 and \u2018/\u2019 we can use Base64 encoding. A reasonable question would be, what should be the length of the short key? 6, 8, or 10 characters? Using base64 encoding, a 6 letters long key would result in 64^6 = ~68.7 billion possible strings Using base64 encoding, an 8 letters long key would result in 64^8 = ~281 trillion possible strings With 68.7B unique strings, let\u2019s assume six letter keys would suffice for our system. If we use the MD5 algorithm as our hash function, it\u2019ll produce a 128-bit hash value. After base64 encoding, we\u2019ll get a string having more than 21 characters (since each base64 character encodes 6 bits of the hash value). Now we only have space for 8 characters per short key, how will we choose our key then? We can take the first 6 (or 8) letters for the key. This could result in key duplication, to resolve that, we can choose some other characters out of the encoding string or swap some characters. What are the different issues with our solution? We have the following couple of problems with our encoding scheme: If multiple users enter the same URL, they can get the same shortened URL, which is not acceptable. What if parts of the URL are URL-encoded? e.g., http://www.educative.io/distributed.php?id=design, and http://www.educative.io/distributed.php%3Fid%3Ddesign are identical except for the URL encoding. Workaround for the issues: We can append an increasing sequence number to each input URL to make it unique, and then generate a hash of it. We don\u2019t need to store this sequence number in the databases, though. Possible problems with this approach could be an ever-increasing sequence number. Can it overflow? Appending an increasing sequence number will also impact the performance of the service. Another solution could be to append user id (which should be unique) to the input URL. However, if the user has not signed in, we would have to ask the user to choose a uniqueness key. Even after this, if we have a conflict, we have to keep generating a key until we get a unique one.","title":"Encoding actual URL"},{"location":"DesigningURLShorteningService/#generating-keys-offline","text":"We can have a standalone Key Generation Service (KGS) that generates random six-letter strings beforehand and stores them in a database (let\u2019s call it key-DB). Whenever we want to shorten a URL, we will just take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won\u2019t have to worry about duplications or collisions. KGS will make sure all the keys inserted into key-DB are unique Can concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure it doesn\u2019t get reuse. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem? Servers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table. KGS can always keep some keys in memory so that it can quickly provide them whenever a server needs them. For simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys\u2013which could be acceptable, given the huge number of keys we have. KGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server. What would be the key-DB size? With base64 encoding, we can generate 68.7B unique six letters keys. If we need one byte to store one alpha-numeric character, we can store all these keys in: 6 (characters per key) * 68.7B (unique keys) = 412 GB. Isn\u2019t KGS a single point of failure? Yes, it is. To solve this, we can have a standby replica of KGS. Whenever the primary server dies, the standby server can take over to generate and provide keys. Can each app server cache some keys from key-DB? Yes, this can surely speed things up. Although in this case, if the application server dies before consuming all the keys, we will end up losing those keys. This can be acceptable since we have 68B unique six-letter keys. How would we perform a key lookup? We can look up the key in our database to get the full URL. If it\u2019s present in the DB, issue an \u201cHTTP 302 Redirect\u201d status back to the browser, passing the stored URL in the \u201cLocation\u201d field of the request. If that key is not present in our system, issue an \u201cHTTP 404 Not Found\u201d status or redirect the user back to the homepage. Should we impose size limits on custom aliases? Our service supports custom aliases. Users can pick any \u2018key\u2019 they like, but providing a custom alias is not mandatory. However, it is reasonable (and often desirable) to impose a size limit on a custom alias to ensure we have a consistent URL database. Let\u2019s assume users can specify a maximum of 16 characters per customer key (as reflected in the above database schema).","title":"Generating keys offline"},{"location":"DesigningURLShorteningService/#data-partitioning-and-replication","text":"To scale out our DB, we need to partition it so that it can store information about billions of URLs. We need to come up with a partitioning scheme that would divide and store our data into different DB servers.","title":"Data Partitioning and Replication"},{"location":"DesigningURLShorteningService/#range-based-partitioning","text":"We can store URLs in separate partitions based on the first letter of the hash key. Hence we save all the URLs starting with letter \u2018A\u2019 (and \u2018a\u2019) in one partition, save those that start with letter \u2018B\u2019 in another partition and so on. This approach is called range-based partitioning. We can even combine certain less frequently occurring letters into one database partition. We should come up with a static partitioning scheme so that we can always store/find a URL in a predictable manner. The main problem with this approach is that it can lead to unbalanced DB servers. For example, we decide to put all URLs starting with letter \u2018E\u2019 into a DB partition, but later we realize that we have too many URLs that start with the letter \u2018E\u2019.","title":"Range Based Partitioning:"},{"location":"DesigningURLShorteningService/#hash-based-partitioning","text":"In this scheme, we take a hash of the object we are storing. We then calculate which partition to use based upon the hash. In our case, we can take the hash of the \u2018key\u2019 or the short link to determine the partition in which we store the data object. Our hashing function will randomly distribute URLs into different partitions (e.g., our hashing function can always map any \u2018key\u2019 to a number between [1\u2026256]), and this number would represent the partition in which we store our object. This approach can still lead to overloaded partitions, which can be solved by using Consistent Hashing.","title":"Hash-Based Partitioning:"},{"location":"DesigningURLShorteningService/#cache","text":"We can cache URLs that are frequently accessed. We can use some off-the-shelf solution like Memcached, which can store full URLs with their respective hashes. The application servers, before hitting backend storage, can quickly check if the cache has the desired URL. How much cache memory should we have? We can start with 20% of daily traffic and, based on clients\u2019 usage pattern, we can adjust how many cache servers we need. As estimated above, we need 170GB memory to cache 20% of daily traffic. Since a modern-day server can have 256GB memory, we can easily fit all the cache into one machine. Alternatively, we can use a couple of smaller servers to store all these hot URLs. Which cache eviction policy would best fit our needs? When the cache is full, and we want to replace a link with a newer/hotter URL, how would we choose? Least Recently Used (LRU) can be a reasonable policy for our system. Under this policy, we discard the least recently used URL first. We can use a Linked Hash Map or a similar data structure to store our URLs and Hashes, which will also keep track of the URLs that have been accessed recently. To further increase the efficiency, we can replicate our caching servers to distribute the load between them. How can each cache replica be updated? Whenever there is a cache miss, our servers would be hitting a backend database. Whenever this happens, we can update the cache and pass the new entry to all the cache replicas. Each replica can update its cache by adding the new entry. If a replica already has that entry, it can simply ignore it.","title":"Cache"},{"location":"DesigningURLShorteningService/#load-balancer-lb","text":"We can add a Load balancing layer at three places in our system: Between Clients and Application servers Between Application Servers and database servers Between Application Servers and Cache servers Initially, we could use a simple Round Robin approach that distributes incoming requests equally among backend servers. This LB is simple to implement and does not introduce any overhead. Another benefit of this approach is that if a server is dead, LB will take it out of the rotation and will stop sending any traffic to it. A problem with Round Robin LB is that we don\u2019t take the server load into consideration. If a server is overloaded or slow, the LB will not stop sending new requests to that server. To handle this, a more intelligent LB solution can be placed that periodically queries the backend server about its load and adjusts traffic based on that.","title":"Load Balancer (LB)"},{"location":"DesigningURLShorteningService/#purging-or-db-cleanup","text":"Should entries stick around forever or should they be purged? If a user-specified expiration time is reached, what should happen to the link? If we chose to actively search for expired links to remove them, it would put a lot of pressure on our database. Instead, we can slowly remove expired links and do a lazy cleanup. Our service will make sure that only expired links will be deleted, although some expired links can live longer but will never be returned to users. Whenever a user tries to access an expired link, we can delete the link and return an error to the user. A separate Cleanup service can run periodically to remove expired links from our storage and cache. This service should be very lightweight and can be scheduled to run only when the user traffic is expected to be low. We can have a default expiration time for each link (e.g., two years). After removing an expired link, we can put the key back in the key-DB to be reused. Should we remove links that haven\u2019t been visited in some length of time, say six months? This could be tricky. Since storage is getting cheap, we can decide to keep links forever.","title":"Purging or DB cleanup"},{"location":"DesigningURLShorteningService/#telemetry","text":"How many times a short URL has been used, what were user locations, etc.? How would we store these statistics? If it is part of a DB row that gets updated on each view, what will happen when a popular URL is slammed with a large number of concurrent requests? Some statistics worth tracking: country of the visitor, date and time of access, web page that refers the click, browser, or platform from where the page was accessed.","title":"Telemetry"},{"location":"DesigningURLShorteningService/#security-and-permissions","text":"Can users create private URLs or allow a particular set of users to access a URL? We can store the permission level (public/private) with each URL in the database. We can also create a separate table to store UserIDs that have permission to see a specific URL. If a user does not have permission and tries to access a URL, we can send an error (HTTP 401) back. Given that we are storing our data in a NoSQL wide-column database like Cassandra, the key for the table storing permissions would be the \u2018Hash\u2019 (or the KGS generated \u2018key\u2019). The columns will store the UserIDs of those users that have the permission to see the URL.","title":"Security and Permissions"},{"location":"Events/","text":"Long-Polling vs WebSockets vs Server-Sent Events What is the difference between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are popular communication protocols between a client like a web browser and a web server. First, let\u2019s start with understanding what a standard HTTP web request looks like. Following are a sequence of events for regular HTTP request: The client opens a connection and requests data from the server. The server calculates the response. The server sends the response back to the client on the opened request. Ajax Polling Polling is a standard technique used by the vast majority of AJAX applications. The basic idea is that the client repeatedly polls (or requests) a server for data. The client makes a request and waits for the server to respond with data. If no data is available, an empty response is returned. The client opens a connection and requests data from the server using regular HTTP. The requested webpage sends requests to the server at regular intervals (e.g., 0.5 seconds). The server calculates the response and sends it back, just like regular HTTP traffic. The client repeats the above three steps periodically to get updates from the server. The problem with Polling is that the client has to keep asking the server for any new data. As a result, a lot of responses are empty, creating HTTP overhead. HTTP Long-Polling This is a variation of the traditional polling technique that allows the server to push information to a client whenever the data is available. With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately. That\u2019s why this technique is sometimes referred to as a \u201cHanging GET\u201d. If the server does not have any data available for the client, instead of sending an empty response, the server holds the request and waits until some data becomes available. Once the data becomes available, a full response is sent to the client. The client then immediately re-request information from the server so that the server will almost always have an available waiting request that it can use to deliver data in response to an event. The basic life cycle of an application using HTTP Long-Polling is as follows: The client makes an initial request using regular HTTP and then waits for a response. The server delays its response until an update is available or a timeout has occurred. When an update is available, the server sends a full response to the client. The client typically sends a new long-poll request, either immediately upon receiving a response or after a pause to allow an acceptable latency period. Each Long-Poll request has a timeout. The client has to reconnect periodically after the connection is closed due to timeouts. WebSockets WebSocket provides Full duplex communication channels over a single TCP connection. It provides a persistent connection between a client and a server that both parties can use to start sending data at any time. The client establishes a WebSocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the browser without being asked by the client and allowing for messages to be passed back and forth while keeping the connection open. In this way, a two-way (bi-directional) ongoing conversation can take place between a client and a server. Server-Sent Events (SSEs) Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. If the client wants to send data to the server, it would require the use of another technology/protocol to do so. Client requests data from a server using regular HTTP. The requested webpage opens a connection to the server. The server sends the data to the client whenever there\u2019s new information available. SSEs are best when we need real-time traffic from the server to the client or if the server is generating data in a loop and will be sending multiple events to the client.","title":"Long-Polling vs WebSockets vs Server-Sent Events"},{"location":"Events/#long-polling-vs-websockets-vs-server-sent-events","text":"What is the difference between Long-Polling, WebSockets, and Server-Sent Events? Long-Polling, WebSockets, and Server-Sent Events are popular communication protocols between a client like a web browser and a web server. First, let\u2019s start with understanding what a standard HTTP web request looks like. Following are a sequence of events for regular HTTP request: The client opens a connection and requests data from the server. The server calculates the response. The server sends the response back to the client on the opened request.","title":"Long-Polling vs WebSockets vs Server-Sent Events"},{"location":"Events/#ajax-polling","text":"Polling is a standard technique used by the vast majority of AJAX applications. The basic idea is that the client repeatedly polls (or requests) a server for data. The client makes a request and waits for the server to respond with data. If no data is available, an empty response is returned. The client opens a connection and requests data from the server using regular HTTP. The requested webpage sends requests to the server at regular intervals (e.g., 0.5 seconds). The server calculates the response and sends it back, just like regular HTTP traffic. The client repeats the above three steps periodically to get updates from the server. The problem with Polling is that the client has to keep asking the server for any new data. As a result, a lot of responses are empty, creating HTTP overhead.","title":"Ajax Polling"},{"location":"Events/#http-long-polling","text":"This is a variation of the traditional polling technique that allows the server to push information to a client whenever the data is available. With Long-Polling, the client requests information from the server exactly as in normal polling, but with the expectation that the server may not respond immediately. That\u2019s why this technique is sometimes referred to as a \u201cHanging GET\u201d. If the server does not have any data available for the client, instead of sending an empty response, the server holds the request and waits until some data becomes available. Once the data becomes available, a full response is sent to the client. The client then immediately re-request information from the server so that the server will almost always have an available waiting request that it can use to deliver data in response to an event. The basic life cycle of an application using HTTP Long-Polling is as follows: The client makes an initial request using regular HTTP and then waits for a response. The server delays its response until an update is available or a timeout has occurred. When an update is available, the server sends a full response to the client. The client typically sends a new long-poll request, either immediately upon receiving a response or after a pause to allow an acceptable latency period. Each Long-Poll request has a timeout. The client has to reconnect periodically after the connection is closed due to timeouts.","title":"HTTP Long-Polling"},{"location":"Events/#websockets","text":"WebSocket provides Full duplex communication channels over a single TCP connection. It provides a persistent connection between a client and a server that both parties can use to start sending data at any time. The client establishes a WebSocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server. This is made possible by providing a standardized way for the server to send content to the browser without being asked by the client and allowing for messages to be passed back and forth while keeping the connection open. In this way, a two-way (bi-directional) ongoing conversation can take place between a client and a server.","title":"WebSockets"},{"location":"Events/#server-sent-events-sses","text":"Under SSEs the client establishes a persistent and long-term connection with the server. The server uses this connection to send data to a client. If the client wants to send data to the server, it would require the use of another technology/protocol to do so. Client requests data from a server using regular HTTP. The requested webpage opens a connection to the server. The server sends the data to the client whenever there\u2019s new information available. SSEs are best when we need real-time traffic from the server to the client or if the server is generating data in a loop and will be sending multiple events to the client.","title":"Server-Sent Events (SSEs)"},{"location":"Indexes/","text":"Indexes Indexes are well known when it comes to databases. Sooner or later there comes a time when database performance is no longer satisfactory. One of the very first things you should turn to when that happens is database indexing. The goal of creating an index on a particular table in a database is to make it faster to search through the table and find the row or rows that we want. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records. Example: A library catalog A library catalog is a register that contains the list of books found in a library. The catalog is organized like a database table generally with four columns: book title, writer, subject, and date of publication. There are usually two such catalogs: one sorted by the book title and one sorted by the writer name. That way, you can either think of a writer you want to read and then look through their books or look up a specific book title you know you want to read in case you don\u2019t know the writer\u2019s name. These catalogs are like indexes for the database of books. They provide a sorted list of data that is easily searchable by relevant information. Simply saying, an index is a data structure that can be perceived as a table of contents that points us to the location where actual data lives. So when we create an index on a column of a table, we store that column and a pointer to the whole row in the index. Let\u2019s assume a table containing a list of books, the following diagram shows how an index on the \u2018Title\u2019 column looks like: Just like a traditional relational data store, we can also apply this concept to larger datasets. The trick with indexes is that we must carefully consider how users will access the data. In the case of data sets that are many terabytes in size, but have very small payloads (e.g., 1 KB), indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge, since we can\u2019t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is spread over several physical devices\u2014this means we need some way to find the correct physical location of the desired data. Indexes are the best way to do this. How do Indexes decrease write performance? An index can dramatically speed up data retrieval but may itself be large due to the additional keys, which slow down data insertion & update. When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance. This performance degradation applies to all insert, update, and delete operations for the table. For this reason, adding unnecessary indexes on tables should be avoided and indexes that are no longer used should be removed. To reiterate, adding indexes is about improving the performance of search queries. If the goal of the database is to provide a data store that is often written to and rarely read from, in that case, decreasing the performance of the more common operation, which is writing, is probably not worth the increase in performance we get from reading. For more details, see Database Indexes","title":"Indexes"},{"location":"Indexes/#indexes","text":"Indexes are well known when it comes to databases. Sooner or later there comes a time when database performance is no longer satisfactory. One of the very first things you should turn to when that happens is database indexing. The goal of creating an index on a particular table in a database is to make it faster to search through the table and find the row or rows that we want. Indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records.","title":"Indexes"},{"location":"Indexes/#example-a-library-catalog","text":"A library catalog is a register that contains the list of books found in a library. The catalog is organized like a database table generally with four columns: book title, writer, subject, and date of publication. There are usually two such catalogs: one sorted by the book title and one sorted by the writer name. That way, you can either think of a writer you want to read and then look through their books or look up a specific book title you know you want to read in case you don\u2019t know the writer\u2019s name. These catalogs are like indexes for the database of books. They provide a sorted list of data that is easily searchable by relevant information. Simply saying, an index is a data structure that can be perceived as a table of contents that points us to the location where actual data lives. So when we create an index on a column of a table, we store that column and a pointer to the whole row in the index. Let\u2019s assume a table containing a list of books, the following diagram shows how an index on the \u2018Title\u2019 column looks like: Just like a traditional relational data store, we can also apply this concept to larger datasets. The trick with indexes is that we must carefully consider how users will access the data. In the case of data sets that are many terabytes in size, but have very small payloads (e.g., 1 KB), indexes are a necessity for optimizing data access. Finding a small payload in such a large dataset can be a real challenge, since we can\u2019t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is spread over several physical devices\u2014this means we need some way to find the correct physical location of the desired data. Indexes are the best way to do this.","title":"Example: A library catalog"},{"location":"Indexes/#how-do-indexes-decrease-write-performance","text":"An index can dramatically speed up data retrieval but may itself be large due to the additional keys, which slow down data insertion & update. When adding rows or making updates to existing rows for a table with an active index, we not only have to write the data but also have to update the index. This will decrease the write performance. This performance degradation applies to all insert, update, and delete operations for the table. For this reason, adding unnecessary indexes on tables should be avoided and indexes that are no longer used should be removed. To reiterate, adding indexes is about improving the performance of search queries. If the goal of the database is to provide a data store that is often written to and rarely read from, in that case, decreasing the performance of the more common operation, which is writing, is probably not worth the increase in performance we get from reading. For more details, see Database Indexes","title":"How do Indexes decrease write performance?"},{"location":"KeyCharDistributedSystem/","text":"Key Characteristics of Distributed Systems Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them: Scalability Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly. Horizontal vs. Vertical Scaling: Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#key-characteristics-of-distributed-systems","text":"Key characteristics of a distributed system include Scalability, Reliability, Availability, Efficiency, and Manageability. Let\u2019s briefly review them:","title":"Key Characteristics of Distributed Systems"},{"location":"KeyCharDistributedSystem/#scalability","text":"Scalability is the capability of a system, process, or a network to grow and manage increased demand. Any distributed system that can continuously evolve in order to support the growing amount of work is considered to be scalable. A system may have to scale because of many reasons like increased data volume or increased amount of work, e.g., number of transactions. A scalable system would like to achieve this scaling without performance loss. Generally, the performance of a system, although designed (or claimed) to be scalable, declines with the system size due to the management or environment cost. For instance, network speed may become slower because machines tend to be far apart from one another. More generally, some tasks may not be distributed, either because of their inherent atomic nature or because of some flaw in the system design. At some point, such tasks would limit the speed-up obtained by distribution. A scalable architecture avoids this situation and attempts to balance the load on all the participating nodes evenly.","title":"Scalability"},{"location":"KeyCharDistributedSystem/#horizontal-vs-vertical-scaling","text":"Horizontal scaling means that you scale by adding more servers into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM, Storage, etc.) to an existing server. With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool; Vertical-scaling is usually limited to the capacity of a single server and scaling beyond that capacity often involves downtime and comes with an upper limit. Good examples of horizontal scaling are Cassandra and MongoDB as they both provide an easy way to scale horizontally by adding more machines to meet growing needs. Similarly, a good example of vertical scaling is MySQL as it allows for an easy way to scale vertically by switching from smaller to bigger machines. However, this process often involves downtime.","title":"Horizontal vs. Vertical Scaling:"},{"location":"LoadBalancing/","text":"Load Balancing Load Balancer (LB) is another critical component of any distributed system. It helps to spread the traffic across a cluster of servers to improve responsiveness and availability of applications, websites or databases. LB also keeps track of the status of all the resources while distributing requests. If a server is not available to take new requests or is not responding or has elevated error rate, LB will stop sending traffic to such a server. Typically a load balancer sits between the client and the server accepting incoming network and application traffic and distributing the traffic across multiple backend servers using various algorithms. By balancing application requests across multiple servers, a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure, thus improving overall application availability and responsiveness. To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add LBs at three places: Between the user and the web server Between web servers and an internal platform layer, like application servers or cache servers Between internal platform layer and database. Benefits of Load Balancing Users experience faster, uninterrupted service. Users won\u2019t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource. Service providers experience less downtime and higher throughput. Even a full server failure won\u2019t affect the end user experience as the load balancer will simply route around it to a healthy server. Load balancing makes it easier for system administrators to handle incoming requests while decreasing wait time for users. Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the smart load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions. System administrators experience fewer failed or stressed components. Instead of a single device performing a lot of work, load balancing has several devices perform a little bit of work. Load Balancing Algorithms How does the load balancer choose the backend server? Load balancers consider two factors before forwarding a request to a backend server. They will first ensure that the server they choose is actually responding appropriately to requests and then use a pre-configured algorithm to select one from the set of healthy servers. We will discuss these algorithms shortly. Health Checks Load balancers should only forward traffic to \u201chealthy\u201d backend servers. To monitor the health of a backend server, \u201chealth checks\u201d regularly attempt to connect to backend servers to ensure that servers are listening. If a server fails a health check, it is automatically removed from the pool, and traffic will not be forwarded to it until it responds to the health checks again. There is a variety of load balancing methods, which use different algorithms for different needs. 1. Least Connection Method This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers. 2. Least Response Time Method This algorithm directs traffic to the server with the fewest active connections and the lowest average response time. 3. Least Bandwidth Method This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps). 4. Round Robin Method This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections. 5. Weighted Round Robin Method The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights. 6. IP Hash Under this method, a hash of the IP address of the client is calculated to redirect the request to a server. Redundant Load Balancers The load balancer can be a single point of failure; to overcome this, a second load balancer can be connected to the first to form a cluster. Each LB monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over. Following links have some good discussion about load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing","text":"Load Balancer (LB) is another critical component of any distributed system. It helps to spread the traffic across a cluster of servers to improve responsiveness and availability of applications, websites or databases. LB also keeps track of the status of all the resources while distributing requests. If a server is not available to take new requests or is not responding or has elevated error rate, LB will stop sending traffic to such a server. Typically a load balancer sits between the client and the server accepting incoming network and application traffic and distributing the traffic across multiple backend servers using various algorithms. By balancing application requests across multiple servers, a load balancer reduces individual server load and prevents any one application server from becoming a single point of failure, thus improving overall application availability and responsiveness. To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add LBs at three places: Between the user and the web server Between web servers and an internal platform layer, like application servers or cache servers Between internal platform layer and database.","title":"Load Balancing"},{"location":"LoadBalancing/#benefits-of-load-balancing","text":"Users experience faster, uninterrupted service. Users won\u2019t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource. Service providers experience less downtime and higher throughput. Even a full server failure won\u2019t affect the end user experience as the load balancer will simply route around it to a healthy server. Load balancing makes it easier for system administrators to handle incoming requests while decreasing wait time for users. Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the smart load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions. System administrators experience fewer failed or stressed components. Instead of a single device performing a lot of work, load balancing has several devices perform a little bit of work.","title":"Benefits of Load Balancing"},{"location":"LoadBalancing/#load-balancing-algorithms","text":"","title":"Load Balancing Algorithms"},{"location":"LoadBalancing/#how-does-the-load-balancer-choose-the-backend-server","text":"Load balancers consider two factors before forwarding a request to a backend server. They will first ensure that the server they choose is actually responding appropriately to requests and then use a pre-configured algorithm to select one from the set of healthy servers. We will discuss these algorithms shortly.","title":"How does the load balancer choose the backend server?"},{"location":"LoadBalancing/#health-checks","text":"Load balancers should only forward traffic to \u201chealthy\u201d backend servers. To monitor the health of a backend server, \u201chealth checks\u201d regularly attempt to connect to backend servers to ensure that servers are listening. If a server fails a health check, it is automatically removed from the pool, and traffic will not be forwarded to it until it responds to the health checks again. There is a variety of load balancing methods, which use different algorithms for different needs.","title":"Health Checks"},{"location":"LoadBalancing/#1-least-connection-method","text":"This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers.","title":"1. Least Connection Method"},{"location":"LoadBalancing/#2-least-response-time-method","text":"This algorithm directs traffic to the server with the fewest active connections and the lowest average response time.","title":"2. Least Response Time Method"},{"location":"LoadBalancing/#3-least-bandwidth-method","text":"This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps).","title":"3. Least Bandwidth Method"},{"location":"LoadBalancing/#4-round-robin-method","text":"This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections.","title":"4. Round Robin Method"},{"location":"LoadBalancing/#5-weighted-round-robin-method","text":"The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights.","title":"5. Weighted Round Robin Method"},{"location":"LoadBalancing/#6-ip-hash","text":"Under this method, a hash of the IP address of the client is calculated to redirect the request to a server.","title":"6. IP Hash"},{"location":"LoadBalancing/#redundant-load-balancers","text":"The load balancer can be a single point of failure; to overcome this, a second load balancer can be connected to the first to form a cluster. Each LB monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over. Following links have some good discussion about load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Redundant Load Balancers"},{"location":"Proxies/","text":"Proxies A proxy server is an intermediate server between the client and the back-end server. Clients connect to proxy servers to make a request for a service like a web page, file, connection, etc. In short, a proxy server is a piece of software or hardware that acts as an intermediary for requests from clients seeking resources from other servers. Typically, proxies are used to filter requests, log requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another advantage of a proxy server is that its cache can serve a lot of requests. If multiple clients access a particular resource, the proxy server can cache it and serve it to all the clients without going to the remote server. Proxy Server Types Proxies can reside on the client\u2019s local server or anywhere between the client and the remote servers. Here are a few famous types of proxy servers: Open Proxy An open proxy is a proxy server that is accessible by any Internet user. Generally, a proxy server only allows users within a network group (i.e. a closed proxy) to store and forward Internet services such as DNS or web pages to reduce and control the bandwidth used by the group. With an open proxy, however, any user on the Internet is able to use this forwarding service. There two famous open proxy types: Anonymous Proxy - Th\u0456s proxy reve\u0430ls \u0456ts \u0456dent\u0456ty \u0430s \u0430 server but does not d\u0456sclose the \u0456n\u0456t\u0456\u0430l IP \u0430ddress. Though th\u0456s proxy server c\u0430n be d\u0456scovered e\u0430s\u0456ly \u0456t c\u0430n be benef\u0456c\u0456\u0430l for some users \u0430s \u0456t h\u0456des their IP \u0430ddress. Tr\u0430nsp\u0430rent Proxy \u2013 Th\u0456s proxy server \u0430g\u0430\u0456n \u0456dent\u0456f\u0456es \u0456tself, \u0430nd w\u0456th the support of HTTP he\u0430ders, the f\u0456rst IP \u0430ddress c\u0430n be v\u0456ewed. The m\u0430\u0456n benef\u0456t of us\u0456ng th\u0456s sort of server \u0456s \u0456ts \u0430b\u0456l\u0456ty to c\u0430che the webs\u0456tes. Reverse Proxy A reverse proxy retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the proxy server itself References Open Proxy Reverse Proxy","title":"Proxies"},{"location":"Proxies/#proxies","text":"A proxy server is an intermediate server between the client and the back-end server. Clients connect to proxy servers to make a request for a service like a web page, file, connection, etc. In short, a proxy server is a piece of software or hardware that acts as an intermediary for requests from clients seeking resources from other servers. Typically, proxies are used to filter requests, log requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Another advantage of a proxy server is that its cache can serve a lot of requests. If multiple clients access a particular resource, the proxy server can cache it and serve it to all the clients without going to the remote server.","title":"Proxies"},{"location":"Proxies/#proxy-server-types","text":"Proxies can reside on the client\u2019s local server or anywhere between the client and the remote servers. Here are a few famous types of proxy servers:","title":"Proxy Server Types"},{"location":"Proxies/#open-proxy","text":"An open proxy is a proxy server that is accessible by any Internet user. Generally, a proxy server only allows users within a network group (i.e. a closed proxy) to store and forward Internet services such as DNS or web pages to reduce and control the bandwidth used by the group. With an open proxy, however, any user on the Internet is able to use this forwarding service. There two famous open proxy types: Anonymous Proxy - Th\u0456s proxy reve\u0430ls \u0456ts \u0456dent\u0456ty \u0430s \u0430 server but does not d\u0456sclose the \u0456n\u0456t\u0456\u0430l IP \u0430ddress. Though th\u0456s proxy server c\u0430n be d\u0456scovered e\u0430s\u0456ly \u0456t c\u0430n be benef\u0456c\u0456\u0430l for some users \u0430s \u0456t h\u0456des their IP \u0430ddress. Tr\u0430nsp\u0430rent Proxy \u2013 Th\u0456s proxy server \u0430g\u0430\u0456n \u0456dent\u0456f\u0456es \u0456tself, \u0430nd w\u0456th the support of HTTP he\u0430ders, the f\u0456rst IP \u0430ddress c\u0430n be v\u0456ewed. The m\u0430\u0456n benef\u0456t of us\u0456ng th\u0456s sort of server \u0456s \u0456ts \u0430b\u0456l\u0456ty to c\u0430che the webs\u0456tes.","title":"Open Proxy"},{"location":"Proxies/#reverse-proxy","text":"A reverse proxy retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client, appearing as if they originated from the proxy server itself References Open Proxy Reverse Proxy","title":"Reverse Proxy"},{"location":"RedundancyReplication/","text":"Redundancy and Replication Redundancy is the duplication of critical components or functions of a system with the intention of increasing the reliability of the system, usually in the form of a backup or fail-safe, or to improve actual system performance. For example, if there is only one copy of a file stored on a single server, then losing that server means losing the file. Since losing data is seldom a good thing, we can create duplicate or redundant copies of the file to solve this problem. Redundancy plays a key role in removing the single points of failure in the system and provides backups if needed in a crisis. For example, if we have two instances of a service running in production and one fails, the system can failover to the other one. Replication means sharing information to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility. Replication is widely used in many database management systems (DBMS), usually with a master-slave relationship between the original and the copies. The master gets all the updates, which then ripple through to the slaves. Each slave outputs a message stating that it has received the update successfully, thus allowing the sending of subsequent updates. References Replication Redundancy Fault Tolerance","title":"Redundancy and Replication"},{"location":"RedundancyReplication/#redundancy-and-replication","text":"Redundancy is the duplication of critical components or functions of a system with the intention of increasing the reliability of the system, usually in the form of a backup or fail-safe, or to improve actual system performance. For example, if there is only one copy of a file stored on a single server, then losing that server means losing the file. Since losing data is seldom a good thing, we can create duplicate or redundant copies of the file to solve this problem. Redundancy plays a key role in removing the single points of failure in the system and provides backups if needed in a crisis. For example, if we have two instances of a service running in production and one fails, the system can failover to the other one. Replication means sharing information to ensure consistency between redundant resources, such as software or hardware components, to improve reliability, fault-tolerance, or accessibility. Replication is widely used in many database management systems (DBMS), usually with a master-slave relationship between the original and the copies. The master gets all the updates, which then ripple through to the slaves. Each slave outputs a message stating that it has received the update successfully, thus allowing the sending of subsequent updates. References Replication Redundancy Fault Tolerance","title":"Redundancy and Replication"},{"location":"SQLvsNoSQL/","text":"SQL vs. NoSQL In the world of databases, there are two main types of solutions: SQL and NoSQL (or relational databases and non-relational databases). Both of them differ in the way they were built, the kind of information they store, and the storage method they use. Relational databases are structured and have predefined schemas like phone books that store phone numbers and addresses. Non-relational databases are unstructured, distributed, and have a dynamic schema like file folders that hold everything from a person\u2019s address and phone number to their Facebook \u2018likes\u2019 and online shopping preferences. SQL Relational databases store data in rows and columns. Each row contains all the information about one entity and each column contains all the separate data points. Some of the most popular relational databases are MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB. NoSQL Following are the most common types of NoSQL: Key-Value Stores: Data is stored in an array of key-value pairs. The \u2018key\u2019 is an attribute name which is linked to a \u2018value\u2019. Well-known key-value stores include Redis, Voldemort, and Dynamo. Document Databases: In these databases, data is stored in documents (instead of rows and columns in a table) and these documents are grouped together in collections. Each document can have an entirely different structure. Document databases include the CouchDB and MongoDB. Wide-Column Databases: Instead of \u2018tables,\u2019 in columnar databases we have column families, which are containers for rows. Unlike relational databases, we don\u2019t need to know all the columns up front and each row doesn\u2019t have to have the same number of columns. Columnar databases are best suited for analyzing large datasets - big names include Cassandra and HBase. Graph Databases: These databases are used to store data whose relations are best represented in a graph. Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities). Examples of graph database include Neo4J and InfiniteGraph. High level differences between SQL and NoSQL Storage: SQL stores data in tables where each row represents an entity and each column represents a data point about that entity; for example, if we are storing a car entity in a table, different columns could be \u2018Color\u2019, \u2018Make\u2019, \u2018Model\u2019, and so on. NoSQL databases have different data storage models. The main ones are key-value, document, graph, and columnar. We will discuss differences between these databases below. Schema: In SQL, each record conforms to a fixed schema, meaning the columns must be decided and chosen before data entry and each row must have data for each column. The schema can be altered later, but it involves modifying the whole database and going offline. In NoSQL, schemas are dynamic. Columns can be added on the fly and each \u2018row\u2019 (or equivalent) doesn\u2019t have to contain data for each \u2018column.\u2019 Querying: SQL databases use SQL (structured query language) for defining and manipulating the data, which is very powerful. In a NoSQL database, queries are focused on a collection of documents. Sometimes it is also called UnQL (Unstructured Query Language). Different databases have different syntax for using UnQL. Scalability: In most common situations, SQL databases are vertically scalable, i.e., by increasing the horsepower (higher Memory, CPU, etc.) of the hardware, which can get very expensive. It is possible to scale a relational database across multiple servers, but this is a challenging and time-consuming process. On the other hand, NoSQL databases are horizontally scalable, meaning we can add more servers easily in our NoSQL database infrastructure to handle a lot of traffic. Any cheap commodity hardware or cloud instances can host NoSQL databases, thus making it a lot more cost-effective than vertical scaling. A lot of NoSQL technologies also distribute data across servers automatically. Reliability or ACID Compliancy (Atomicity, Consistency, Isolation, Durability): The vast majority of relational databases are ACID compliant. So, when it comes to data reliability and safe guarantee of performing transactions, SQL databases are still the better bet. Most of the NoSQL solutions sacrifice ACID compliance for performance and scalability. SQL VS. NoSQL - Which one to use? When it comes to database technology, there\u2019s no one-size-fits-all solution. That\u2019s why many businesses rely on both relational and non-relational databases for different needs. Even as NoSQL databases are gaining popularity for their speed and scalability, there are still situations where a highly structured SQL database may perform better; choosing the right technology hinges on the use case. Reasons to use SQL database Here are a few reasons to choose a SQL database: We need to ensure ACID compliance. ACID compliance reduces anomalies and protects the integrity of your database by prescribing exactly how transactions interact with the database. Generally, NoSQL databases sacrifice ACID compliance for scalability and processing speed, but for many e-commerce and financial applications, an ACID-compliant database remains the preferred option. Your data is structured and unchanging. If your business is not experiencing massive growth that would require more servers and if you\u2019re only working with data that is consistent, then there may be no reason to use a system designed to support a variety of data types and high traffic volume. Reasons to use NoSQL database When all the other components of our application are fast and seamless, NoSQL databases prevent data from being the bottleneck. Big data is contributing to a large success for NoSQL databases, mainly because it handles data differently than the traditional relational databases. A few popular examples of NoSQL databases are MongoDB, CouchDB, Cassandra, and HBase. Storing large volumes of data that often have little to no structure. A NoSQL database sets no limits on the types of data we can store together and allows us to add new types as the need changes. With document-based databases, you can store data in one place without having to define what \u201ctypes\u201d of data those are in advance. Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution but requires data to be easily spread across multiple servers to scale up. Using commodity (affordable, smaller) hardware on-site or in the cloud saves you the hassle of additional software and NoSQL databases like Cassandra are designed to be scaled across multiple data centers out of the box, without a lot of headaches. Rapid development. NoSQL is extremely useful for rapid development as it doesn\u2019t need to be prepped ahead of time. If you\u2019re working on quick iterations of your system which require making frequent updates to the data structure without a lot of downtime between versions, a relational database will slow you down.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql-vs-nosql","text":"In the world of databases, there are two main types of solutions: SQL and NoSQL (or relational databases and non-relational databases). Both of them differ in the way they were built, the kind of information they store, and the storage method they use. Relational databases are structured and have predefined schemas like phone books that store phone numbers and addresses. Non-relational databases are unstructured, distributed, and have a dynamic schema like file folders that hold everything from a person\u2019s address and phone number to their Facebook \u2018likes\u2019 and online shopping preferences.","title":"SQL vs. NoSQL"},{"location":"SQLvsNoSQL/#sql","text":"Relational databases store data in rows and columns. Each row contains all the information about one entity and each column contains all the separate data points. Some of the most popular relational databases are MySQL, Oracle, MS SQL Server, SQLite, Postgres, and MariaDB.","title":"SQL"},{"location":"SQLvsNoSQL/#nosql","text":"Following are the most common types of NoSQL: Key-Value Stores: Data is stored in an array of key-value pairs. The \u2018key\u2019 is an attribute name which is linked to a \u2018value\u2019. Well-known key-value stores include Redis, Voldemort, and Dynamo. Document Databases: In these databases, data is stored in documents (instead of rows and columns in a table) and these documents are grouped together in collections. Each document can have an entirely different structure. Document databases include the CouchDB and MongoDB. Wide-Column Databases: Instead of \u2018tables,\u2019 in columnar databases we have column families, which are containers for rows. Unlike relational databases, we don\u2019t need to know all the columns up front and each row doesn\u2019t have to have the same number of columns. Columnar databases are best suited for analyzing large datasets - big names include Cassandra and HBase. Graph Databases: These databases are used to store data whose relations are best represented in a graph. Data is saved in graph structures with nodes (entities), properties (information about the entities), and lines (connections between the entities). Examples of graph database include Neo4J and InfiniteGraph.","title":"NoSQL"},{"location":"SQLvsNoSQL/#high-level-differences-between-sql-and-nosql","text":"Storage: SQL stores data in tables where each row represents an entity and each column represents a data point about that entity; for example, if we are storing a car entity in a table, different columns could be \u2018Color\u2019, \u2018Make\u2019, \u2018Model\u2019, and so on. NoSQL databases have different data storage models. The main ones are key-value, document, graph, and columnar. We will discuss differences between these databases below. Schema: In SQL, each record conforms to a fixed schema, meaning the columns must be decided and chosen before data entry and each row must have data for each column. The schema can be altered later, but it involves modifying the whole database and going offline. In NoSQL, schemas are dynamic. Columns can be added on the fly and each \u2018row\u2019 (or equivalent) doesn\u2019t have to contain data for each \u2018column.\u2019 Querying: SQL databases use SQL (structured query language) for defining and manipulating the data, which is very powerful. In a NoSQL database, queries are focused on a collection of documents. Sometimes it is also called UnQL (Unstructured Query Language). Different databases have different syntax for using UnQL. Scalability: In most common situations, SQL databases are vertically scalable, i.e., by increasing the horsepower (higher Memory, CPU, etc.) of the hardware, which can get very expensive. It is possible to scale a relational database across multiple servers, but this is a challenging and time-consuming process. On the other hand, NoSQL databases are horizontally scalable, meaning we can add more servers easily in our NoSQL database infrastructure to handle a lot of traffic. Any cheap commodity hardware or cloud instances can host NoSQL databases, thus making it a lot more cost-effective than vertical scaling. A lot of NoSQL technologies also distribute data across servers automatically. Reliability or ACID Compliancy (Atomicity, Consistency, Isolation, Durability): The vast majority of relational databases are ACID compliant. So, when it comes to data reliability and safe guarantee of performing transactions, SQL databases are still the better bet. Most of the NoSQL solutions sacrifice ACID compliance for performance and scalability.","title":"High level differences between SQL and NoSQL"},{"location":"SQLvsNoSQL/#sql-vs-nosql-which-one-to-use","text":"When it comes to database technology, there\u2019s no one-size-fits-all solution. That\u2019s why many businesses rely on both relational and non-relational databases for different needs. Even as NoSQL databases are gaining popularity for their speed and scalability, there are still situations where a highly structured SQL database may perform better; choosing the right technology hinges on the use case.","title":"SQL VS. NoSQL - Which one to use?"},{"location":"SQLvsNoSQL/#reasons-to-use-sql-database","text":"Here are a few reasons to choose a SQL database: We need to ensure ACID compliance. ACID compliance reduces anomalies and protects the integrity of your database by prescribing exactly how transactions interact with the database. Generally, NoSQL databases sacrifice ACID compliance for scalability and processing speed, but for many e-commerce and financial applications, an ACID-compliant database remains the preferred option. Your data is structured and unchanging. If your business is not experiencing massive growth that would require more servers and if you\u2019re only working with data that is consistent, then there may be no reason to use a system designed to support a variety of data types and high traffic volume.","title":"Reasons to use SQL database"},{"location":"SQLvsNoSQL/#reasons-to-use-nosql-database","text":"When all the other components of our application are fast and seamless, NoSQL databases prevent data from being the bottleneck. Big data is contributing to a large success for NoSQL databases, mainly because it handles data differently than the traditional relational databases. A few popular examples of NoSQL databases are MongoDB, CouchDB, Cassandra, and HBase. Storing large volumes of data that often have little to no structure. A NoSQL database sets no limits on the types of data we can store together and allows us to add new types as the need changes. With document-based databases, you can store data in one place without having to define what \u201ctypes\u201d of data those are in advance. Making the most of cloud computing and storage. Cloud-based storage is an excellent cost-saving solution but requires data to be easily spread across multiple servers to scale up. Using commodity (affordable, smaller) hardware on-site or in the cloud saves you the hassle of additional software and NoSQL databases like Cassandra are designed to be scaled across multiple data centers out of the box, without a lot of headaches. Rapid development. NoSQL is extremely useful for rapid development as it doesn\u2019t need to be prepped ahead of time. If you\u2019re working on quick iterations of your system which require making frequent updates to the data structure without a lot of downtime between versions, a relational database will slow you down.","title":"Reasons to use NoSQL database"},{"location":"SystemDesignBasic/","text":"System Design Basics Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basic"},{"location":"SystemDesignBasic/#system-design-basics","text":"Whenever we are designing a large system, we need to consider a few things: 1. What are the different architectural pieces that can be used? 2. How do these pieces work with each other? 3. How can we best utilize these pieces: what are the right tradeoffs? Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save valuable time and resources in the future. In the following chapters, we will try to define some of the core building blocks of scalable systems. Familiarizing these concepts would greatly benefit in understanding distributed system concepts. In the next section, we will go through Consistent Hashing, CAP Theorem, Load Balancing, Caching, Data Partitioning, Indexes, Proxies, Queues, Replication, and choosing between SQL vs. NoSQL. Let\u2019s start with the Key Characteristics of Distributed Systems.","title":"System Design Basics"},{"location":"SystemDesignInterviews/","text":"System Design Interviews: A step by step guide A lot of software engineers struggle with system design interviews (SDIs) primarily because of three reasons: The unstructured nature of SDIs, where the candidates are asked to work on an open-ended design problem that doesn\u2019t have a standard answer. Candidates lack of experience in developing complex and large scale systems. Candidates did not spend enough time to prepare for SDIs. Like coding interviews, candidates who haven\u2019t put a deliberate effort to prepare for SDIs, mostly perform poorly specially at top companies like Google, Facebook, Amazon, Microsoft, etc. In these companies, candidates who do not perform above average have a limited chance to get an offer. On the other hand, a good performance always results in a better offer (higher position and salary), since it shows the candidate\u2019s ability to handle a complex system. In this course, we\u2019ll follow a step by step approach to solve multiple design problems. First, let\u2019s go through these steps: Step 1: Requirements clarifications It is always a good idea to ask questions about the exact scope of the problem we are solving. Design questions are mostly open-ended, and they don\u2019t have ONE correct answer, that\u2019s why clarifying ambiguities early in the interview becomes critical. Candidates who spend enough time to define the end goals of the system always have a better chance to be successful in the interview. Also, since we only have 35-40 minutes to design a (supposedly) large system, we should clarify what parts of the system we will be focusing on. Let\u2019s expand this with an actual example of designing a Twitter-like service. Here are some questions for designing Twitter that should be answered before moving on to the next steps: Will users of our service be able to post tweets and follow other people? Should we also design to create and display the user\u2019s timeline? Will tweets contain photos and videos? Are we focusing on the backend only or are we developing the front-end too? Will users be able to search tweets? Do we need to display hot trending topics? Will there be any push notification for new (or important) tweets? All such questions will determine how our end design will look like. Step 2: Back-of-the-envelope estimation It is always a good idea to estimate the scale of the system we\u2019re going to design. This will also help later when we will be focusing on scaling, partitioning, load balancing and caching. What scale is expected from the system (e.g., number of new tweets, number of tweet views, number of timeline generations per sec., etc.)? How much storage will we need? We will have different storage requirements if users can have photos and videos in their tweets. What network bandwidth usage are we expecting? This will be crucial in deciding how we will manage traffic and balance load between servers. Step 3: System interface definition Define what APIs are expected from the system. This will not only establish the exact contract expected from the system but will also ensure if we haven\u2019t gotten any requirements wrong. Some examples of APIs for our Twitter-like service will be: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026) Step 4: Defining data model Defining the data model in the early part of the interview will clarify how data will flow between different components of the system. Later, it will guide for data partitioning and management. The candidate should be able to identify various entities of the system, how they will interact with each other, and different aspects of data management like storage, transportation, encryption, etc. Here are some entities for our Twitter-like service: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Which database system should we use? Will NoSQL like Cassandra best fit our needs, or should we use a MySQL-like solution? What kind of block storage should we use to store photos and videos? Step 5: High-level design Draw a block diagram with 5-6 boxes representing the core components of our system. We should identify enough components that are needed to solve the actual problem from end-to-end. For Twitter, at a high-level, we will need multiple application servers to serve all the read/write requests with load balancers in front of them for traffic distributions. If we\u2019re assuming that we will have a lot more read traffic (as compared to write), we can decide to have separate servers for handling these scenarios. On the backend, we need an efficient database that can store all the tweets and can support a huge number of reads. We will also need a distributed file storage system for storing photos and videos. Step 6: Detailed design Dig deeper into two or three major components; interviewer\u2019s feedback should always guide us to what parts of the system need further discussion. We should be able to present different approaches, their pros and cons, and explain why we will prefer one approach on the other. Remember there is no single answer; the only important thing is to consider tradeoffs between different options while keeping system constraints in mind. Since we will be storing a massive amount of data, how should we partition our data to distribute it to multiple databases? Should we try to store all the data of a user on the same database? What issue could it cause? How will we handle hot users who tweet a lot or follow lots of people? Since users\u2019 timeline will contain the most recent (and relevant) tweets, should we try to store our data in such a way that is optimized for scanning the latest tweets? How much and at which layer should we introduce cache to speed things up? What components need better load balancing? Step 7: Identifying and resolving bottlenecks Try to discuss as many bottlenecks as possible and different approaches to mitigate them. Is there any single point of failure in our system? What are we doing to mitigate it? Do we have enough replicas of the data so that if we lose a few servers, we can still serve our users? Similarly, do we have enough copies of different services running such that a few failures will not cause total system shutdown? How are we monitoring the performance of our service? Do we get alerts whenever critical components fail or their performance degrades? Summary In short, preparation and being organized during the interview are the keys to be successful in system design interviews. The steps mentioned above should guide you to remain on track and cover all the different aspects while designing a system. Let\u2019s apply the above guidelines to design a few systems that are asked in SDIs.","title":"System Design Interviews"},{"location":"SystemDesignInterviews/#system-design-interviews-a-step-by-step-guide","text":"A lot of software engineers struggle with system design interviews (SDIs) primarily because of three reasons: The unstructured nature of SDIs, where the candidates are asked to work on an open-ended design problem that doesn\u2019t have a standard answer. Candidates lack of experience in developing complex and large scale systems. Candidates did not spend enough time to prepare for SDIs. Like coding interviews, candidates who haven\u2019t put a deliberate effort to prepare for SDIs, mostly perform poorly specially at top companies like Google, Facebook, Amazon, Microsoft, etc. In these companies, candidates who do not perform above average have a limited chance to get an offer. On the other hand, a good performance always results in a better offer (higher position and salary), since it shows the candidate\u2019s ability to handle a complex system. In this course, we\u2019ll follow a step by step approach to solve multiple design problems. First, let\u2019s go through these steps:","title":"System Design Interviews: A step by step guide"},{"location":"SystemDesignInterviews/#step-1-requirements-clarifications","text":"It is always a good idea to ask questions about the exact scope of the problem we are solving. Design questions are mostly open-ended, and they don\u2019t have ONE correct answer, that\u2019s why clarifying ambiguities early in the interview becomes critical. Candidates who spend enough time to define the end goals of the system always have a better chance to be successful in the interview. Also, since we only have 35-40 minutes to design a (supposedly) large system, we should clarify what parts of the system we will be focusing on. Let\u2019s expand this with an actual example of designing a Twitter-like service. Here are some questions for designing Twitter that should be answered before moving on to the next steps: Will users of our service be able to post tweets and follow other people? Should we also design to create and display the user\u2019s timeline? Will tweets contain photos and videos? Are we focusing on the backend only or are we developing the front-end too? Will users be able to search tweets? Do we need to display hot trending topics? Will there be any push notification for new (or important) tweets? All such questions will determine how our end design will look like.","title":"Step 1: Requirements clarifications"},{"location":"SystemDesignInterviews/#step-2-back-of-the-envelope-estimation","text":"It is always a good idea to estimate the scale of the system we\u2019re going to design. This will also help later when we will be focusing on scaling, partitioning, load balancing and caching. What scale is expected from the system (e.g., number of new tweets, number of tweet views, number of timeline generations per sec., etc.)? How much storage will we need? We will have different storage requirements if users can have photos and videos in their tweets. What network bandwidth usage are we expecting? This will be crucial in deciding how we will manage traffic and balance load between servers.","title":"Step 2: Back-of-the-envelope estimation"},{"location":"SystemDesignInterviews/#step-3-system-interface-definition","text":"Define what APIs are expected from the system. This will not only establish the exact contract expected from the system but will also ensure if we haven\u2019t gotten any requirements wrong. Some examples of APIs for our Twitter-like service will be: postTweet(user_id, tweet_data, tweet_location, user_location, timestamp, \u2026) generateTimeline(user_id, current_time, user_location, \u2026) markTweetFavorite(user_id, tweet_id, timestamp, \u2026)","title":"Step 3: System interface definition"},{"location":"SystemDesignInterviews/#step-4-defining-data-model","text":"Defining the data model in the early part of the interview will clarify how data will flow between different components of the system. Later, it will guide for data partitioning and management. The candidate should be able to identify various entities of the system, how they will interact with each other, and different aspects of data management like storage, transportation, encryption, etc. Here are some entities for our Twitter-like service: User: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollowo: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp Which database system should we use? Will NoSQL like Cassandra best fit our needs, or should we use a MySQL-like solution? What kind of block storage should we use to store photos and videos?","title":"Step 4: Defining data model"},{"location":"SystemDesignInterviews/#step-5-high-level-design","text":"Draw a block diagram with 5-6 boxes representing the core components of our system. We should identify enough components that are needed to solve the actual problem from end-to-end. For Twitter, at a high-level, we will need multiple application servers to serve all the read/write requests with load balancers in front of them for traffic distributions. If we\u2019re assuming that we will have a lot more read traffic (as compared to write), we can decide to have separate servers for handling these scenarios. On the backend, we need an efficient database that can store all the tweets and can support a huge number of reads. We will also need a distributed file storage system for storing photos and videos.","title":"Step 5: High-level design"},{"location":"SystemDesignInterviews/#step-6-detailed-design","text":"Dig deeper into two or three major components; interviewer\u2019s feedback should always guide us to what parts of the system need further discussion. We should be able to present different approaches, their pros and cons, and explain why we will prefer one approach on the other. Remember there is no single answer; the only important thing is to consider tradeoffs between different options while keeping system constraints in mind. Since we will be storing a massive amount of data, how should we partition our data to distribute it to multiple databases? Should we try to store all the data of a user on the same database? What issue could it cause? How will we handle hot users who tweet a lot or follow lots of people? Since users\u2019 timeline will contain the most recent (and relevant) tweets, should we try to store our data in such a way that is optimized for scanning the latest tweets? How much and at which layer should we introduce cache to speed things up? What components need better load balancing?","title":"Step 6: Detailed design"},{"location":"SystemDesignInterviews/#step-7-identifying-and-resolving-bottlenecks","text":"Try to discuss as many bottlenecks as possible and different approaches to mitigate them. Is there any single point of failure in our system? What are we doing to mitigate it? Do we have enough replicas of the data so that if we lose a few servers, we can still serve our users? Similarly, do we have enough copies of different services running such that a few failures will not cause total system shutdown? How are we monitoring the performance of our service? Do we get alerts whenever critical components fail or their performance degrades?","title":"Step 7: Identifying and resolving bottlenecks"},{"location":"SystemDesignInterviews/#summary","text":"In short, preparation and being organized during the interview are the keys to be successful in system design interviews. The steps mentioned above should guide you to remain on track and cover all the different aspects while designing a system. Let\u2019s apply the above guidelines to design a few systems that are asked in SDIs.","title":"Summary"},{"location":"about/","text":"copyright@jayaemekar","title":"About"}]}