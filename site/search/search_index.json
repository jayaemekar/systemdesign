{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is System Design? System design is the process of defining the architecture, interfaces, and data for a system that satisfies specific requirements. Whenever we are designing a large system, we need to consider a few things: What are the different architectural pieces that can be used? How do these pieces work with each other? How can we best utilize these pieces: what are the right tradeoffs? System design is a broad topic. There is a vast amount of resources scattered throughout the web on system design principles. In addition to coding interviews, system design is a required component of the technical interview process at many tech companies. System design requires a systematic approach to building and engineering systems. A good system design requires you to think about everything in an infrastructure, from the hardware and software, all the way down to the data and how it\u2019s stored.","title":"Home"},{"location":"#what-is-system-design","text":"System design is the process of defining the architecture, interfaces, and data for a system that satisfies specific requirements. Whenever we are designing a large system, we need to consider a few things: What are the different architectural pieces that can be used? How do these pieces work with each other? How can we best utilize these pieces: what are the right tradeoffs? System design is a broad topic. There is a vast amount of resources scattered throughout the web on system design principles. In addition to coding interviews, system design is a required component of the technical interview process at many tech companies. System design requires a systematic approach to building and engineering systems. A good system design requires you to think about everything in an infrastructure, from the hardware and software, all the way down to the data and how it\u2019s stored.","title":"What is System Design?"},{"location":"BasicTerminologies/","text":"Basic Terminologies We try to explain some of the terminologies in simple words. Lookup wiki for a more formal definition. Reliability Reliability is the probability a system will fail in a given period. A distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail Replication Replication refers to frequently copying the data across multiple machines. Post replication, multiple copies of the data exists across machines. This might help in case one or more of the machines die due to some failure. Consistency Assuming you have a storage system which has more than one machine, consistency implies that the data is same across the cluster, so you can read or write to/from any node and get the same data. Eventual consistency : Exactly what the name suggests. In a cluster, if multiple machines store the same data, an eventual consistent model implies that all machines will have the same data eventually. Its possible that at a given instance, those machines have different versions of the same data ( temporarily inconsistent ) but they will eventually reach a state where they have the same data. Availability It is a simple measure of the percentage of time that a system, service, or a machine remains operational under normal conditions. In the context of a database cluster, Availability refers to the ability to always respond to queries ( read or write ) irrespective of nodes going down. Reliability Vs. Availability If a system is reliable, it is available. However, if it is available, it is not necessarily reliable. Partition Tolerance In the context of a database cluster, cluster continues to function even if there is a \u201cpartition\u201d (communications break) between two nodes (both nodes are up, but can\u2019t communicate). Scaling Scalability is the capability of a system, process, or a network to grow and manage increased demand. There are two types of scaling, Horizontal and Vertical Scaling In simple terms, to scale horizontally is adding more servers. To scale vertically is to increase the resources of the server ( RAM, CPU, storage, etc. ). Sharding Sharding is a method for distributing data across multiple machines. With most huge systems, data does not fit on a single machine. In such cases, sharding refers to splitting the very large database into smaller, faster and more manageable parts called data shards.","title":"Basic Terminologies"},{"location":"BasicTerminologies/#basic-terminologies","text":"We try to explain some of the terminologies in simple words. Lookup wiki for a more formal definition.","title":"Basic Terminologies"},{"location":"BasicTerminologies/#reliability","text":"Reliability is the probability a system will fail in a given period. A distributed system is considered reliable if it keeps delivering its services even when one or several of its software or hardware components fail","title":"Reliability"},{"location":"BasicTerminologies/#replication","text":"Replication refers to frequently copying the data across multiple machines. Post replication, multiple copies of the data exists across machines. This might help in case one or more of the machines die due to some failure.","title":"Replication"},{"location":"BasicTerminologies/#consistency","text":"Assuming you have a storage system which has more than one machine, consistency implies that the data is same across the cluster, so you can read or write to/from any node and get the same data. Eventual consistency : Exactly what the name suggests. In a cluster, if multiple machines store the same data, an eventual consistent model implies that all machines will have the same data eventually. Its possible that at a given instance, those machines have different versions of the same data ( temporarily inconsistent ) but they will eventually reach a state where they have the same data.","title":"Consistency"},{"location":"BasicTerminologies/#availability","text":"It is a simple measure of the percentage of time that a system, service, or a machine remains operational under normal conditions. In the context of a database cluster, Availability refers to the ability to always respond to queries ( read or write ) irrespective of nodes going down. Reliability Vs. Availability If a system is reliable, it is available. However, if it is available, it is not necessarily reliable.","title":"Availability"},{"location":"BasicTerminologies/#partition-tolerance","text":"In the context of a database cluster, cluster continues to function even if there is a \u201cpartition\u201d (communications break) between two nodes (both nodes are up, but can\u2019t communicate).","title":"Partition Tolerance"},{"location":"BasicTerminologies/#scaling","text":"Scalability is the capability of a system, process, or a network to grow and manage increased demand. There are two types of scaling, Horizontal and Vertical Scaling In simple terms, to scale horizontally is adding more servers. To scale vertically is to increase the resources of the server ( RAM, CPU, storage, etc. ).","title":"Scaling"},{"location":"BasicTerminologies/#sharding","text":"Sharding is a method for distributing data across multiple machines. With most huge systems, data does not fit on a single machine. In such cases, sharding refers to splitting the very large database into smaller, faster and more manageable parts called data shards.","title":"Sharding"},{"location":"CAPTheorem/","text":"CAP Theorem CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of the following guarantees (CAP): Consistency, Availability, and Partition tolerance. When we design a distributed system, trading off among CAP is almost the first thing we want to consider. CAP theorem says while designing a distributed system we can pick only two of the following three options: Consistency: All nodes see the same data at the same time. Consistency is achieved by updating several nodes before allowing further reads. Availability: Every request gets a response on success/failure. Availability is achieved by replicating the data across different servers. Partition tolerance: The system continues to work despite message loss or partial failure. A system that is partition-tolerant can sustain any amount of network failure that doesn\u2019t result in a failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages. We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures. We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.","title":"CAP Theorem"},{"location":"CAPTheorem/#cap-theorem","text":"CAP theorem states that it is impossible for a distributed software system to simultaneously provide more than two out of three of the following guarantees (CAP): Consistency, Availability, and Partition tolerance. When we design a distributed system, trading off among CAP is almost the first thing we want to consider. CAP theorem says while designing a distributed system we can pick only two of the following three options: Consistency: All nodes see the same data at the same time. Consistency is achieved by updating several nodes before allowing further reads. Availability: Every request gets a response on success/failure. Availability is achieved by replicating the data across different servers. Partition tolerance: The system continues to work despite message loss or partial failure. A system that is partition-tolerant can sustain any amount of network failure that doesn\u2019t result in a failure of the entire network. Data is sufficiently replicated across combinations of nodes and networks to keep the system up through intermittent outages. We cannot build a general data store that is continually available, sequentially consistent, and tolerant to any partition failures. We can only build a system that has any two of these three properties. Because, to be consistent, all nodes should see the same set of updates in the same order. But if the network suffers a partition, updates in one partition might not make it to the other partitions before a client reads from the out-of-date partition after having read from the up-to-date one. The only thing that can be done to cope with this possibility is to stop serving requests from the out-of-date partition, but then the service is no longer 100% available.","title":"CAP Theorem"},{"location":"Caching/","text":"Caching What is Cache? A cache is like short-term memory which has a limited amount of space. It is typically faster than the original data source. Caching consists of precalculating results (e.g. the number of visits from each referring domain for the previous day) pre-generating expensive indexes (e.g. suggested stories based on a user\u2019s click history) storing copies of frequently accessed data in a faster backend (e.g. Memcache instead of PostgreSQL. Caches in different layers 1 Client-side Use case: Accelerate retrieval of web content from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Accelerate retrieval of web content from web/app servers. Manage Web Sessions (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Accelerate application performance and data access Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: Basically it keeps a cache directly on the Application server. Each time a request is made to the service, the node will quickly return local, cached data if it exists. If not, the requesting node will query the data by going to network storage such as a database. When the application server is expanded to many nodes, we may face the following issues: The load balancer randomly distributes requests across the nodes. The same request can go to different nodes, increase cache misses. Extra storage since the same data will be stored in two or more different nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Reduce latency associated with database query requests Tech: Database buffers, Key/Value data stores Solutions: The database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance, can also use Redis, Memcached 6 Content Distribution Network (CDN) Use case: Take the burden of serving static media off of your application servers and provide a geographic distribution. Solutions: Amazon CloudFront, Akamai Note: If the system is not large enough for CDN, it can be built like this: Serving static media off a separate subdomain using a lightweight HTTP server (e.g. Nginx, Apache). Cutover the DNS from this subdomain to a CDN layer. 7 Other Cache CPU Cache: Small memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop, and server microprocessors may have as many as six types of cache (between levels and functions) GPU Cache Disk Cache: While CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel Cache Invalidation If the data is modified in the database, it should be invalidated in the cache, if not, this can cause inconsistent application behavior. There are majorly three kinds of caching systems: Write-through cache, Write-around cache, Write-back cache. Solving this problem is known as cache invalidation; there are three main schemes that are used: Write-through cache: Under this policy, data is written into the cache and the corresponding database at the same time. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions. Although, write through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations. Write-around cache: This technique is similar to write through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a \u201ccache miss\u201d and must be read from slower back-end storage and experience higher latency. Write-back cache: Under this policy, data is written to cache alone and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low latency and high throughput for write-intensive applications, however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache. Cache eviction policies Following are some of the most common cache eviction policies: First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before. Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before. Least Recently Used (LRU): Discards the least recently used items first. Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first. Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first. Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. Following links have some good discussion about caching: Cache Introduction to architecting systems","title":"Caching"},{"location":"Caching/#caching","text":"","title":"Caching"},{"location":"Caching/#what-is-cache","text":"A cache is like short-term memory which has a limited amount of space. It is typically faster than the original data source. Caching consists of precalculating results (e.g. the number of visits from each referring domain for the previous day) pre-generating expensive indexes (e.g. suggested stories based on a user\u2019s click history) storing copies of frequently accessed data in a faster backend (e.g. Memcache instead of PostgreSQL.","title":"What is Cache?"},{"location":"Caching/#caches-in-different-layers","text":"1 Client-side Use case: Accelerate retrieval of web content from websites (browser or device) Tech: HTTP Cache Headers, Browsers Solutions: Browser Specific 2 DNS Use case: Domain to IP Resolution Tech: DNS Servers Solutions: Amazon Route 53 3 Web Server Use case: Accelerate retrieval of web content from web/app servers. Manage Web Sessions (server-side) Tech: HTTP Cache Headers, CDNs, Reverse Proxies, Web Accelerators, Key/Value Stores Solutions: Amazon CloudFront, ElastiCache for Redis, ElastiCache for Memcached, Partner Solutions 4 Application Use case: Accelerate application performance and data access Tech: Key/Value data stores, Local caches Solutions: Redis, Memcached Note: Basically it keeps a cache directly on the Application server. Each time a request is made to the service, the node will quickly return local, cached data if it exists. If not, the requesting node will query the data by going to network storage such as a database. When the application server is expanded to many nodes, we may face the following issues: The load balancer randomly distributes requests across the nodes. The same request can go to different nodes, increase cache misses. Extra storage since the same data will be stored in two or more different nodes. Solutions for the issues: Global caches Distributed caches 5 Database Use case: Reduce latency associated with database query requests Tech: Database buffers, Key/Value data stores Solutions: The database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance, can also use Redis, Memcached 6 Content Distribution Network (CDN) Use case: Take the burden of serving static media off of your application servers and provide a geographic distribution. Solutions: Amazon CloudFront, Akamai Note: If the system is not large enough for CDN, it can be built like this: Serving static media off a separate subdomain using a lightweight HTTP server (e.g. Nginx, Apache). Cutover the DNS from this subdomain to a CDN layer. 7 Other Cache CPU Cache: Small memories on or close to the CPU can operate faster than the much larger main memory. Most CPUs since the 1980s have used one or more caches, sometimes in cascaded levels; modern high-end embedded, desktop, and server microprocessors may have as many as six types of cache (between levels and functions) GPU Cache Disk Cache: While CPU caches are generally managed entirely by hardware, a variety of software manages other caches. The page cache in main memory, which is an example of disk cache, is managed by the operating system kernel","title":"Caches in different layers"},{"location":"Caching/#cache-invalidation","text":"If the data is modified in the database, it should be invalidated in the cache, if not, this can cause inconsistent application behavior. There are majorly three kinds of caching systems: Write-through cache, Write-around cache, Write-back cache. Solving this problem is known as cache invalidation; there are three main schemes that are used: Write-through cache: Under this policy, data is written into the cache and the corresponding database at the same time. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Also, this scheme ensures that nothing will get lost in case of a crash, power failure, or other system disruptions. Although, write through minimizes the risk of data loss, since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations. Write-around cache: This technique is similar to write through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a \u201ccache miss\u201d and must be read from slower back-end storage and experience higher latency. Write-back cache: Under this policy, data is written to cache alone and completion is immediately confirmed to the client. The write to the permanent storage is done after specified intervals or under certain conditions. This results in low latency and high throughput for write-intensive applications, however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache.","title":"Cache Invalidation"},{"location":"Caching/#cache-eviction-policies","text":"Following are some of the most common cache eviction policies: First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before. Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before. Least Recently Used (LRU): Discards the least recently used items first. Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first. Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first. Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary. Following links have some good discussion about caching: Cache Introduction to architecting systems","title":"Cache eviction policies"},{"location":"DataPartitioning/","text":"Data Partitioning Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers. Partitioning Methods There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications. Horizontal partitioning (often called sharding). In this strategy, each partition is a separate data store, but all partitions have the same schema. Each partition is known as a shard and holds a specific subset of the data, such as all the orders for a specific set of customers. Vertical Partitioning In this strategy, each partition holds a subset of the fields for items in the data store. The fields are divided according to their pattern of use. For example, frequently accessed fields might be placed in one vertical partition and less frequently accessed fields in another. Functional partitioning In this strategy, data is aggregated according to how it is used by each bounded context in the system. For example, an e-commerce system might store invoice data in one partition and product inventory data in another. Directory Based Partitioning A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application. Partitioning Criteria a. Key or Hash-based partitioning: we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. b. List partitioning: Each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries. c. Round-robin partitioning: This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n). d. Composite partitioning: Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed.","title":"Data Partitioning"},{"location":"DataPartitioning/#data-partitioning","text":"Data partitioning is a technique to break up a big database (DB) into many smaller parts. It is the process of splitting up a DB/table across multiple machines to improve the manageability, performance, availability, and load balancing of an application. The justification for data partitioning is that, after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers.","title":"Data Partitioning"},{"location":"DataPartitioning/#partitioning-methods","text":"There are many different schemes one could use to decide how to break up an application database into multiple smaller DBs. Below are three of the most popular schemes used by various large scale applications. Horizontal partitioning (often called sharding). In this strategy, each partition is a separate data store, but all partitions have the same schema. Each partition is known as a shard and holds a specific subset of the data, such as all the orders for a specific set of customers. Vertical Partitioning In this strategy, each partition holds a subset of the fields for items in the data store. The fields are divided according to their pattern of use. For example, frequently accessed fields might be placed in one vertical partition and less frequently accessed fields in another. Functional partitioning In this strategy, data is aggregated according to how it is used by each bounded context in the system. For example, an e-commerce system might store invoice data in one partition and product inventory data in another. Directory Based Partitioning A loosely coupled approach to work around issues mentioned in the above schemes is to create a lookup service which knows your current partitioning scheme and abstracts it away from the DB access code. So, to find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its DB server. This loosely coupled approach means we can perform tasks like adding servers to the DB pool or changing our partitioning scheme without having an impact on the application.","title":"Partitioning Methods"},{"location":"DataPartitioning/#partitioning-criteria","text":"a. Key or Hash-based partitioning: we apply a hash function to some key attributes of the entity we are storing; that yields the partition number. b. List partitioning: Each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there. For example, we can decide all users living in Iceland, Norway, Sweden, Finland, or Denmark will be stored in a partition for the Nordic countries. c. Round-robin partitioning: This is a very simple strategy that ensures uniform data distribution. With \u2018n\u2019 partitions, the \u2018i\u2019 tuple is assigned to partition (i mod n). d. Composite partitioning: Under this scheme, we combine any of the above partitioning schemes to devise a new scheme. For example, first applying a list partitioning scheme and then a hash based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the key space to a size that can be listed.","title":"Partitioning Criteria"},{"location":"DesignCache/","text":"Design a distributed key value caching system, like Memcached or Redis. Features: This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote. Q: What is the amount of data that we need to cache? A: Let's assume we are looking to cache on the scale of Google or Twitter. The total size of the cache would be a few TBs. Q: What should be the eviction strategy? A: It is possible that we might get entries when we would not have space to accommodate new entries. In such cases, we would need to remove one or more entries to make space for the new entry. Which entry should we evict? There are multiple strategies possible. Following are a few standard ones : FIFO ( First in, first out ) : The entry that was first added to the queue would be evicted/removed first. In other words, Elements are evicted in the same order as they come in. This is the simplest to implement and performs well when either, access pattern is completely random ( All elements are equally probably to be accessed ) OR if the use of an element makes it less likely to be used in the future. LRU ( Least Recently Used ) : Default. As the name suggests, the element evicted is the ones which has the oldest last access time. The last accessed timestamp is updated when an element is put into the cache or an element is retrieved from the cache with a get call. This is by far the most popular eviction strategy. LFU ( Least Frequently Used ) : Again, as the name suggests, every entry has a frequency associated with it. At the time of eviction, the entry with lowest frequency is evicted. This is most effective in cases when most of access is limited to a very small portion of the data ( pareto distribution ). As you can see, every eviction strategy has its own set of cases when they are most effective. The choice of which eviction strategy to choose is largely dependent on the expected access pattern. We will go with the default here which is LRU. Q. What should be the access pattern for the given cache? A: There are majorly three kinds of caching systems : Write through cache : This is a caching system where writes go through the cache and write is confirmed as success only if writes to DB and the cache BOTH succeed. This is really useful for applications which write and re-read the information quickly. However, write latency will be higher in this case as there are writes to 2 separate systems. Write around cache : This is a caching system where write directly goes to the DB. The cache system reads the information from DB incase of a miss. While this ensures lower write load to the cache and faster writes, this can lead to higher read latency incase of applications which write and re-read the information quickly. Write back cache : This is a caching system where the write is directly done to the caching layer and the write is confirmed as soon as the write to the cache completes. The cache then asynchronously syncs this write to the DB. This would lead to a really quick write latency and high write throughput. But, as is the case with any non-persistent / in-memory write, we stand the risk of losing the data incase the caching layer dies. We can improve our odds by introducing having more than one replica acknowledging the write ( so that we don\u2019t lose data if just one of the replica dies ). Estimations: This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier. Q: What is the kind of QPS we expect for the system? A: This estimation is important to understand the number of machines we will need to answer the queries. For example, if our estimations state that a single machine is going to handle 1M QPS, we run into a high risk of high latency / the machine dying because of queries not being answered fast enough and hence ending up in the backlog queue. Again, let's assume the scale of Twitter / Google. We can expect around 10M QPS if not more. Q: What is the number of machines required to cache? A: A cache has to be inherently of low latency. Which means all cache data has to reside in main memory. A production level caching machine would be 72G or 144G of RAM. Assuming beefier cache machines, we have 72G of main memory for 1 machine. Min. number of machine required = 30 TB / 72G which is close to 420 machines. Do know that this is the absolute minimum. Its possible we might need more machines because the QPS per machine is higher than we want it to be. Design Goals: Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other. Q: Is Latency a very important metric for us? A: Yes. The whole point of caching is low latency. Q: Consistency vs Availability? A: Unavailability in a caching system means that the caching machine goes down, which in turn means that we have a cache miss which leads to a high latency. As said before, we are caching for a Twitter / Google like system. When fetching a timeline for a user, I would be okay if I miss on a few tweets which were very recently posted as long as I eventually see them in reasonable time. Unavailability could lead to latency spikes and increased load on DB. Choosing from consistency and availability, we should prioritize for availability. Skeleton of the design: The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction. Deep Dive: Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes). Q: How would a LRU cache work on a single machine which is single threaded? Q: What if we never had to remove entries from the LRU cache because we had enough space, what would you use to support and get and set? A: A simple map / hashmap would suffice. Q: How should we modify our approach if we also have to evict keys at some stage? A: We need a data structure which at any given instance can give me the least recently used objects in order. Let's see if we can maintain a linked list to do it. We try to keep the list ordered by the order in which they are used. So whenever, a get operation happens, we would need to move that object from a certain position in the list to the front of the list. Which means a delete followed by insert at the beginning. Insert at the beginning of the list is trivial. How do we achieve erase of the object from a random position in least time possible? How about we maintain another map which stores the value to the corresponding linked list node. Ok, now when we know the node, we would need to know its previous and next node in the list to enable the deletion of the node from the list. We can get the next in the list from next pointer ? What about the previous node ? To encounter that, we make the list doubly linked list. A: Since we only have one thread to work with, we cannot do things in parallel. So we will take a simple approach and implement a LRU cache using a linked list and a map. The Map stores the value to the corresponding linked list node and is useful to move the recently accessed node to the front of the list. Q: How would a LRU cache work on a single machine which is multi threaded ? Q: How would you break down cache write and read into multiple instructions? A: Read path : Read a value corresponding to a key. This requires : Operation 1 : A read from the HashMap and then, Operation 2 : An update in the doubly LinkedList Write path : Insert a new key-value entry to the LRU cache. This requires : If the cache is full, then Operation 3: Figure out the least recently used item from the linkedList Operation 4: Remove it from the hashMap Operation 5: Remove the entry from the linkedList. Operation 6: Insert the new item in the hashMap Operation 7: Insert the new item in the linkedList. Q: How would you prioritize above operations to keep latency to a minimum for our system? A: As is the case with most concurrent systems, writes compete with reads and other writes. That requires some form of locking when a write is in progress. We can choose to have writes as granular as possible to help with performance. Read path is going to be highly frequent. As latency is our design goal, Operation 1 needs to be really fast and should require minimum locks. Operation 2 can happen asynchronously. Similarly, all of the write path can happen asynchronously and the client\u2019s latency need not be affected by anything other than Operation 1. Let's dig deeper into Operation 1. What are the things that Hashmap is dealing with? Hashmap deals with Operation 1, 4 and 6 with Operation 4 and 6 being write operations. One simple, but not so efficient way of handling read/write would be to acquire a higher level Read lock for Operation 1 and Write lock for Operation 4 and 6. However, Operation 1 as stressed earlier is the most frequent ( by a huge margin ) operation and its performance is critical to how our caching system works. Q: How would you implement HashMap? A: The HashMap itself could be implemented in multiple ways. One common way could be hashing with linked list (colliding values linked together in a linkedList) : Let's say our hashmap size is N and we wish to add (k,v) to it Let H = size N array of pointers with every element initialized to NULL For a given key k, generate g = hash(k) % N newEntry = LinkedList Node with value = v newEntry.next = H[g] H[g] = newEntry More details at https://en.wikipedia.org/wiki/Hash_table Given this implementation, we can see that instead of having a lock on a hashmap level, we can have it for every single row. This way, a read for row i and a write for row j would not affect each other if i != j. Note that we would try to keep N as high as possible here to increase granularity.","title":"Design Cache"},{"location":"DesignCache/#design-a-distributed-key-value-caching-system-like-memcached-or-redis","text":"","title":"Design a distributed key value caching system, like Memcached or Redis."},{"location":"DesignCache/#features","text":"This is the first part of any system design interview, coming up with the features which the system should support. As an interviewee, you should try to list down all the features you can think of which our system should support. Try to spend around 2 minutes for this section in the interview. You can use the notes section alongside to remember what you wrote.","title":"Features:"},{"location":"DesignCache/#estimations","text":"This is usually the second part of a design interview, coming up with the estimated numbers of how scalable our system should be. Important parameters to remember for this section is the number of queries per second and the data which the system will be required to handle. Try to spend around 5 minutes for this section in the interview. Total cache size : Let's say 30TB as discussed earlier.","title":"Estimations:"},{"location":"DesignCache/#design-goals","text":"Latency - Is this problem very latency sensitive (Or in other words, Are requests with high latency and a failing request, equally bad?). For example, search typeahead suggestions are useless if they take more than a second. Consistency - Does this problem require tight consistency? Or is it okay if things are eventually consistent? Availability - Does this problem require 100% availability? There could be more goals depending on the problem. It's possible that all parameters might be important, and some of them might conflict. In that case, you\u2019d need to prioritize one over the other.","title":"Design Goals:"},{"location":"DesignCache/#skeleton-of-the-design","text":"The next step in most cases is to come up with the barebone design of your system, both in terms of API and the overall workflow of a read and write request. Workflow of read/write request here refers to specifying the important components and how they interact. Try to spend around 5 minutes for this section in the interview. Important : Try to gather feedback from the interviewer here to indicate if you are headed in the right direction.","title":"Skeleton of the design:"},{"location":"DesignCache/#deep-dive","text":"Lets dig deeper into every component one by one. Discussion for this section will take majority of the interview time(20-30 minutes).","title":"Deep Dive:"},{"location":"LoadBalancing/","text":"Load Balancing A load balancer works as a \u201ctraffic cop\u201d sitting in front of your server and routing client requests across all servers. It helps to spread the traffic across a cluster of servers to improve responsiveness and availability of applications, websites or databases. LB also keeps track of the status of all the resources while distributing requests. A load balancer can be a physical device or a virtualized instance running on specialized hardware or a software process. If a server is not available to take new requests or is not responding or has elevated error rate, Load Balancer will stop sending traffic to such a server. To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add load balancers at three places: Between the user and the web server Between web servers and an internal platform layer, like application servers or cache servers Between internal platform layer and database. Load Balancing Algorithms There is a variety of load balancing methods, which use different algorithms for different needs. 1. Least Connection Method : This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers. 2. Least Response Time Method : This algorithm directs traffic to the server with the fewest active connections and the lowest average response time. 3. Least Bandwidth Method : This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps). 4. Round Robin Method : This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights. 6. IP Hash : Under this method, a hash of the IP address of the client is calculated to redirect the request to a server. Redundant Load Balancers The load balancer can be a single point of failure; to overcome this, a second load balancer can be connected to the first to form a cluster. Each load balancer monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over. Benefits of Load Balancing Users experience faster, uninterrupted service. Users won\u2019t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource. Service providers experience less downtime and higher throughput. Even a full server failure won\u2019t affect the end user experience as the load balancer will simply route around it to a healthy server. Load balancing makes it easier for system administrators to handle incoming requests while decreasing wait time for users. Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the smart load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions. System administrators experience fewer failed or stressed components. Instead of a single device performing a lot of work, load balancing has several devices perform a little bit of work. Following links have some good discussion about load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing","text":"A load balancer works as a \u201ctraffic cop\u201d sitting in front of your server and routing client requests across all servers. It helps to spread the traffic across a cluster of servers to improve responsiveness and availability of applications, websites or databases. LB also keeps track of the status of all the resources while distributing requests. A load balancer can be a physical device or a virtualized instance running on specialized hardware or a software process. If a server is not available to take new requests or is not responding or has elevated error rate, Load Balancer will stop sending traffic to such a server. To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add load balancers at three places: Between the user and the web server Between web servers and an internal platform layer, like application servers or cache servers Between internal platform layer and database.","title":"Load Balancing"},{"location":"LoadBalancing/#load-balancing-algorithms","text":"There is a variety of load balancing methods, which use different algorithms for different needs. 1. Least Connection Method : This method directs traffic to the server with the fewest active connections. This approach is quite useful when there are a large number of persistent client connections which are unevenly distributed between the servers. 2. Least Response Time Method : This algorithm directs traffic to the server with the fewest active connections and the lowest average response time. 3. Least Bandwidth Method : This method selects the server that is currently serving the least amount of traffic measured in megabits per second (Mbps). 4. Round Robin Method : This method cycles through a list of servers and sends each new request to the next server. When it reaches the end of the list, it starts over at the beginning. It is most useful when the servers are of equal specification and there are not many persistent connections. 5. Weighted Round Robin Method : The weighted round-robin scheduling is designed to better handle servers with different processing capacities. Each server is assigned a weight (an integer value that indicates the processing capacity). Servers with higher weights receive new connections before those with less weights and servers with higher weights get more connections than those with less weights. 6. IP Hash : Under this method, a hash of the IP address of the client is calculated to redirect the request to a server.","title":"Load Balancing Algorithms"},{"location":"LoadBalancing/#redundant-load-balancers","text":"The load balancer can be a single point of failure; to overcome this, a second load balancer can be connected to the first to form a cluster. Each load balancer monitors the health of the other and, since both of them are equally capable of serving traffic and failure detection, in the event the main load balancer fails, the second load balancer takes over.","title":"Redundant Load Balancers"},{"location":"LoadBalancing/#benefits-of-load-balancing","text":"Users experience faster, uninterrupted service. Users won\u2019t have to wait for a single struggling server to finish its previous tasks. Instead, their requests are immediately passed on to a more readily available resource. Service providers experience less downtime and higher throughput. Even a full server failure won\u2019t affect the end user experience as the load balancer will simply route around it to a healthy server. Load balancing makes it easier for system administrators to handle incoming requests while decreasing wait time for users. Smart load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the smart load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions. System administrators experience fewer failed or stressed components. Instead of a single device performing a lot of work, load balancing has several devices perform a little bit of work. Following links have some good discussion about load balancers: What is load balancing Introduction to architecting systems Load balancing","title":"Benefits of Load Balancing"},{"location":"PreRequisites/","text":"Pre Requisites We assume in this section that you : Have some experience working with a relational DB ( like MySQL ). Have a basic idea about NoSQL DBs. Understand the basics of the following : 1. Concurrency : Do you understand threads, deadlock, and starvation? What happens when multiple processes / threads are trying to modify the same data? A basic understanding of read and write locks. 2. Networking : Do you roughly understand basic networking protocols like TCP and UDP? Do you understand the role of switches and routers? 3. File systems : You should understand the systems you\u2019re building upon. Do you know roughly how an OS, file system, and database work? Do you know about the various levels of caching in a modern OS?","title":"Pre Requisites"},{"location":"PreRequisites/#pre-requisites","text":"We assume in this section that you : Have some experience working with a relational DB ( like MySQL ). Have a basic idea about NoSQL DBs. Understand the basics of the following : 1. Concurrency : Do you understand threads, deadlock, and starvation? What happens when multiple processes / threads are trying to modify the same data? A basic understanding of read and write locks. 2. Networking : Do you roughly understand basic networking protocols like TCP and UDP? Do you understand the role of switches and routers? 3. File systems : You should understand the systems you\u2019re building upon. Do you know roughly how an OS, file system, and database work? Do you know about the various levels of caching in a modern OS?","title":"Pre Requisites"},{"location":"StepsToApproachAProblem/","text":"","title":"Steps To Approach A Problem"},{"location":"about/","text":"","title":"About"}]}